[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects Collection",
    "section": "",
    "text": "These featured projects were selected to highlight my skillset and advancements in data.\n\n\nRent Forecast PowerBI App\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nI construct and compare multiple predictive models to forecast rent price. Leveraging comprehensive data, I provide actionable insights through an interactive PowerBI dashboard, highlighting the factors shaping rental costs across the country.\nApp →\nArticle →\n\n\nRental Market Analysis\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nI delve into the complexities of Canada’s rental market by constructing a statistical model. Using data from Statistics Canada and CMHC, I employ ordinary least squares regression to identify key factors influencing rent prices nationwide.\nArticle →\n\n\nMetacritic Analysis App\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nI leverage Shiny to craft an intuitive dashboard exploring Metacritic’s game ratings over time. By emphasizing interactive elements, I provide an application that empowers users to explore and uncover hidden patterns and trends within the gaming industry.\nApp →\nArticle →"
  },
  {
    "objectID": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html",
    "href": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html",
    "title": "Network Mapping the Most Influential Addresses Using DAI (2021)",
    "section": "",
    "text": "Decentralized Autonomous Organizations (DAO) use the blockchain to conduct transparent corporate operations. One of the most successful DAOs to date is MakerDAO. Their staying power in the tumultuous environment of crypto is largely attributed to the wide adoption of their overcollateralized backed USD stable coin, DAI.\nAll blockchain transactions are public and pseudonymous. This gives unprecedented public transparency to the inner workings of blockchain based businesses and their products. In this report, I extract all DAI specific transfers of value and map influential addresses in an interactive network using ggraph and visNetwork to investigate key actors in the DAI ecosystem. I will outline my methodology and analyze the resultant networkmap."
  },
  {
    "objectID": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#introduction",
    "href": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#introduction",
    "title": "Network Mapping the Most Influential Addresses Using DAI (2021)",
    "section": "",
    "text": "Decentralized Autonomous Organizations (DAO) use the blockchain to conduct transparent corporate operations. One of the most successful DAOs to date is MakerDAO. Their staying power in the tumultuous environment of crypto is largely attributed to the wide adoption of their overcollateralized backed USD stable coin, DAI.\nAll blockchain transactions are public and pseudonymous. This gives unprecedented public transparency to the inner workings of blockchain based businesses and their products. In this report, I extract all DAI specific transfers of value and map influential addresses in an interactive network using ggraph and visNetwork to investigate key actors in the DAI ecosystem. I will outline my methodology and analyze the resultant networkmap."
  },
  {
    "objectID": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#methodology",
    "href": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#methodology",
    "title": "Network Mapping the Most Influential Addresses Using DAI (2021)",
    "section": "Methodology",
    "text": "Methodology\nUsing the MakerDAO events table I created, I subset all DAI events. Values were stored in BigQuery as hexadecimals because values were too large to store in BigQuery.\n\nCREATE TABLE \n  makerdao_operations.dai_events_may2021to2022\nPARTITION BY DATE (\n  block_timestamp\n)\nCLUSTER BY\n  makerdao_smartcontract,\n  makerdao_function\nAS (\n  SELECT\n    block_timestamp,\n    makerdao_smartcontract,\n    makerdao_function,\n    to_address AS transaction_sender,\n    from_address AS transaction_reciever,\n    sender AS event_sender,\n    reciever AS event_reciever,\n    eth_value,\n    value,\n    percision\n  FROM `ethereum-explorer-354118.makerdao_operations.may2021to2022`\n  WHERE makerdao_smartcontract IN (\n    'DAI',\n    'DAIJOIN',\n    'GEMJOIN',\n    'JUG',\n    'POT'\n  )\n)\n\nThe subset table was imported into R and converted into decimal form according to the event specific degree of precision.\n\n#clean data from extracted from google bigquery\ndata &lt;- as_tibble(\n  read.csv(\n    file = 'dai_events_may2021to2022.csv',\n    header = TRUE,\n    colClasses = c(\"Date\", \"character\", \"character\", \"character\",\n                  \"character\", \"numeric\", \"character\"))) %&gt;% \n  mutate(value =\n    ifelse (percision == 'wad', value/10e18,\n      ifelse (percision == 'ray', value/10e27,\n        ifelse (percision == 'rad', value/10e45, NaN)))) %&gt;% \n  select(-percision)\n\nFinally, all transfer (of value) events pertaining to the DAI smartcontract were extracted.\n\n#filter events to pull out only DAI transaction data\ndai_transactions &lt;- data %&gt;% \n  filter(makerdao_smartcontract == 'DAI' & \n    makerdao_function == 'transfer')\n\nBefore mapping a network, data had to be transformed into a form readable by network mapping packages. The node list was formed from unique senders and receivers. The edge list was formed from all transactions between unique addresses. Multiple transactions between the same addresses were counted as ‘weight’. Total transaction value was summed for each unique address pair.\n\n#create node list (list of nodes with corresponding IDs)\nnode_list &lt;- dai_transactions %&gt;% \n  distinct(sender) %&gt;% \n  full_join(distinct(dai_transactions, reciever), \n    by = c(\"sender\" = \"reciever\")) %&gt;% \n  rowid_to_column(\"id\")\n\n#check node list\nnode_list\n\n#create edge list with count as weight and add value column\nedge_list &lt;- dai_transactions %&gt;% \n  group_by(sender, reciever) %&gt;% \n  summarize(weight = n(), amount = sum(value)) %&gt;% \n  ungroup()\n\n#convert edge list addresses into id specified in node_list\nedge_list_byid &lt;- edge_list %&gt;%\n  left_join(node_list, by = \"sender\") %&gt;%\n  rename(from = id) %&gt;%\n  left_join(node_list, by = c(\"reciever\" = \"sender\")) %&gt;%\n  rename(to = id) %&gt;%\n  select(from, to, weight, amount)\n\n#check converted edge_list then remove original\nedge_list_byid\n\nIt is not uncommon for a user to have multiple accounts which can skew network analysis if not accounted for. To alleviate the effects of this, I used the infomap algorithm to cluster addresses into communities. The rationale was that most alternate accounts would closely transact with each other forming distinct clusters. The infomap algorithm was used as it takes into account edge (trade) direction. In order to identified key nodes in the network, addresses’ eigenvector centrality was calculated using trade frequency as a weight.\n\n#using the node_list and edge_list_byid we can create a\n#graph object\ngraph &lt;- tbl_graph(nodes = node_list, edges = edge_list_byid, \n          directed = TRUE)\n\n#cluster nodes to find communities and centralities and rewrite graph object\ngraph &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = group_infomap(weights = weight)) %&gt;%\n  mutate(centrality = centrality_eigen(weights = weight, directed = TRUE))\n\nAs the DAI transaction dataset consists of over 1,500,000 transactions, addresses (nodes) were filtered for unique transaction pairs (edges) that were in the top 50% quantile for total transaction amount. Low value transacting nodes are unlikely to be key to the network. As I am looking for important actors in the network, I selected for the top 0.1% of addresses by eigenvector centrality. The selected addresses were collapsed on their communities.\n\n#filter graph by centrality to reduce size by trimming data to visualize, value amount filtered to remove low value actors\n#note this also reduces community sizing as low value actors are excluded\nfiltered_graph &lt;- graph %&gt;%\n  activate(edges) %&gt;%\n  filter(amount &gt; quantile(amount, 0.50), weight &gt; 100) %&gt;%\n  activate(nodes) %&gt;%\n  filter(centrality &gt; quantile(centrality, 0.999)) %&gt;%\n  filter(!node_is_isolated())\n\n#network is too large to be visualized so it will be contracted on its communities, weight and value will be summed\ncontracted_graph &lt;- filtered_graph %&gt;%\n  activate(nodes) %&gt;%\n  convert(to_contracted, community) %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community_size = map_dbl(.orig_data, ~ nrow(.x))) %&gt;%\n  activate(edges) %&gt;%\n  mutate(contracted_weight = map_dbl(.orig_data, ~ sum(.x$weight)), \n         contracted_amount = map_dbl(.orig_data, ~ sum(.x$amount)))\n\n\ncontracted_graph &lt;- readRDS(\"contracted_graph.rds\")\ncontracted_graph\n\nThis graph was created by an old(er) igraph version.\n  Call upgrade_graph() on it to use with the current igraph version\n  For now we convert it on the fly...\n\n\n# A tbl_graph: 184 nodes and 968 edges\n#\n# A directed simple graph with 1 component\n#\n# Edge Data: 968 × 6 (active)\n    from    to .tidygraph_edge_index .orig_data        contracted_weight\n   &lt;int&gt; &lt;int&gt; &lt;list&gt;                &lt;list&gt;                        &lt;dbl&gt;\n 1     1     2 &lt;int [45]&gt;            &lt;tibble [45 × 4]&gt;             61537\n 2     1     3 &lt;int [4]&gt;             &lt;tibble [4 × 4]&gt;               1777\n 3     1     4 &lt;int [20]&gt;            &lt;tibble [20 × 4]&gt;              5699\n 4     1     5 &lt;int [2]&gt;             &lt;tibble [2 × 4]&gt;                332\n 5     1     6 &lt;int [13]&gt;            &lt;tibble [13 × 4]&gt;              4668\n 6     1     7 &lt;int [15]&gt;            &lt;tibble [15 × 4]&gt;             13995\n 7     1     8 &lt;int [2]&gt;             &lt;tibble [2 × 4]&gt;                493\n 8     1     9 &lt;int [8]&gt;             &lt;tibble [8 × 4]&gt;               3170\n 9     1    10 &lt;int [12]&gt;            &lt;tibble [12 × 4]&gt;             14052\n10     1    11 &lt;int [5]&gt;             &lt;tibble [5 × 4]&gt;               1281\n# ℹ 958 more rows\n# ℹ 1 more variable: contracted_amount &lt;dbl&gt;\n#\n# Node Data: 184 × 4\n  community .orig_data        .tidygraph_node_index community_size\n      &lt;int&gt; &lt;list&gt;            &lt;list&gt;                         &lt;dbl&gt;\n1        56 &lt;tibble [95 × 3]&gt; &lt;int [95]&gt;                        95\n2         8 &lt;tibble [4 × 3]&gt;  &lt;int [4]&gt;                          4\n3        72 &lt;tibble [1 × 3]&gt;  &lt;int [1]&gt;                          1\n# ℹ 181 more rows\n\n\nResulting network was graphed using ggraph. Node radius reflected community size and color reflected degree of connections (darker blue representing a higher degree).\n\n#graph network\nnetwork_graph &lt;- ggraph(contracted_graph, layout = 'igraph', algorithm = 'kk') +\n  geom_edge_link(aes(edge_alpha = contracted_weight, edge_colour = contracted_amount)) +\n  scale_edge_colour_continuous(high = '#000000', low = '#56B1F7') +\n  geom_node_point(aes(size = sqrt(community_size), colour = centrality_degree())) +\n  scale_colour_continuous(high = '#000000', low = '#56B1F7')\n\n\n\n\n\n\n\n\n\n\nThough some tentative conclusions may be made about the general network, there are too many nodes to pick out key addresses. Only the highest valued transaction pairs were selected to keep the map readable.\n\n#subset graph on value transacted to visualize high value connections (and importantly, visualize readable labels)\n\nsubset_graph &lt;- contracted_graph %&gt;%\n  activate(edges) %&gt;%\n  top_n(50, contracted_amount) %&gt;%\n  activate(nodes) %&gt;%\n  filter(!node_is_isolated())\n\nsubset_network_graph &lt;- ggraph(subset_graph, layout = 'igraph', algorithm = 'kk') +\n  geom_edge_link(aes(edge_alpha = contracted_weight, edge_colour = contracted_amount)) +\n  scale_edge_colour_continuous(high = '#000000', low = '#56B1F7') +\n  geom_node_point(aes(size = sqrt(community_size), colour = centrality_degree())) +\n  scale_colour_continuous(high = '#000000', low = '#56B1F7') +\n  geom_node_text(aes(label = community), position = position_nudge(x = 0.2, y = 0.2))\n\n\n\n\n\n\n\n\n\n\nCommunity 56 looked to be the most promising community and was thus graphed. As addresses are long and cumbersome to read, I thought an interactive visualization may be more user friendly. Before mapping in visNetwork, community 56 data had to be transformed into a format visNetwork would accept.\n\n#prepare data for visualization of community 56 in order to display addresses (sender) in a readable way\n\n#create color palette for groups\npalette &lt;- as_tibble(brewer.pal(6, 'Set3'))\n\nvis_edge_list &lt;- community_56_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(to)) %&gt;%\n  as_tibble() %&gt;%\n  mutate(arrows = 'to') %&gt;%\n  rename(title = amount, value = weight)\n\nvis_edge_list\n\n# A tibble: 86 × 5\n    from    to value    title arrows\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt; \n 1    49    52     1 1200000  to    \n 2    17    50    34 1614047. to    \n 3    18    49    48 1403938. to    \n 4    18    48     8  898990. to    \n 5    18    47    20 1287273. to    \n 6    18    46     3  836676. to    \n 7    10    45    32  797796. to    \n 8    17    44    20 2348153. to    \n 9    17    43    53 1594078. to    \n10    28    40    14 1570956. to    \n# ℹ 76 more rows\n\nvis_node_list &lt;- community_56_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(grouping = group_infomap()) %&gt;%\n  as_tibble() %&gt;%\n  select(-id, -community) %&gt;%\n  rowid_to_column('id') %&gt;%\n  rename(title = sender, value = centrality) %&gt;%\n  mutate(label = \"\", color = case_when(grouping == 1 ~ '#FB8072',\n                                       grouping == 2 ~ '#FFFFB3',\n                                       grouping == 3 ~ '#BEBADA',\n                                       grouping == 4 ~ '#8DD3C7',\n                                       grouping == 5 ~ '#80B1D3',\n                                       grouping == 6 ~ '#FDB462'))\n\nvis_node_list\n\n# A tibble: 54 × 6\n      id title                                        value grouping label color\n   &lt;int&gt; &lt;chr&gt;                                        &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1     1 0x0000000000007f150bd6f54c40a34d7c3d5e9f56 5.98e-2        2 \"\"    #FFF…\n 2     2 0xf7d9c9c06812c4a611a352ac82f638bdca6e09a8 2.00e-2        4 \"\"    #8DD…\n 3     3 0xd0effc6828972483db1c64106f71d6ad12606a53 1.35e-2        4 \"\"    #8DD…\n 4     4 0x2af33e93f68b8497bb535c492f5867c7e83f2bc4 4.82e-4        1 \"\"    #FB8…\n 5     5 0xe592427a0aece92de3edee1f18e0157c05861564 4.47e-1        4 \"\"    #8DD…\n 6     6 0x6c2d992b7739dfb363a473cc4f28998b7f1f6de2 2.19e-3        2 \"\"    #FFF…\n 7     7 0x2a84e2bd2e961b1557d6e516ca647268b432cba4 9.11e-3        3 \"\"    #BEB…\n 8     8 0x4a137fd5e7a256ef08a7de531a17d0be0cc7b6b6 1.65e-2        3 \"\"    #BEB…\n 9     9 0x4d246be90c2f36730bb853ad41d0a189061192d3 2.61e-2        5 \"\"    #80B…\n10    10 0xbf3f6477dbd514ef85b7d3ec6ac2205fd0962039 1.05e-2        1 \"\"    #FB8…\n# ℹ 44 more rows\n\n\nNetwork was mapped so that addresses would display on node mouse over and total transaction value would display on edge mouse over. Click lights up one degree of edges.\n\n#graph dynamic network with prepared data\ndynamic_network_graph &lt;- visNetwork(vis_node_list, vis_edge_list, main = \n                                    \"Transaction Network of Top 1% Most Influential Addresses Using DAI\") %&gt;%\n  visOptions(highlightNearest = list(enabled = TRUE, degree = list(from = 0, to = 1))) %&gt;%\n  visInteraction(dragView = FALSE) %&gt;%\n  visLayout(randomSeed = 11)\n\ndynamic_network_graph"
  },
  {
    "objectID": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#analysis",
    "href": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#analysis",
    "title": "Network Mapping the Most Influential Addresses Using DAI (2021)",
    "section": "Analysis",
    "text": "Analysis\nTo determine if my analysis picked out key players in the the DAI ecosystem, I checked to see if there were any exchange addresses. I expect exchanges would be central to the network because they deal in high transaction volumes and values. Indeed, I found 0x220bda5c8994804ac96ebe4df184d25e5c2196d4 and 0x1111111254fb6c44bAC0beD2854e76F90643097d. Both are smart contracts of 1inch, a popular DeFi aggregator. Sushiswap’s MANA-DAI pool, 0x495F8Ef80E13e9E1e77d60d2f384bb49694823ef, was among the addresses visualized. And finally, Uniswap’s V3 router 0xe592427a0aece92de3edee1f18e0157c05861564 was also present.\nThe more unknown addresses provided interesting insight into the users that hold up DAI. 0x1e3D6eAb4BCF24bcD04721caA11C478a2e59852D is one such mysterious address. Activity on this address stopped around May 2021 but beforehand seemed involved in high frequency trading of renBTC and wBTC. One explanation may be that this address was an arbitrage trader transacting in BTC derived cryptos. Their high DAI transaction values may indicate they used DAI as an arbitrage currency. DAI allows for fee-less flash-loans (loans that are instantaneously paid) making it very permissive for arbitrage traders."
  },
  {
    "objectID": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#references",
    "href": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#references",
    "title": "Network Mapping the Most Influential Addresses Using DAI (2021)",
    "section": "References",
    "text": "References\nAlmende B.V. and Contributors, Thieurmel B, Robert T (2021). visNetwork: Network Visualization using ‘vis.js’ Library. R package version 2.1.0, URL https://CRAN.R-project.org/package=visNetwork\nGoogle (2021). BigQuery. URL https://cloud.google.com/bigquery\nNeuwirth E (2022). RColorBrewer: ColorBrewer Palettes. R package version 1.1-3, URL https://CRAN.R-project.org/package=RColorBrewer\nPedersen T (2021). ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. R package version 2.0.5, URL https://CRAN.R-project.org/package=ggraph\nPedersen T (2022). tidygraph: A Tidy API for Graph Manipulation. R package version 1.2.1, URL https://CRAN.R-project.org/package=tidygraph\nR Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, URL https://doi.org/10.21105/joss.01686"
  },
  {
    "objectID": "posts/MakerDAO_dataset_generation_report.html",
    "href": "posts/MakerDAO_dataset_generation_report.html",
    "title": "Network Mapping the Most Influential Addresses using DAI (2021): Extracting Blockchain Data from Public Google Datasets with BigQuery and SQL",
    "section": "",
    "text": "Blockchain technologies are inarguably reshaping how the general public thinks of finance. One particularly interesting innovation is the concept of decentralized autonomous organizations (or DAOs). Leveraging the transparency of the blockchain, DAOs endeavour to collaboratively run organizations in a trustless way. This novel method of conducting corporate operations makes a compelling subject for analysis.\nOne prominent DAO is MakerDAO. They are most well known for their over-collateralized USD stable coin, DAI. As all core MakerDAO operations and products operate through the blockchain, I retrieved all emitted event logs pertaining to MakerDAO smart contracts (executes blockchain functions) using BigQuery and the MakerDAO technical docs."
  },
  {
    "objectID": "posts/MakerDAO_dataset_generation_report.html#introduction",
    "href": "posts/MakerDAO_dataset_generation_report.html#introduction",
    "title": "Network Mapping the Most Influential Addresses using DAI (2021): Extracting Blockchain Data from Public Google Datasets with BigQuery and SQL",
    "section": "",
    "text": "Blockchain technologies are inarguably reshaping how the general public thinks of finance. One particularly interesting innovation is the concept of decentralized autonomous organizations (or DAOs). Leveraging the transparency of the blockchain, DAOs endeavour to collaboratively run organizations in a trustless way. This novel method of conducting corporate operations makes a compelling subject for analysis.\nOne prominent DAO is MakerDAO. They are most well known for their over-collateralized USD stable coin, DAI. As all core MakerDAO operations and products operate through the blockchain, I retrieved all emitted event logs pertaining to MakerDAO smart contracts (executes blockchain functions) using BigQuery and the MakerDAO technical docs."
  },
  {
    "objectID": "posts/MakerDAO_dataset_generation_report.html#methodology",
    "href": "posts/MakerDAO_dataset_generation_report.html#methodology",
    "title": "Network Mapping the Most Influential Addresses using DAI (2021): Extracting Blockchain Data from Public Google Datasets with BigQuery and SQL",
    "section": "Methodology",
    "text": "Methodology\nTable of MakerDAO events was created using the publicly available Ethereum blockchain dataset on BigQuery. As addresses are stored on the blockchain as a 42 character hexadecimal, all smart contract addresses were converted to their contract name using MakerDAO technical docs. Topic 0 of event logs typically contains a unqiue event type specific index and was used to annotate function events. Each unique event type was individually parsed for sender identity, reciever identity and value. Makerdao stores values as integers which stand for differing decimals of percision depending on the event type. An unfortunate side effect of this storage method is that the values are too large for BigQuery to handle (can only go up to INT64). This necessitated using R to view the correct values. Values were stored on BigQuery as hexadecimals.\n\nCREATE TABLE \n     makerdao_operations.may2021to2022\nPARTITION BY DATE (\n     block_timestamp\n)\nCLUSTER BY\n     makerdao_smartcontract,\n     makerdao_function\nAS (\n\nSELECT\n     logs.block_timestamp AS block_timestamp,\n     transaction_hash,\n     from_address,\n     to_address,\n     value AS eth_value,\n     gas_price,\n     CASE\n          WHEN address = '0x6b175474e89094c44da98b954eedeac495271d0f' THEN 'DAI'\n          WHEN address = '0x9759A6Ac90977b93B58547b4A71c78317f391A28' THEN 'DAIJOIN'\n          WHEN address = '0x2F0b23f53734252Bda2277357e97e1517d6B042A' THEN 'GEMJOIN'\n          WHEN address = '0x197E90f9FAD81970bA7976f33CbD77088E5D7cf7' THEN 'POT'\n          WHEN address = '0x19c0976f590D67707E62397C87829d896Dc0f1F1' THEN 'JUG'\n          WHEN address = '0xA950524441892A31ebddF91d3cEEFa04Bf454466' THEN 'VOW'\n          WHEN address = '0xC4269cC7acDEdC3794b221aA4D9205F564e27f0d' THEN 'FLAPPER'\n          WHEN address = '0xA41B6EF151E06da0e34B009B86E828308986736D' THEN 'FLOPPER'\n          WHEN address = '0x35D1b3F3D7966A1DFe207aa4514C12a259A0492B' THEN 'VAT'\n          WHEN address = '0x65C79fcB50Ca1594B025960e539eD7A9a6D434A3' THEN 'SPOT'\n          WHEN address = '0x135954d155898D42C90D2a57824C690e0c7BEf1B' THEN 'DOG'\n          WHEN address = '0xc67963a226eddd77B91aD8c421630A1b0AdFF270' THEN 'CLIPPER'\n          WHEN address = '0x1EB4CF3A948E7D72A198fe073cCb8C7a948cD853' THEN 'FLASH'\n          WHEN address = '0x9f8F72aA9304c8B593d555F12eF6589cC3A579A2' THEN 'MKR'\n          WHEN address = '0x0a3f6849f78076aefaDf113F5BED87720274dDC0' THEN 'DSCHIEF'\n          WHEN address = '0xA618E54de493ec29432EbD2CA7f14eFbF6Ac17F7' THEN 'DSTOKEN'\n          WHEN address = '0x82ecD135Dce65Fbc6DbdD0e4237E0AF93FFD5038' THEN 'PROXY'\n          WHEN address = '0x5ef30b9986345249bc32d8928B7ee64DE9435E39' THEN 'CDPMANAGER'\n               END AS makerdao_smartcontract,\n     CASE\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(1)] NOT IN (\n                                        '0x0000000000000000000000000000000000000000',\n                                        '0x2F0b23f53734252Bda2277357e97e1517d6B042A',\n                                        '0x0a3f6849f78076aefadf113f5bed87720274ddc0'\n                                        )                                                               AND\n               topics[OFFSET(2)] NOT IN ('0x0000000000000000000000000000000000000000',\n                                        '0x2F0b23f53734252Bda2277357e97e1517d6B042A',\n                                        '0x0a3f6849f78076aefadf113f5bed87720274ddc0'\n                                        )                                                               THEN 'transfer' --DAI/MKR/IOU\n          WHEN topics[OFFSET(0)] = '0x8c5be1e5ebec7d5bd14f71427d1e84f3dd0314c0f7b2291e5b200ac8c7c3b925' THEN 'approve' --DAI/MKR/IOU\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(1)] = '0x0000000000000000000000000000000000000000'                         THEN 'mint/exit' --DAIJOIN\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(2)] = '0x0000000000000000000000000000000000000000'                         THEN 'burn/join' --DAIJOIN\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(1)] = '0x2F0b23f53734252Bda2277357e97e1517d6B042A'                         THEN 'mint/exit' --GEMJOIN\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(2)] = '0x2F0b23f53734252Bda2277357e97e1517d6B042A'                         THEN 'burn/join' --GEMJOIN\n          WHEN topics[OFFSET(0)] = '0x9f678cca00000000000000000000000000000000000000000000000000000000' THEN 'drip' --POT\n          WHEN topics[OFFSET(0)] = '0x7f8661a100000000000000000000000000000000000000000000000000000000' THEN 'exit' --POT\n          WHEN topics[OFFSET(0)] = '0x049878f300000000000000000000000000000000000000000000000000000000' THEN 'join' --POT\n          WHEN topics[OFFSET(0)] = '0x44e2a5a800000000000000000000000000000000000000000000000000000000' THEN 'drip' --JUG\n          WHEN topics[OFFSET(0)] = '0xf37ac61c00000000000000000000000000000000000000000000000000000000' THEN 'heal' --VOW/VAT\n          WHEN topics[OFFSET(0)] = '0xd7ee674b00000000000000000000000000000000000000000000000000000000' THEN 'flog' --VOW\n          WHEN topics[OFFSET(0)] = '0xc959c42b00000000000000000000000000000000000000000000000000000000' THEN 'deal' --FLAPPER\n          WHEN topics[OFFSET(0)] = '0xa3b22fc400000000000000000000000000000000000000000000000000000000' THEN 'hope' --VAT\n          WHEN topics[OFFSET(0)] = '0xdc4d20fa00000000000000000000000000000000000000000000000000000000' THEN 'nope' --VAT\n          WHEN topics[OFFSET(0)] = '0xbb35783b00000000000000000000000000000000000000000000000000000000' THEN 'move' --VAT\n          WHEN topics[OFFSET(0)] = '0x7cdd3fde00000000000000000000000000000000000000000000000000000000' THEN 'slip' --VAT\n          WHEN topics[OFFSET(0)] = '0x7bab3f4000000000000000000000000000000000000000000000000000000000' THEN 'grab' --VAT\n          WHEN topics[OFFSET(0)] = '0xdfd7467e425a8107cfd368d159957692c25085aacbcf5228ce08f10f2146486e' THEN 'poke' --SPOT\n          WHEN topics[OFFSET(0)] = '0x85258d09e1e4ef299ff3fc11e74af99563f022d21f3f940db982229dc2a3358c' THEN 'bark' --DOG\n          WHEN topics[OFFSET(0)] = '0x54f095dc7308776bf01e8580e4dd40fd959ea4bf50b069975768320ef8d77d8a' THEN 'digs' --DOG\n          WHEN topics[OFFSET(0)] = '0x7c5bfdc0a5e8192f6cd4972f382cec69116862fb62e6abff8003874c58e064b8' THEN 'kick' --CLIPPER\n          WHEN topics[OFFSET(0)] = '0x05e309fd6ce72f2ab888a20056bb4210df08daed86f21f95053deb19964d86b1' THEN 'take' --CLIPPER\n          WHEN topics[OFFSET(0)] = '0x0d7d75e01ab95780d3cd1c8ec0dd6c2ce19e3a20427eec8bf53283b6fb8e95f0' THEN 'flashloan' --FLASH\n          WHEN topics[OFFSET(0)] = '0xcc16f5dbb4873280815c1ee09dbd06736cffcc184412cf7a71a0fdb75d397ca5' THEN 'burn' --MKR/DSTOKEN\n          WHEN topics[OFFSET(0)] = '0x0f6798a560793a54c3bcfe86a93cde1e73087d944c0ea20544137d4121396885' THEN 'mint' --MKR/DSTOKEN\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(1)] = '0x0a3f6849f78076aefadf113f5bed87720274ddc0'                         THEN 'free' --DSCHIEF\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(2)] = '0x0a3f6849f78076aefadf113f5bed87720274ddc0'                         THEN 'lock' --DSCHIEF\n          WHEN topics[OFFSET(0)] = '0x4f0892983790f53eea39a7a496f6cb40e8811b313871337b6a761efc6c67bb1f' THEN 'etch' --DSCHIEF(slate) \n          WHEN topics[OFFSET(0)] IN (\n                                   '0xa69beaba00000000000000000000000000000000000000000000000000000000',\n                                   '0xed08132900000000000000000000000000000000000000000000000000000000'\n                                   )                                                                    THEN 'vote' --DSCHIEF\n          WHEN topics[OFFSET(0)] = '0xd6be0bc178658a382ff4f91c8c68b542aa6b71685b8fe427966b87745c3ea7a2' THEN 'open' --CDPMANAGER\n          WHEN topics[OFFSET(0)] = '0xfcafcc6800000000000000000000000000000000000000000000000000000000' THEN 'give' --CDPMANAGER\n          WHEN topics[OFFSET(0)] = '0x45e6bdcd00000000000000000000000000000000000000000000000000000000' THEN 'frob' --CDPMANAGER\n          WHEN topics[OFFSET(0)] = '0x9bb8f83800000000000000000000000000000000000000000000000000000000' THEN 'flux' --CDPMANAGER\n          WHEN topics[OFFSET(0)] = '0xf9f30db600000000000000000000000000000000000000000000000000000000' THEN 'move' --CDPMANAGER\n          WHEN topics[OFFSET(0)] = '0x1b0dbf7200000000000000000000000000000000000000000000000000000000' THEN 'quit' --CDPMANAGER\n               END AS makerdao_function,\n     CASE --note following are in hex string format (as the numbers are too big for... bigquery)\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' --transfer/burn/mint value\n               THEN data --wad (10e18)\n          WHEN topics[OFFSET(0)] = '0x7f8661a100000000000000000000000000000000000000000000000000000000' --pot/DSR exit value\n               THEN topics[OFFSET(2)] --wad\n          WHEN topics[OFFSET(0)] = '0x049878f300000000000000000000000000000000000000000000000000000000' --pot/DSR join value\n               THEN topics[OFFSET(3)] --wad\n          WHEN topics[OFFSET(0)] = '0xbb35783b00000000000000000000000000000000000000000000000000000000' --vat move value\n               THEN topics[OFFSET(2)] --rad (10e45)\n          WHEN topics[OFFSET(0)] = '0xf37ac61c00000000000000000000000000000000000000000000000000000000' --vow/vat heal amount\n               THEN topics[OFFSET(1)] --rad\n          WHEN topics[OFFSET(0)] = '0x7cdd3fde00000000000000000000000000000000000000000000000000000000' --vat slip amount\n               THEN topics[OFFSET(3)] --wad\n          WHEN topics[OFFSET(0)] = '0x7bab3f4000000000000000000000000000000000000000000000000000000000' --vat grab debt (dart) amount\n               THEN CONCAT('0x', SUBSTR(data, 459, 64)) --wad\n          WHEN topics[OFFSET(0)] = '0x85258d09e1e4ef299ff3fc11e74af99563f022d21f3f940db982229dc2a3358c' --dog bark liquidation (art)\n               THEN CONCAT('0x', SUBSTR(data, 67, 64)) --wad\n          WHEN topics[OFFSET(0)] = '0x7c5bfdc0a5e8192f6cd4972f382cec69116862fb62e6abff8003874c58e064b8' --clipper kick DAI debt (tab) \n               THEN CONCAT('0x', SUBSTR(data, 67, 64)) --rad\n          WHEN topics[OFFSET(0)] = '0x05e309fd6ce72f2ab888a20056bb4210df08daed86f21f95053deb19964d86b1' --clipper take bid (price) \n               THEN CONCAT('0x', SUBSTR(data, 67, 64)) --ray (10e27)\n          WHEN topics[OFFSET(0)] = '0x0d7d75e01ab95780d3cd1c8ec0dd6c2ce19e3a20427eec8bf53283b6fb8e95f0' --flashloan loan value (amount)\n               THEN CONCAT('0x', SUBSTR(data, 67, 64)) --ray\n          WHEN topics[OFFSET(0)] = '0xcc16f5dbb4873280815c1ee09dbd06736cffcc184412cf7a71a0fdb75d397ca5' --MKR/DSTOKEN burn value\n               THEN data --wad\n          WHEN topics[OFFSET(0)] = '0x0f6798a560793a54c3bcfe86a93cde1e73087d944c0ea20544137d4121396885' --MKR/DSTOKEN mint value\n               THEN data --wad\n          WHEN topics[OFFSET(0)] = '0x45e6bdcd00000000000000000000000000000000000000000000000000000000' --CDPMANAGER frob debt change\n               THEN CONCAT('0x', SUBSTR(data, 267, 64)) --wad (assuming dart value of similar percision across contracts)\n          WHEN topics[OFFSET(0)] = '0x9bb8f83800000000000000000000000000000000000000000000000000000000' --CDPMANAGER flux collateral\n               THEN CONCAT('0x', SUBSTR(data, 203, 64)) --wad\n          WHEN topics[OFFSET(0)] = '0xf9f30db600000000000000000000000000000000000000000000000000000000' --CDPMANAGER move value\n               THEN CONCAT('0x', SUBSTR(data, 267, 64)) --rad\n                    END AS value,\n     CASE\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' --transfer/burn/mint\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x8c5be1e5ebec7d5bd14f71427d1e84f3dd0314c0f7b2291e5b200ac8c7c3b925' --approve\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x9f678cca00000000000000000000000000000000000000000000000000000000' --drip\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x7f8661a100000000000000000000000000000000000000000000000000000000' --exit\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x049878f300000000000000000000000000000000000000000000000000000000' --join\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x44e2a5a800000000000000000000000000000000000000000000000000000000' --drip\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xf37ac61c00000000000000000000000000000000000000000000000000000000' --heal\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xd7ee674b00000000000000000000000000000000000000000000000000000000' --flog\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xc959c42b00000000000000000000000000000000000000000000000000000000' --deal\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xa3b22fc400000000000000000000000000000000000000000000000000000000' --hope\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xdc4d20fa00000000000000000000000000000000000000000000000000000000' --nope\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xbb35783b00000000000000000000000000000000000000000000000000000000' --move(vat)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x7bab3f4000000000000000000000000000000000000000000000000000000000' --grab (u; user liquidated)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x85258d09e1e4ef299ff3fc11e74af99563f022d21f3f940db982229dc2a3358c' --bark (urn liquidated)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x7c5bfdc0a5e8192f6cd4972f382cec69116862fb62e6abff8003874c58e064b8' --kick (auction id)\n               THEN SAFE_CAST(SAFE_CAST(topics[OFFSET(1)] AS INT64) AS STRING)\n          WHEN topics[OFFSET(0)] = '0x05e309fd6ce72f2ab888a20056bb4210df08daed86f21f95053deb19964d86b1' --take (who; auction bidder)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x0d7d75e01ab95780d3cd1c8ec0dd6c2ce19e3a20427eec8bf53283b6fb8e95f0' --flashloan (reciever)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xcc16f5dbb4873280815c1ee09dbd06736cffcc184412cf7a71a0fdb75d397ca5' --burn (DSTOKEN)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xa69beaba00000000000000000000000000000000000000000000000000000000' --vote\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xd6be0bc178658a382ff4f91c8c68b542aa6b71685b8fe427966b87745c3ea7a2' --open\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xfcafcc6800000000000000000000000000000000000000000000000000000000' --give (cdp id)\n               THEN topics[OFFSET(2)]\n          WHEN topics[OFFSET(0)] = '0x45e6bdcd00000000000000000000000000000000000000000000000000000000' --frob\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x9bb8f83800000000000000000000000000000000000000000000000000000000' --flux (cpd id)\n               THEN topics[OFFSET(2)]\n          WHEN topics[OFFSET(0)] = '0xf9f30db600000000000000000000000000000000000000000000000000000000' --move (cdp id)\n               THEN topics[OFFSET(2)]\n          WHEN topics[OFFSET(0)] = '0x1b0dbf7200000000000000000000000000000000000000000000000000000000' --quit (cdp id)\n               THEN topics[OFFSET(2)]\n                    END AS sender,\n     CASE\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' --transfer/burn/mint\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x8c5be1e5ebec7d5bd14f71427d1e84f3dd0314c0f7b2291e5b200ac8c7c3b925' --approve\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xbb35783b00000000000000000000000000000000000000000000000000000000' --move(vat)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x7cdd3fde00000000000000000000000000000000000000000000000000000000' --slip(vat)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x7bab3f4000000000000000000000000000000000000000000000000000000000' --grab (w; user recieving debt)\n               THEN CONCAT('0x', SUBSTR(data, 355, 40)) --retrieve address from data (not logged)\n          WHEN topics[OFFSET(0)] = '0x85258d09e1e4ef299ff3fc11e74af99563f022d21f3f940db982229dc2a3358c' --bark (auction id)\n               THEN SAFE_CAST(SAFE_CAST(topics[OFFSET(3)] AS INT64) AS STRING)\n          WHEN topics[OFFSET(0)] = '0x7c5bfdc0a5e8192f6cd4972f382cec69116862fb62e6abff8003874c58e064b8' --kick (kpr of dai incentive)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(3)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x05e309fd6ce72f2ab888a20056bb4210df08daed86f21f95053deb19964d86b1' --take (auction id)\n               THEN SAFE_CAST(SAFE_CAST(topics[OFFSET(1)] AS INT64) AS STRING)\n          WHEN topics[OFFSET(0)] = '0x0d7d75e01ab95780d3cd1c8ec0dd6c2ce19e3a20427eec8bf53283b6fb8e95f0' --flashloan (token)\n               THEN CONCAT('0x', SUBSTR(data, 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x0f6798a560793a54c3bcfe86a93cde1e73087d944c0ea20544137d4121396885' --mint (DSTOKEN)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x4f0892983790f53eea39a7a496f6cb40e8811b313871337b6a761efc6c67bb1f' --etch\n               THEN topics[OFFSET(1)]\n          WHEN topics[OFFSET(0)] = '0xa69beaba00000000000000000000000000000000000000000000000000000000' --vote (slate)\n               THEN topics[OFFSET(2)]\n          WHEN topics[OFFSET(0)] = '0xd6be0bc178658a382ff4f91c8c68b542aa6b71685b8fe427966b87745c3ea7a2' --open (cdp id)\n               THEN topics[OFFSET(3)]\n          WHEN topics[OFFSET(0)] = '0xfcafcc6800000000000000000000000000000000000000000000000000000000' --give (dst address)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x45e6bdcd00000000000000000000000000000000000000000000000000000000' --frob (affected cdp)\n               THEN topics[OFFSET(2)]\n          WHEN topics[OFFSET(0)] = '0x9bb8f83800000000000000000000000000000000000000000000000000000000' --flux (dst)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(3)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xf9f30db600000000000000000000000000000000000000000000000000000000' --move (dst)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(3)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x1b0dbf7200000000000000000000000000000000000000000000000000000000' --quit (dst)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(3)], 27, 40)) --remove zero padding\n                    END AS reciever,\n     CASE WHEN topics[OFFSET(0)] IN ('0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef',\n                                     '0x7f8661a100000000000000000000000000000000000000000000000000000000',\n                                     '0x049878f300000000000000000000000000000000000000000000000000000000',\n                                     '0x7cdd3fde00000000000000000000000000000000000000000000000000000000',\n                                     '0x7bab3f4000000000000000000000000000000000000000000000000000000000',\n                                     '0x85258d09e1e4ef299ff3fc11e74af99563f022d21f3f940db982229dc2a3358c',\n                                     '0xcc16f5dbb4873280815c1ee09dbd06736cffcc184412cf7a71a0fdb75d397ca5',\n                                     '0x0f6798a560793a54c3bcfe86a93cde1e73087d944c0ea20544137d4121396885',\n                                     '0x45e6bdcd00000000000000000000000000000000000000000000000000000000',\n                                     '0x9bb8f83800000000000000000000000000000000000000000000000000000000'\n                                   )\n               THEN 'wad'\n          WHEN topics[OFFSET(0)] IN ('0x05e309fd6ce72f2ab888a20056bb4210df08daed86f21f95053deb19964d86b1',\n                                     '0x0d7d75e01ab95780d3cd1c8ec0dd6c2ce19e3a20427eec8bf53283b6fb8e95f0'\n                                   )\n               THEN 'ray'\n          WHEN topics[OFFSET(0)] IN ('0xbb35783b00000000000000000000000000000000000000000000000000000000',\n                                     '0xf37ac61c00000000000000000000000000000000000000000000000000000000',\n                                     '0x7c5bfdc0a5e8192f6cd4972f382cec69116862fb62e6abff8003874c58e064b8',\n                                     '0xf9f30db600000000000000000000000000000000000000000000000000000000'\n                                   )\n               THEN 'rad'\n                    END AS percision, --see value section\n     topics,\n     data\nFROM bigquery-public-data.crypto_ethereum.logs AS logs\nRIGHT JOIN bigquery-public-data.crypto_ethereum.transactions AS transactions\n     ON logs.transaction_hash = transactions.hash\nWHERE logs.block_timestamp BETWEEN '2021-05-30' and '2022-05-30'\n     AND address IN (\n     '0x6b175474e89094c44da98b954eedeac495271d0f', -- DAI address (ERC20 form)\n     '0x9759A6Ac90977b93B58547b4A71c78317f391A28', -- DAIJOIN address (mint/burn)\n     '0x2F0b23f53734252Bda2277357e97e1517d6B042A', -- GEMJOIN address (collateral)\n     '0x197E90f9FAD81970bA7976f33CbD77088E5D7cf7', -- POT address (savings contract)\n     '0x19c0976f590D67707E62397C87829d896Dc0f1F1', -- JUG address (stability fees)\n     '0xA950524441892A31ebddF91d3cEEFa04Bf454466', -- VOW address (balance sheet)\n     '0xC4269cC7acDEdC3794b221aA4D9205F564e27f0d', -- FLAPPER address (surplus)\n     '0xA41B6EF151E06da0e34B009B86E828308986736D', -- FLOPPER address (deficit - never been called thus far)\n     '0x35D1b3F3D7966A1DFe207aa4514C12a259A0492B', -- VAT address (vaults)\n     '0x135954d155898D42C90D2a57824C690e0c7BEf1B', -- DOG address (liquidations)\n     '0xc67963a226eddd77B91aD8c421630A1b0AdFF270', -- CLIPPER address (liquidations)\n     '0x1EB4CF3A948E7D72A198fe073cCb8C7a948cD853', -- FLASH address (flash loans)\n     '0x9f8F72aA9304c8B593d555F12eF6589cC3A579A2', -- MKR address (governance ERC20)\n     '0x0a3f6849f78076aefaDf113F5BED87720274dDC0', -- DSCHIEF address (governance voting system)\n     '0xA618E54de493ec29432EbD2CA7f14eFbF6Ac17F7', -- DSTOKEN address (governance IOU)\n     '0x82ecD135Dce65Fbc6DbdD0e4237E0AF93FFD5038', -- PROXY address (for proxy actions)\n     '0x5ef30b9986345249bc32d8928B7ee64DE9435E39'  -- CDPMANAGER address (for managing vaults)\n     )\n\n)"
  },
  {
    "objectID": "posts/MakerDAO_dataset_generation_report.html#references",
    "href": "posts/MakerDAO_dataset_generation_report.html#references",
    "title": "Network Mapping the Most Influential Addresses using DAI (2021): Extracting Blockchain Data from Public Google Datasets with BigQuery and SQL",
    "section": "References",
    "text": "References\nMakerDAO (2021). MakerDAO Technical Docs. URL https://docs.makerdao.com/\nGoogle (2021). BigQuery. URL https://cloud.google.com/bigquery"
  },
  {
    "objectID": "posts/video_game_analysis/video_game_analysis_2024_report.html",
    "href": "posts/video_game_analysis/video_game_analysis_2024_report.html",
    "title": "Shiny App of Metacritic Featured Games Throughout Time",
    "section": "",
    "text": "Gaming is a rapidly growing industry with a consumer history that stretches back to the 1970s. Throughout the years many games have been released that have had profound impact on the gaming community. To sift through the quantity of releases, video game critics have established themselves as core members of the gaming ecosystem. Metacritic is a website that aggregates video game (among other mediums) reviews and has proven to be highly influential.\nTaking the history of games featured on Metacritic and diving into the data can reveal many insights into the historic interplay between games and consumer opinion. It can also help in suggesting high quality games to play (which may be my main motivator for this analysis…). Using Shiny can aid in this exploration by facilitating dynamic viewpoints of the data."
  },
  {
    "objectID": "posts/video_game_analysis/video_game_analysis_2024_report.html#introduction",
    "href": "posts/video_game_analysis/video_game_analysis_2024_report.html#introduction",
    "title": "Shiny App of Metacritic Featured Games Throughout Time",
    "section": "",
    "text": "Gaming is a rapidly growing industry with a consumer history that stretches back to the 1970s. Throughout the years many games have been released that have had profound impact on the gaming community. To sift through the quantity of releases, video game critics have established themselves as core members of the gaming ecosystem. Metacritic is a website that aggregates video game (among other mediums) reviews and has proven to be highly influential.\nTaking the history of games featured on Metacritic and diving into the data can reveal many insights into the historic interplay between games and consumer opinion. It can also help in suggesting high quality games to play (which may be my main motivator for this analysis…). Using Shiny can aid in this exploration by facilitating dynamic viewpoints of the data."
  },
  {
    "objectID": "posts/video_game_analysis/video_game_analysis_2024_report.html#methodology",
    "href": "posts/video_game_analysis/video_game_analysis_2024_report.html#methodology",
    "title": "Shiny App of Metacritic Featured Games Throughout Time",
    "section": "Methodology",
    "text": "Methodology\nMetacritic data was obtained through Kaggle as a publicly available dataset (link: https://www.kaggle.com/datasets/beridzeg45/video-games). Data was obtained by the publisher through scraping the Metacritic website using Python libraries.\nData was then enriched through personal knowledge to create a more descriptive dataset. In brief, genres were condensed into 12 categories.\n\n  mutate(\n    ...,\n    genre = case_when(\n      str_detect(genre, \"RPG\") ~ \"RPG\",\n      str_detect(genre, \"Adventure|Platform|Metroid|Point\") ~ \"Adventure\",\n      str_detect(genre, \"Strategy|RTS|Tactic|MOBA|Defense\") ~ \"Strategy\",\n      str_detect(genre, \"Racing\") ~ \"Racing\",\n      str_detect(genre, \"Fight\") ~ \"Fighter\",\n      str_detect(genre, \"Action|Beat|Arcade|Sandbox|Rougelike\") ~ \"Action\",\n      str_detect(genre, \"Shoot|FPS|Gun|Artillery\") ~ \"Shooter\",\n      str_detect(genre, \"Sim|Virtual|Manage|Tycoon|Visual Novel\") ~ \"Sim\",\n      str_detect(genre, \"Puzzle|Hidden|Trivia|Edutainment\") ~ \"Puzzle\",\n      str_detect(genre, \"Sport|Ball|ball|Golf|Ski|Wrest|Rugb|Hock|Billi|Tenn|Socc|Bik|Skat|Hunt|Surf|Crick|Bowl|Fish|Athlet|Gambl\") ~ \"Sport\",\n      str_detect(genre, \"Danc|Party|Rhythm|Exercise\") ~ \"Party\",\n      str_detect(genre, \"Survival\") ~ \"Survival\",\n      TRUE ~ \"Other\"\n    ),\n    ...\n  )\n\nPlatform versions were grouped into console families.\n\n  mutate(\n    platform_condensed = case_when(\n      str_detect(platform, \"iOS\") ~ \"Phone\",\n      str_detect(platform, \"Xbox\") ~ \"Xbox\",\n      str_detect(platform, \"Playstation|PlayStation|PSP\") ~ \"Playstation\",\n      str_detect(platform, \"Game|Wii|Nintendo|DS\") ~ \"Nintendo\",\n      str_detect(platform, \"Dreamcast\") ~ \"Dreamcast\",\n      str_detect(platform, \"PC\") ~ \"PC\",\n      str_detect(platform, \"Meta Quest\") ~ \"Meta VR\",\n      TRUE ~ \"Other\"\n    ),\n    ...\n  )\n\nLocality of each platform was annotated.\n\n  mutate(\n    ...,\n    locality = case_when(\n      str_detect(platform, \"iOS|PSP|DS|Game Boy\") ~ \"Mobile\",\n      TRUE ~ \"Home Console\"\n    ),\n    ...\n  )\n\nExclusivity was determined by marking duplicated titles.\n\n  mutate(\n    ...,\n    exclusivity = case_when(\n    duplicated(title) | duplicated(title, fromLast = TRUE) ~ \"Multi-Platform\",\n    TRUE ~ \"Exclusive\"\n    ),\n    ...\n  )\n\nScores were annotated with colloquial interpretations to give a more accurate representation of sentiment.\n\n  mutate(\n    ...,\n    metacritic_score = as.numeric(metacritic_score),\n    metacritic_score_factorized = case_when(\n      metacritic_score &lt;= 50 ~ \"Poor\",\n      metacritic_score &gt; 50 & metacritic_score &lt;= 70 ~ \"Average\",\n      metacritic_score &gt; 70 & metacritic_score &lt;= 90 ~ \"Good\",\n      metacritic_score &gt; 90 ~ \"Excellent\",\n      TRUE ~ \"Unscored\"\n    ),\n    ...\n  )\n\nIndependence was defined as publishers with only one developer under them. It should be noted that the validity of this categorization increases with time as successful indie companies eventually become commercial.\n\n  group_by(\n    publisher\n  ) |&gt;\n  mutate(\n    total_developer_under_publisher = n_distinct(developer)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    independance = case_when(\n      total_developer_under_publisher == 1 ~ \"Indie\",\n      TRUE ~ \"Commercial\"\n    )\n  )\n\nThe full data cleaning manipulation procedure can be found on my github: https://github.com/Spingelhoff/video_game_analysis_2024/blob/main/video_game_analysis_2024.R\nCleaned data was then showcased through Shiny. Number of games featured were divided by genre and color coded by colloquial score categories in a stacked bar graph. Distribution of titles by user score and Metacritic score were plotted as a scatterplot. Both graphs were connected interactively via ggiraph. By hovering over each genre bar, the distribution of points on the score scatterplot corresponding to the genre selected is highlighted.\n\noutput$combined_plot &lt;- renderGirafe({\n    genre_plot &lt;- ggplot(\n      yearly_genre_ratings(),\n      aes(x = fct_rev(genre), y = n_scored, fill = metacritic_score_factorized, data_id = genre)) +\n      geom_col_interactive(color = \"black\") +\n      coord_flip() +\n      theme_classic() +\n      scale_x_discrete(expand = c(0,0)) +\n      scale_y_continuous(expand = c(0,0)) +\n      scale_fill_brewer(palette = \"RdBu\", direction = -1) +\n      theme(\n        legend.position = \"bottom\",\n        text = element_text(size = 10)\n      ) +\n      xlab(\"# of Games\") +\n      ylab(\"Genre\") +\n      labs(\n        fill = \"Scoring\"\n      )\n    \n    ambivalence_plot &lt;- ggplot(\n      yearly_ambivalence(),\n      aes(x = avg_user_score, y = avg_metacritic_score, size = critic_coverage, alpha = critic_coverage, color = classification, data_id = genre)) +\n      geom_point_interactive(\n        aes(tooltip = paste0(\n          \"Title: \", title, \"\\n\",\n          \"Genre: \", genre, \"\\n\",\n          \"Critic Score: \", avg_metacritic_score, \"\\n\",\n          \"User Score:\", avg_user_score, \"\\n\")\n        )\n      ) +\n      theme_classic() +\n      scale_color_manual(\n        name = \"Preference\",\n        values = c(\"#4F7EBB\", \"grey\", \"#BC3D41\")) +\n      scale_size_continuous(name = \"Coverage\") +\n      scale_alpha_continuous(name = \"Coverage\") +\n      theme(\n        legend.position = \"bottom\",\n        text = element_text(size = 10)\n      ) +\n      xlab(\"User Score\") +\n      ylab(\"Metacritic Score\") +\n      guides(\n        color = \"none\"\n      )\n    \n    girafe(ggobj = genre_plot + ambivalence_plot, width_svg = 12, height_svg = 5) |&gt;\n      girafe_options(\n        opts_hover(css = \"stroke:black;stroke-width:2px;fill-opacity:1;\"),\n        opts_hover_inv(css = \"fill-opacity:0.2\"),\n        opts_selection(css = \"stroke:black;stroke-width:2px;fill-opacity:1;\",\n                       type = \"multiple\"),\n        opts_selection_inv(css = \"fill-opacity:0.2\")\n      )\n  })\n\nTop 5 Metacritic and user scored games as well as most divergently scored Metacritic and user scored games were shown through tables. Finally, locality, exclusivity and independence of titles were plotted as pie charts."
  },
  {
    "objectID": "posts/video_game_analysis/video_game_analysis_2024_report.html#references",
    "href": "posts/video_game_analysis/video_game_analysis_2024_report.html#references",
    "title": "Shiny App of Metacritic Featured Games Throughout Time",
    "section": "References",
    "text": "References\nChang W, Cheng J, Allaire J, Sievert C, Schloerke B, Xie Y, Allen J, McPherson J, Dipert A, Borges B (2022). shiny: Web Application Framework for R. R package version 1.7.4, https://CRAN.R-project.org/package=shiny.\nDavid Gohel and Panagiotis Skintzos (2024). ggiraph: Make ‘ggplot2’ Graphics Interactive. R package version 0.8.9. https://CRAN.R-project.org/package=ggiraph\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686."
  },
  {
    "objectID": "posts/revenue_exploration_app/revenue_data_exploration_app_report.html",
    "href": "posts/revenue_exploration_app/revenue_data_exploration_app_report.html",
    "title": "Creating a Data Exploration Tool for Revenue Data Using Shiny",
    "section": "",
    "text": "Shiny provides a framework for rapid development of data-oriented applications. This can be particularly useful when combined with data manipulation frameworks. Leveraging both technologies it is possible to disseminate substantial amounts of aggregated data, in a readable, interactive way, for domain experts to make use of.\nIn order to demonstrate the functionality of R, and particularly the Shiny framework, to the workplace, I took some of my own time to create a demo app summarizing and visualizing sales data. The variables and comparisons implemented for exploration were based on current workplace data availability. The app was then ported to a desktop format using the DesktopDeployR framework to avoid IT security bureaucracy."
  },
  {
    "objectID": "posts/revenue_exploration_app/revenue_data_exploration_app_report.html#introduction",
    "href": "posts/revenue_exploration_app/revenue_data_exploration_app_report.html#introduction",
    "title": "Creating a Data Exploration Tool for Revenue Data Using Shiny",
    "section": "",
    "text": "Shiny provides a framework for rapid development of data-oriented applications. This can be particularly useful when combined with data manipulation frameworks. Leveraging both technologies it is possible to disseminate substantial amounts of aggregated data, in a readable, interactive way, for domain experts to make use of.\nIn order to demonstrate the functionality of R, and particularly the Shiny framework, to the workplace, I took some of my own time to create a demo app summarizing and visualizing sales data. The variables and comparisons implemented for exploration were based on current workplace data availability. The app was then ported to a desktop format using the DesktopDeployR framework to avoid IT security bureaucracy."
  },
  {
    "objectID": "posts/revenue_exploration_app/revenue_data_exploration_app_report.html#methodology",
    "href": "posts/revenue_exploration_app/revenue_data_exploration_app_report.html#methodology",
    "title": "Creating a Data Exploration Tool for Revenue Data Using Shiny",
    "section": "Methodology",
    "text": "Methodology\nThe data used by the app is synthetic, but the organization is strongly influenced by the format of the data currently obtainable. The benefit of this is two-fold. First, the structure of data is familiar which aids in communication of app functions (i.e., employees can extrapolate how the data is crunched as they have worked with it before). Second, by forming a template of minimally required information, new data can more easily be coerced into an app compatible form (given all the necessary variables exist).\nA key feature of the app is the ability to optionally filter the data for time period and all categorical variables (Department, Taker, Salesman ID, Vendor and Customer) allowing the user control over the granularity of detail visualized. The ability to filter dates also allows for comparisons across time periods. Filtering of data was placed directly upstream of all visual/analytical endpoints (i.e., all visualizations are reactive and change to the same filtered dataset). Filtering placed on a collapsible sidebar to allow convenient manipulation.\n\nSales_Values &lt;- reactive({\n\ntempData &lt;- importedData %\\&gt;%\n\ndplyr::filter(Invoice.Date &gt;= input$MInput[1] & Invoice.Date &lt;= input$MInput[2])\n\nif (!is.null(input$DInput)) {\n\ntempData &lt;- tempData %\\&gt;%\n\nfilter(Department %in% input$DInput)\n\n}\n\nif (!is.null(input$TInput)) {\n\ntempData &lt;- tempData %\\&gt;%\n\nfilter(Taker %in% input$TInput)\n\n}\n\nif (!is.null(input$RInput)) {\n\ntempData &lt;- tempData %\\&gt;%\n\nfilter(Salesrep.Id %in% input$RInput)\n\n}\n\nif (!is.null(input$CInput)) {\n\ntempData &lt;- tempData %\\&gt;%\n\nfilter(Customer.Name %in% input$CInput)\n\n}\n\nif (!is.null(input$VInput)) {\n\ntempData &lt;- tempData %\\&gt;%\n\nfilter(Vendor %in% input$VInput)\n\n}\n\ntempData\n\n})\n\nDirectly right of the sidebar are three key summary stats on the filtered data (Revenue, Number of Invoices, Unique Customers). These are frozen on top of the screen as they are numbers that are very likely to be relevant to analysis.\nA collection of analytical visualizations was placed in a set of tabs. The Summary section expands the summary stats frozen at the top of the tab set. One key measure available here is a calculation of profit margin. By making the Summary section the default tab, computation is kept to a minimum allowing the app to respond quickly to user filters. Other tabs contain graphical interfaces to the data and take longer to respond – particularly if the data visualized is large.\nA combination of ggplot and plotly (mainly to convert the ggplot into an interactive plot) were used to create graphs that splay out data in a way that is permissive to exploration. Plotly interactivity features were key in unlocking the ability to drill down on data. The hope is that these plots will not only allow users to summarize data visually but also form further questions which can feed back into further analysis. A raw table was also included using the DT package.\nR is an open-source language geared towards data analysis but is unfortunately not ubiquitous in business. IT bureaucracy is notorious in approving new technologies and with shiny being a webapp framework primarily – which requires investment of web infrastructure, quick deployment of solutions is all but impossible. To circumvent this annoyance, I used DesktopDeployR to convert the shiny app into a desktop app (essentially). Briefly, DesktopDeployR uses a combination of a portable R framework and scripts to run the shiny app from folder. Importantly, this framework does not require administrative privileges or effort on the IT teams’ part to safely deploy."
  },
  {
    "objectID": "posts/revenue_exploration_app/revenue_data_exploration_app_report.html#references",
    "href": "posts/revenue_exploration_app/revenue_data_exploration_app_report.html#references",
    "title": "Creating a Data Exploration Tool for Revenue Data Using Shiny",
    "section": "References",
    "text": "References\nC. Sievert. Interactive Web-Based Data Visualization with R, plotly, and shiny. Chapman and Hall/CRC Florida, 2020.\nChang W, Cheng J, Allaire J, Sievert C, Schloerke B, Xie Y, Allen J, McPherson J, Dipert A, Borges B (2022). shiny: Web Application Framework for R. R package version 1.7.4, https://CRAN.R-project.org/package=shiny.\nwleepang (2020) DesktopDeployR [Source code] https://github.com/wleepang/DesktopDeployR.\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686.\nXie Y, Cheng J, Tan X (2023). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.27, https://CRAN.R-project.org/package=DT."
  },
  {
    "objectID": "posts/WTO_export_map/WTO_export_map_report.html",
    "href": "posts/WTO_export_map/WTO_export_map_report.html",
    "title": "World Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet",
    "section": "",
    "text": "The recent pandemic has caused disruptions to global trade activities. Coupled with economic insecurity and rearranging world politics, market relations between countries have changed greatly from what we remember. In order to clarify the current state of global markets, data was retrieved from the World Trade Organization stats portal for exports by partner economies in 2021 and visualized on a map using leaflet. Reported trade connections were plotted alongside value to visualize potential associations between a country’s trade connectivity and export value. In this report, I will go over the methodology used to create the map as well as some insight the map provides."
  },
  {
    "objectID": "posts/WTO_export_map/WTO_export_map_report.html#introduction",
    "href": "posts/WTO_export_map/WTO_export_map_report.html#introduction",
    "title": "World Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet",
    "section": "",
    "text": "The recent pandemic has caused disruptions to global trade activities. Coupled with economic insecurity and rearranging world politics, market relations between countries have changed greatly from what we remember. In order to clarify the current state of global markets, data was retrieved from the World Trade Organization stats portal for exports by partner economies in 2021 and visualized on a map using leaflet. Reported trade connections were plotted alongside value to visualize potential associations between a country’s trade connectivity and export value. In this report, I will go over the methodology used to create the map as well as some insight the map provides."
  },
  {
    "objectID": "posts/WTO_export_map/WTO_export_map_report.html#methodology",
    "href": "posts/WTO_export_map/WTO_export_map_report.html#methodology",
    "title": "World Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet",
    "section": "Methodology",
    "text": "Methodology\nThe data used for constructing the map was sourced from the World Trade Organization, an international organization aimed towards promoting fair and free trade in global economies. In pursuit of this goal they conduct economic research with datasets freely available on their stat portal. Data of export value for agricultural and non-agricultural products by partner economy in 2021 was exported as a csv file and imported into R.\nBefore the data could be visualized, it had to be manipulated into a format that leaflet would accept. Unfortunately, the exported dataset contained extra indicators I did not select for. In addition, some characters were not correctly translated. Data was filtered on the relevant indicators and incorrectly formatted country names were individually transformed into a UTF-8 compliant form.\n\nfiltered_data &lt;- data %&gt;%\n  filter(Indicator == \"AG - Value of exports to partner (imports by partner)\" |\n         Indicator == \"Non-AG - Value of exports to partner (imports by partner)\") %&gt;%\n  select(Reporting.Economy, Partner.Economy, Value) %&gt;%\n  group_by(Reporting.Economy, Partner.Economy) %&gt;%\n  summarise(Value = sum(Value)) %&gt;%\n  mutate(Reporting.Economy = ifelse(Reporting.Economy == \"C\\xf4te d'Ivoire\",\n                                    \"Cote d'Ivoire\",\n                                    ifelse(Reporting.Economy == \"T\\xfcrkiye\",\n                                           \"Turkiye\",\n                                           Reporting.Economy)),\n         Partner.Economy = ifelse(Partner.Economy == \"C\\xf4te d'Ivoire\",\n                                  \"Cote d'Ivoire\",\n                                  ifelse(Partner.Economy == \"T\\xfcrkiye\",\n                                         \"Turkiye\",\n                                         ifelse(Partner.Economy == \"Sao Tom\\xe9 and Principe\",\n                                                \"Sao Tome and Principe\",\n                                                Partner.Economy))))\n\n\n\n# A tibble: 734 × 3\n# Groups:   Reporting.Economy [113]\n   Reporting.Economy   Partner.Economy           Value\n   &lt;chr&gt;               &lt;chr&gt;                     &lt;dbl&gt;\n 1 Albania             Cabo Verde              0.00339\n 2 Albania             Montenegro             20.9    \n 3 Angola              Bhutan                  0.0971 \n 4 Angola              Djibouti                2.78   \n 5 Angola              Gabon                   0.250  \n 6 Angola              Gibraltar               0.259  \n 7 Angola              Liberia                 7.30   \n 8 Angola              Sao Tome and Principe   0.103  \n 9 Angola              Togo                  952.     \n10 Antigua and Barbuda Dominica                1.06   \n# ℹ 724 more rows\n\n\nCoordinates for each country were retrieved from the Google Maps API using ggmap.\n\ngeolocated_data &lt;- filtered_data %&gt;%\n  mutate_geocode(Reporting.Economy) %&gt;%\n  rename(Reporting.lon = lon, Reporting.lat = lat) %&gt;%\n  mutate_geocode(Partner.Economy) %&gt;%\n  rename(Partner.lon = lon, Partner.lat = lat)\n\ngeolocated_data\n\nFinally, data was subset into several tables for future use in leaflet.\n\n##subset coordinate data to reporting country\nReporting_lonlat &lt;- geolocated_data %&gt;%\n  select(Reporting.Economy, Reporting.lon, Reporting.lat) %&gt;%\n  unique() %&gt;%\n  mutate(Grouping = \"World\")\n\n##add degree data for sizing\ndegree_data &lt;- geolocated_data %&gt;%\n  ungroup() %&gt;%\n  select(Reporting.Economy, Partner.Economy) %&gt;%\n  group_by(Reporting.Economy) %&gt;%\n  summarise(Degree = n())\n\n##Create world data frame for sizing\nworld_data &lt;- geolocated_data %&gt;%\n  summarize(Value = sum(Value)) %&gt;%\n  left_join(Reporting_lonlat) %&gt;%\n  left_join(degree_data)\n\n##combine data\ncombined_data &lt;- geolocated_data %&gt;%\n  left_join(degree_data, by = c(\"Partner.Economy\" = \"Reporting.Economy\"))\n\nLeaflet is a popular mapping JavaScript library which has been implemented in R and was used to visualize trade data on a world map. Functions for color scales were provided by leaflet and was used to segment data by five color quantiles based on export value. Trade connectivity (or degree), was visualized by an incrementally increasing circle marker radius centered on each country.\n\n##functions and preparation for leaflet\nquantile_pal &lt;- colorQuantile(\"Reds\", geolocated_data$Value, 5)\nquantile_pal_world &lt;- colorQuantile(\"Blues\", world_data$Value, 5)\ndegree_size &lt;- function(x) {\n  ifelse(x &gt; 40, 60,\n         ifelse(40 &gt; x & x &gt; 30, 50,\n                ifelse(30 &gt; x & x &gt; 20, 40,\n                       ifelse(20 &gt; x & x &gt; 10, 30, 20))))\n}\n\nA minimal background map was used from the provider tiles included in the leaflet package. Each country was then represented as circle markers with sizes that reflected their trade connectivity. The color of each circle marker is assigned based export value quantile (described previously). Each country was given their own overlay group showcasing reported export value by partner economy in red. Total export trade value for the selected country was shown in blue. A “world” group was added with only total export trade values for all reporting economies (this is the default for the map).\n\n##visualize with leaflet\nWTO_Export_Map &lt;- leaflet(combined_data) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\",\n                   options = providerTileOptions(minZoom = 0,\n                                                 maxZoom = 4)) %&gt;%\n  addCircleMarkers(lng = ~Partner.lon,\n                   lat = ~Partner.lat,\n                   group = ~Reporting.Economy,\n                   label = ~paste0(Partner.Economy,\n                                  \": \",\n                                  round(Value, 2),\n                                  \" Million\"),\n                   stroke = TRUE,\n                   color = ~quantile_pal(Value),\n                   radius = ~degree_size(Degree)) %&gt;%\n  addCircleMarkers(data = world_data,\n                   lng = ~Reporting.lon,\n                   lat = ~Reporting.lat,\n                   group = ~Reporting.Economy,\n                   label = ~paste0(Reporting.Economy,\n                                   \" Total Export Value: \",\n                                   round(Value, 2),\n                                   \" Million\"),\n                   color = ~quantile_pal_world(Value),\n                   radius = ~degree_size(Degree)) %&gt;%\n  addCircleMarkers(data = world_data,\n                   lng = ~Reporting.lon,\n                   lat = ~Reporting.lat,\n                   group = ~Grouping,\n                   label = ~paste0(Reporting.Economy,\n                                   \" Total Export Value: \",\n                                   round(Value, 2),\n                                   \" Million\"),\n                   color = ~quantile_pal_world(Value),\n                   radius = ~degree_size(Degree)) %&gt;%\n  addLegend(data = world_data,\n            pal = quantile_pal_world, \n            values = world_data$Value,\n            group = \"World\") %&gt;%\n  addLayersControl(overlayGroups = ~c(Reporting.Economy, \"World\")) %&gt;%\n  hideGroup(group = ~Reporting.Economy) %&gt;%\n  setMaxBounds(-190, -100, 190, 100) %&gt;%\n  setView(10, 10, 2)"
  },
  {
    "objectID": "posts/WTO_export_map/WTO_export_map_report.html#analysis",
    "href": "posts/WTO_export_map/WTO_export_map_report.html#analysis",
    "title": "World Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet",
    "section": "Analysis",
    "text": "Analysis\nVisualization of geographical data in an interactive format makes it easier to do exploratory data analysis of the dataset. It is obvious upon first glance at the “World” overlay group that trade connectivity is generally positively correlated with export value. This is expected as a greater number of trade relations should be correlated with greater export demand which naturally leads to greater export values. One exception may be Australia which seems to handle relatively small export values for its trade connectivity. A closer look at Australia’s trading partners suggest that this may be due to the lack of major export destinations such as the United States or European Union. In contrast, Vietnam, despite a lower number of export destinations, exported 25 times the value of Australia in 2021 (according to this dataset). This is plausibly, in part, due to its export relations with South Korea. It is important to remember while analyzing this dataset, that the pandemic was a time of great global change and trade relations are not yet likely to have been solidified. It would be interesting to redo this map for 2022 and see how trade relations may or may not have changed."
  },
  {
    "objectID": "posts/WTO_export_map/WTO_export_map_report.html#references",
    "href": "posts/WTO_export_map/WTO_export_map_report.html#references",
    "title": "World Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet",
    "section": "References",
    "text": "References\nD. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2. The R Journal, 5(1), 144-161. URL http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf\nJ. Cheng, B. Karambelkar and Y. Xie (2022). leaflet: Create Interactive Web Maps with the JavaScript ‘Leaflet’ Library. R package version 2.1.1 URL https://CRAN.R-project.org/package=leaflet\nR Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/\nR. Vaidyanathan, Y. Xie, J. Allaire, J. Cheng, C. Sievert, K. Russell (2021). htmlwidgets: HTML Widgets for R. R package version 1.5.4, https://CRAN.R-project.org/package=htmlwidgets\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\nWorld Trade Organization (2021). WTO Stats. http://stats.wto.org\nSpecial thanks to Tamim M. for discussion and review."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My journey began in a wet lab, studying epigenetics in C. elegans. This foundation ignited a passion for hypothesis-driven research. As I delved deeper into data analysis using R, a new world of possibilities opened up.\nDriven by curiosity, I immersed myself in the art and science of data manipulation, visualization, and modeling. Through hands-on projects, I solidify my understanding and share my learnings with the world.\nThis website is a testament to my data exploration. Here, I document my projects, providing detailed methodologies alongside results."
  },
  {
    "objectID": "about.html#analyst-researcher-data-specialist",
    "href": "about.html#analyst-researcher-data-specialist",
    "title": "About Me",
    "section": "",
    "text": "My journey began in a wet lab, studying epigenetics in C. elegans. This foundation ignited a passion for hypothesis-driven research. As I delved deeper into data analysis using R, a new world of possibilities opened up.\nDriven by curiosity, I immersed myself in the art and science of data manipulation, visualization, and modeling. Through hands-on projects, I solidify my understanding and share my learnings with the world.\nThis website is a testament to my data exploration. Here, I document my projects, providing detailed methodologies alongside results."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\nGuillevin International | Data Specialist | 2022-2024\nLeveraged data analytics and automation to drive operational efficiency, customer engagement, and financial performance across various departments, resulting in significant cost savings and revenue generation.\n\n\nFreelance | Statistical/R Programmer | 2021 - 2022\nEnhanced data efficiency through automation, streamlining data extraction, cleaning, and analysis processes for clients\n\n\nUniversity of Toronto | Research & Teaching Assistant | 2018 - 2021\nConducted comprehensive research on histone-methyl-binding proteins in C. elegans, resulting in a published thesis and methodology paper, while simultaneously mentoring undergraduate students in laboratory techniques"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nUniversity of Toronto, St. George | Toronto, ON | Master of Cell and Systems Biology | 2018-2020\n\n\nUniversity of Toronto, St. George | Toronto, ON | Honors Bachelors of Science | 2014-2018"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Portfolio Site",
    "section": "",
    "text": "Welcome to my site. I’m excited you’re here! This is where I publish my journey in data, learning and life. Please feel free to take a look around."
  },
  {
    "objectID": "index.html#naviation",
    "href": "index.html#naviation",
    "title": "A Portfolio Site",
    "section": "Naviation",
    "text": "Naviation\n\n\nAbout Me\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nDiscover my journey and what I can do\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nExplore a curated collection of recent projects\n\n\nReports\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nUnderstand my approach to data and analytics"
  },
  {
    "objectID": "posts/leviton_crossreference_scrape.html",
    "href": "posts/leviton_crossreference_scrape.html",
    "title": "Using Rvest to Create a Product Code Translator",
    "section": "",
    "text": "Webpages remain a key source of information in the digital age (think Wikipedia). For repetitive tasks is inefficient to manually interact with these sources of information. Thankfully, web scraping tools exist that help to make retrieving information from the internet easier.\nA recent problem I came across was the conversion of product codes from one manufacturer to another. Different vendors have different naming conventions for the same part. These vendors typically have online resources for converting competitor product names into their own. To accomplish the task of converting our own product list, I developed a small scrapper using the “rvest” package to automate translation."
  },
  {
    "objectID": "posts/leviton_crossreference_scrape.html#introduction",
    "href": "posts/leviton_crossreference_scrape.html#introduction",
    "title": "Using Rvest to Create a Product Code Translator",
    "section": "",
    "text": "Webpages remain a key source of information in the digital age (think Wikipedia). For repetitive tasks is inefficient to manually interact with these sources of information. Thankfully, web scraping tools exist that help to make retrieving information from the internet easier.\nA recent problem I came across was the conversion of product codes from one manufacturer to another. Different vendors have different naming conventions for the same part. These vendors typically have online resources for converting competitor product names into their own. To accomplish the task of converting our own product list, I developed a small scrapper using the “rvest” package to automate translation."
  },
  {
    "objectID": "posts/leviton_crossreference_scrape.html#repository-link",
    "href": "posts/leviton_crossreference_scrape.html#repository-link",
    "title": "Using Rvest to Create a Product Code Translator",
    "section": "Repository Link",
    "text": "Repository Link\nhttps://github.com/Spingelhoff/leviton_crossreference_scraper"
  },
  {
    "objectID": "posts/leviton_crossreference_scrape.html#methodology",
    "href": "posts/leviton_crossreference_scrape.html#methodology",
    "title": "Using Rvest to Create a Product Code Translator",
    "section": "Methodology",
    "text": "Methodology\nThe goal of the project was to create a re-usable system that would be able to read an arbitrary amount of source items and convert them into the closest equivalent of another vendor. The fileR package (personal package) was used to standardize input and output into directories. The rvest package was used to extract relevant information (in this case the equivalent product code).\nThe reference manual was served on static pages. The URL changed predictably with each search query. This allowed for the programmatic generation of URL links from a list of source items.\n\naccess_lev_reference &lt;- function(product) {\n\n  html_search_start &lt;- \"https://www.leviton.com/en/support/resources-tools/manufacturer-cross-reference?id-upc=\"\n  html_search_end &lt;- \"&itemsPerPage=1&page=1\"\n\n  html_link &lt;- str_c(\n    html_search_start,\n    product,\n    html_search_end\n  )\n\n  read_html(html_link)\n}\n\nThis collection of links can then be scraped for translations by targeting the relevant element (using CS selectors).\n\ncrawl_nodes_for_lev &lt;- function(x) {\n  result &lt;- x |&gt;\n    html_elements(\".ss-md.product__title\") |&gt;\n    html_element(\"a\") |&gt;\n    html_attr(\"href\") |&gt;\n    str_remove(\"/en/products/\") |&gt;\n    str_to_upper() |&gt;\n    as.character() # coerce from list to character to allow for row binding\n\n  prefixed_result &lt;- str_c(\n    \"LEV-\",\n    result\n  )\n\n  standardized_result &lt;- if(!length(prefixed_result) == 1) {\n    \"NO MATCH FOUND\"\n  } else {\n    prefixed_result\n  }\n}"
  },
  {
    "objectID": "posts/leviton_crossreference_scrape.html#citations",
    "href": "posts/leviton_crossreference_scrape.html#citations",
    "title": "Using Rvest to Create a Product Code Translator",
    "section": "Citations",
    "text": "Citations\nLao V (2023) fileR: A Pipeable Interface To Directory Creation and Use. R package, https://github.com/Spingelhoff/fileR.\nWickham H (2022). rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.3, https://CRAN.R-project.org/package=rvest."
  },
  {
    "objectID": "posts/rental_price_analysis/rental_analysis_article.html",
    "href": "posts/rental_price_analysis/rental_analysis_article.html",
    "title": "Investigating the Determinants of Rent in Canadian Provinces",
    "section": "",
    "text": "Soaring rent prices across Canada have piqued my interest in understanding the key factors that influence them. Gaining a deeper understanding of these determinants would empower me to make more informed decisions when evaluating economic indicators related to rent.\nTo delve into this issue, I adopted a theoretical foundation, constructing a causal model. Leveraging data from the Canada Mortgage and Housing Corporation (CMHC) and Statistics Canada, I employed linear regression with fixed effects for province and year. This approach allowed me to isolate and measure the independent effects of various factors on rent prices."
  },
  {
    "objectID": "posts/rental_price_analysis/rental_analysis_article.html#introduction",
    "href": "posts/rental_price_analysis/rental_analysis_article.html#introduction",
    "title": "Investigating the Determinants of Rent in Canadian Provinces",
    "section": "",
    "text": "Soaring rent prices across Canada have piqued my interest in understanding the key factors that influence them. Gaining a deeper understanding of these determinants would empower me to make more informed decisions when evaluating economic indicators related to rent.\nTo delve into this issue, I adopted a theoretical foundation, constructing a causal model. Leveraging data from the Canada Mortgage and Housing Corporation (CMHC) and Statistics Canada, I employed linear regression with fixed effects for province and year. This approach allowed me to isolate and measure the independent effects of various factors on rent prices."
  },
  {
    "objectID": "posts/rental_price_analysis/rental_analysis_article.html#results",
    "href": "posts/rental_price_analysis/rental_analysis_article.html#results",
    "title": "Investigating the Determinants of Rent in Canadian Provinces",
    "section": "Results",
    "text": "Results\n\nCausal Model\nMy analysis employed a causal model to investigate the factors influencing rent prices in Canada. This model proposes that rent prices are a consequence of three primary forces: supply, demand, and costs.\n\nSupply\nSupply reflects the availability and overall capacity of the rental market. I captured supply through two key variables. The first is vacancy rate, which indicates the percentage of rental units that are currently unoccupied. A higher vacancy rate signifies greater availability and potentially lower rent prices. The second variable is total rental units, which represents the total number of rental units available in a given location. A larger number of units suggests a more robust rental market supply, potentially lowering rent prices.\n\n\nDemand\nDemand reflects the pressure on rental units created by potential renters. I utilized two variables to capture demand. Population growth signifies an increase in the number of potential renters, potentially driving up demand and rent prices. The second variable is average wage, which serves as a proxy for renters’ spending power. Higher average wages suggest that renters may be able to afford higher rents. My model assumes a relatively stable ratio of renters to non-renters in the population.\n\n\nCost\nCosts represent the ongoing operational expenses undertaken by landlords, which can influence how they price their rentals. I captured costs through two variables. The first is mortgage burden, which reflects the relative cost of mortgage payments for landlords. I adjusted the mortgage interest paid by both the Consumer Price Index (CPI) and population to account for inflation and population growth. A higher mortgage burden could incentivize landlords to raise rents. The second variable is rental property expenses, which represent the cost of utilities (specifically, water and electricity) associated with maintaining a rental property. These are considered significant ongoing expenses for landlords.\n\n\nLocation\nLocation is an important factor that significantly influences rent prices. However, location specific effects are challenging to quantify directly within my model. I believe that location likely exerts an indirect influence on all three main categories identified: supply, demand, and costs. For example, desirable locations with limited land availability might have a lower vacancy rate and higher total rental unit costs, impacting both supply and costs. Similarly, location might influence population growth patterns and average wages, affecting demand.\n\n\nModel Diagram\nThe following diagram is a representation of the causal model presented above:\n\n\nCode\ndagify(\n  rent_price ~ supply + demand + cost,\n  supply ~ vacancy + rental_supply + location,\n  vacancy ~ location,\n  rental_supply ~ location,\n  demand ~ population + avg_hourly_wage + location,\n  population ~ location,\n  avg_hourly_wage ~ location,\n  cost ~ mortgage + electricity + water + location,\n  mortgage ~ location,\n  electricity ~ location,\n  water ~ location,\n  outcome = \"rent_price\",\n  labels = c(\n    rent_price = \"rent price\",\n    supply = \"supply\",\n    demand = \"demand\",\n    cost = \"cost\",\n    vacancy = \"vacancy\",\n    rental_supply = \"rental supply\",\n    population = \"population\",\n    avg_hourly_wage = \"average wage\",\n    mortgage = \"mortgage\",\n    water = \"water\",\n    electricity = \"electricity\",\n    location = \"location\"\n  ),\n  coords = list(\n    x = c(rent_price = 4, supply = 3, demand = 3, cost = 3, vacancy = 2, \n          rental_supply = 2, population = 2, avg_hourly_wage = 2, mortgage = 2, \n          water = 2, electricity = 2, location = 1),\n    y = c(rent_price = 4, supply = 1, demand = 4, cost = 7, vacancy = 1,\n          rental_supply = 2, population = 3, avg_hourly_wage = 5, mortgage = 6, \n          water = 7, electricity = 8, location = 4)\n  )\n) |&gt;\nggdag_status(\n  use_labels = \"label\", \n  text = FALSE,\n) + \n  theme_dag()\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Analysis\n\nFull Causal Rent Model\nI employed ordinary least squares (OLS) regression to investigate how the proposed rental market factors influence rent prices. To account for the influence of location that cannot be directly measured in my model, I fixed the regression on province. Additionally, year was included as a fixed effect to control for any time-varying factors that might affect rent prices. Furthermore, standard errors were clustered on both province and year for robust estimation.\n\n\nCode\ncausal_data_full &lt;- all_data |&gt;\n  filter(\n    room_type == \"all\",\n    year &lt; 2023\n  ) |&gt;\n  select(\n    year,\n    geography,\n    avg_rent_cpi_adj,\n    mortgage_interest_paid_cpi_pop_adj,\n    electricity_index,\n    water_index,\n    population,\n    avg_hourly_wage,\n    vacancy_rate,\n    rental_supply\n  )\n\ncausal_model_full &lt;- feols(\n  log(avg_rent_cpi_adj) ~ \n    log(mortgage_interest_paid_cpi_pop_adj) + log(population) +\n    vacancy_rate + log(rental_supply) + log(avg_hourly_wage) +\n    log(electricity_index) + log(water_index) | year + geography,\n  data = causal_data_full\n)\n\netable(causal_model_full, se = \"twoway\")\n\n\n                                            causal_model_full\nDependent Var.:                         log(avg_rent_cpi_adj)\n                                                             \nlog(mortgage_interest_paid_cpi_pop_adj)    0.4479*** (0.0858)\nlog(population)                             0.6408** (0.1520)\nvacancy_rate                                 -0.0069 (0.0044)\nlog(rental_supply)                          0.4592** (0.1240)\nlog(avg_hourly_wage)                         0.6153. (0.3101)\nlog(electricity_index)                       0.1548* (0.0642)\nlog(water_index)                              0.0437 (0.0487)\nFixed-Effects:                          ---------------------\nyear                                                      Yes\ngeography                                                 Yes\n_______________________________________ _____________________\nS.E.: Clustered                          by: year & geography\nObservations                                              220\nR2                                                    0.96319\nWithin R2                                             0.51192\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe analysis revealed that population growth had the strongest effect on rent prices. Surprisingly, the vacancy rate was not statistically significant implying that vacancy rate might not be a strong predictor of rent prices in my model. Additionally, both total rental units (representing supply) and population (representing demand) showed positive effects on rent price. Finally, the cost of water exhibited a negligible effect on rent price in terms of both magnitude and significance.\n\n\nTime Lagged Causal Rent Model\nRecognizing that the rental market may respond to economic factors with a delay, I implemented the same model using regressors lagged by one year. This allowed me to explore whether the effects of these factors take time to unfold and influence rent prices.\n\n\nCode\ncausal_data_lagged &lt;- all_data |&gt;\n  filter(room_type == \"all\") |&gt;\n  group_by(\n    geography\n  ) |&gt;\n  mutate(\n    lagged_mortgage_interest_paid_cpi_pop_adj = lag(mortgage_interest_paid_cpi_pop_adj, 1),\n    lagged_population = lag(population, 1),\n    lagged_vacancy_rate = lag(vacancy_rate, 1),\n    lagged_rental_supply = lag(rental_supply, 1),\n    lagged_avg_hourly_wage = lag(avg_hourly_wage, 1),\n    lagged_electricity_index = lag(electricity_index, 1),\n    lagged_water_index = lag(water_index, 1)\n  ) |&gt;\n  ungroup() |&gt;\n  select(\n    year,\n    geography,\n    avg_rent_cpi_adj,\n    contains(\"lagged\")\n  ) |&gt;\n  filter(year &gt; 2001)\n\ncausal_model_lagged &lt;- feols(\n  log(avg_rent_cpi_adj) ~ \n    log(lagged_mortgage_interest_paid_cpi_pop_adj) + log(lagged_population) +\n    lagged_vacancy_rate + log(lagged_rental_supply) + log(lagged_avg_hourly_wage) +\n    log(lagged_electricity_index) + log(lagged_water_index) | year + geography,\n  data = causal_data_lagged\n)\n\netable(causal_model_lagged, se = \"twoway\")\n\n\n                                                 causal_model_lagged\nDependent Var.:                                log(avg_rent_cpi_adj)\n                                                                    \nlog(lagged_mortgage_interest_paid_cpi_pop_adj)     0.4237** (0.0919)\nlog(lagged_population)                              0.5343* (0.1691)\nlagged_vacancy_rate                                -0.0149* (0.0046)\nlog(lagged_rental_supply)                           0.3586* (0.1223)\nlog(lagged_avg_hourly_wage)                         0.5725. (0.2984)\nlog(lagged_electricity_index)                       0.1520* (0.0575)\nlog(lagged_water_index)                              0.0323 (0.0446)\nFixed-Effects:                                 ---------------------\nyear                                                             Yes\ngeography                                                        Yes\n________________________________________       _____________________\nS.E.: Clustered                                 by: year & geography\nObservations                                                     220\nR2                                                           0.96550\nWithin R2                                                    0.53032\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe lagged model revealed some interesting findings. Wages exhibited the largest effect size, suggesting that wage growth might have a stronger influence on rent prices with a one-year lag. Notably, the vacancy rate, which was insignificant in the standard model, showed some level of significance in the lagged model. This suggests that changes in vacancy rates might have a delayed impact on rent prices.\nInterestingly, both the within and adjusted R-squared values were higher in the lagged model compared to the standard model. This indicates that the lagged model might explain a slightly larger portion of the variance in rent prices.\n\n\nResidual Analysis\nTo evaluate the performance of the standard and time-lagged causal rent models, a comprehensive residual analysis was conducted. Initial investigations focused on the distribution of standardized residuals for both models. While both distributions exhibited approximate normality, a slight positive skew was observed in each. Visual comparisons did not reveal substantial differences between the residual distributions.\n\n\nCode\nget_fitted_and_residuals &lt;- function(x, y) {\n  tibble(\n    id = 1:length(residuals(x)),\n    model = y,\n    residuals = residuals(x),\n    standardized_residuals = residuals(x) / sigma(x),\n    rescaled_residuals = sqrt(abs(standardized_residuals)),\n    fitted = fitted(x)\n  )\n}\n\ncausal_model_list &lt;- list(\n  causal_model_full = causal_model_full,\n  causal_model_lagged = causal_model_lagged\n)\n\nmodels_residuals &lt;- mapply(\n  get_fitted_and_residuals,\n  x = causal_model_list,\n  y = names(causal_model_list),\n  SIMPLIFY = FALSE\n) |&gt;\n  bind_rows()\n\nggplot(models_residuals, aes(x = standardized_residuals)) +\n  geom_histogram() +\n  facet_wrap(~model)\n\n\n\n\n\n\n\n\n\nTo further scrutinize residual normality, Q-Q plots were generated. These plots provided a clearer distinction between the models. The standard model demonstrated a closer adherence to the theoretical normal distribution compared to the lagged model. The latter exhibited more pronounced deviations, especially in the tails of the distribution.\n\n\nCode\nggplot(models_residuals, aes(sample = standardized_residuals)) +\n  geom_qq() +\n  geom_qq_line() +\n  facet_wrap(vars(model))\n\n\n\n\n\n\n\n\n\nThe independence of errors was assessed by examining the relationship between fitted values and residuals for both models. The presence of a non-linear trend in both scatterplots indicated a violation of the assumption of independent errors.\n\n\nCode\nggplot(models_residuals, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(model))\n\n\n\n\n\n\n\n\n\nHomoscedasticity, or the constant variance of errors, was investigated using scale-location plots. Although neither model perfectly met the assumption of homoscedasticity, as evidenced by deviations from the expected uniform mean trend, the observed variance did not appear excessively large.\n\n\nCode\nggplot(models_residuals, aes(x = fitted, y = rescaled_residuals)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(model))\n\n\n\n\n\n\n\n\n\nFinally, a visual comparison of the model fits was undertaken. The lagged model exhibited a superior fit, particularly during periods of rent price reversal, such as in Alberta during later years. This observation aligns with the higher R-squared value associated with the lagged model.\n\n\nCode\ndisplay_fit &lt;- function(model, title) {\n  fixest_data(model) |&gt;\n    mutate(\n      fitted_values = fitted(model)\n    ) |&gt;\n    select(\n      year,\n      geography,\n      avg_rent_cpi_adj,\n      fitted_values\n    ) |&gt;\n    ggplot(aes(x = year, y = avg_rent_cpi_adj)) +\n    geom_line(color = \"grey\") +\n    geom_line(mapping = aes(x = year, \n                            y = exp(fitted_values)),\n              linetype = \"dotted\") +\n    facet_wrap(~geography) +\n    labs(title = {{ title }})\n}\n\ndisplay_fit(causal_model_full, title = \"Full Model Fit\")\n\n\n\n\n\n\n\n\n\nCode\ndisplay_fit(causal_model_lagged, title = \"Lagged Model Fit\")\n\n\n\n\n\n\n\n\n\n\n\nModel Selection\nThe lagged causal rent model demonstrated superior performance compared to the standard model. This superiority is evidenced by a higher R-squared value and a more visually compelling fit to the data trends. Although the residual distribution for the lagged model exhibited some skewness, the deviation from normality was not severe. Intuitively, the lagged model aligns with the expectation that landlords require time to adjust rental prices in response to fluctuations in market conditions.\n\n\nRefining Selected Model\nWater costs was found to have an insignificant impact on rent prices within the lagged model. This suggests that the water costs may not be a crucial determinant of rental costs. To formally assess the significance of water costs, a Wald test was conducted. The results of the Wald test confirmed the insignificance of water costs. Consequently, the water index variable was removed from the model, and the model was re-estimated.\n\n\nCode\nwald(causal_model_lagged, \"lagged_water_index\", vcov = \"twoway\")\n\n\nWald test, H0: nullity of log(lagged_water_index)\n stat = 0.522823, p-value = 0.470568, on 1 and 182 DoF, VCOV: Clustered (year & geography).\n\n\nThe final model fit is presented below.\n\n\nCode\nrevised_lagged_model &lt;- feols(\n    log(avg_rent_cpi_adj) ~ \n    log(lagged_mortgage_interest_paid_cpi_pop_adj) + \n    log(lagged_population) + lagged_vacancy_rate + \n    log(lagged_rental_supply) + \n    log(lagged_avg_hourly_wage) +\n    log(lagged_electricity_index) | year + geography,\n    data = causal_data_lagged\n)\n\nsummary(revised_lagged_model, vcov = \"twoway\")\n\n\nOLS estimation, Dep. Var.: log(avg_rent_cpi_adj)\nObservations: 220\nFixed-effects: year: 22,  geography: 10\nStandard-errors: Clustered (year & geography) \n                                                Estimate Std. Error  t value\nlog(lagged_mortgage_interest_paid_cpi_pop_adj)  0.430208   0.090552  4.75096\nlog(lagged_population)                          0.534340   0.176338  3.03021\nlagged_vacancy_rate                            -0.014960   0.004484 -3.33629\nlog(lagged_rental_supply)                       0.327565   0.123064  2.66173\nlog(lagged_avg_hourly_wage)                     0.583879   0.288912  2.02096\nlog(lagged_electricity_index)                   0.177070   0.060695  2.91737\n                                                Pr(&gt;|t|)    \nlog(lagged_mortgage_interest_paid_cpi_pop_adj) 0.0010428 ** \nlog(lagged_population)                         0.0142419 *  \nlagged_vacancy_rate                            0.0087114 ** \nlog(lagged_rental_supply)                      0.0259719 *  \nlog(lagged_avg_hourly_wage)                    0.0740076 .  \nlog(lagged_electricity_index)                  0.0171050 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.034619     Adj. R2: 0.958349\n                 Within R2: 0.526195\n\n\nCode\ndisplay_fit(revised_lagged_model, title = \"Revised Model Fit\")"
  },
  {
    "objectID": "posts/rental_price_analysis/rental_analysis_article.html#discussion",
    "href": "posts/rental_price_analysis/rental_analysis_article.html#discussion",
    "title": "Investigating the Determinants of Rent in Canadian Provinces",
    "section": "Discussion",
    "text": "Discussion\n\nModel Comparison\nThis study compared two causal rent models: a standard model assuming a direct impact of economic factors on rent prices and a lagged model incorporating a potential one-year delay in the rental market’s response. I evaluated both models based on their fit to the data, residual diagnostics, and the significance of their coefficients.\nModel fit, as measured by R-squared, indicated minimal differences between the two models, with the lagged model showing slightly better performance. However, the residual analysis revealed a potential issue with the lagged model. While the deviation from normality in the residual distribution was not severe, it was more pronounced compared to the standard model. Other residual diagnostics did not present major concerns.\nDespite the normality issue in the residuals, the lagged model’s superior fit to the data led me to select it for further analysis. This suggests that economic factors likely influence rent prices with a time lag rather than instantaneously in the context of my data.\n\n\nRent Determinants Analysis\nThe initial rent model analysis provides valuable insights into the factors influencing rent prices in Canada. Six variables emerged as statistically significant determinants of rent prices: mortgage interest paid, population growth, rental supply, vacancy rate, average wage, and electricity index. Notably, water costs were found to be statistically insignificant and were subsequently removed from the model. While this finding suggests water costs may have a minimal impact on Canadian rent prices, it is essential to acknowledge the potential limitations of a relatively small sample size (220 observations), which might have influenced the results.\nAs anticipated, mortgage interest paid, population growth, and electricity costs exhibited positive relationships with rent prices. These findings align with the expectation that increased demand, represented by population growth, and higher operating costs, such as those associated with mortgage interest and electricity, exert upward pressure on rental rates. Conversely, the negative relationship between vacancy rate and rent price confirms the theoretical expectation that higher vacancy rates lead to lower rental prices as landlords compete for tenants.\nHowever, the positive relationship between rental supply and rent price contradicts the traditional economic model, which posits an inverse relationship between supply and price. This unexpected finding warrants further investigation. One possible explanation is a time lag in the adjustment of rental supply to changes in demand, particularly within Canada’s rapidly evolving housing market. It is plausible that the data captures a period of imbalance where rental supply has not yet caught up with surging demand, resulting in a temporary positive correlation. To gain a more comprehensive understanding of the relationship between rental supply and rent prices, further analysis with a larger dataset or a more specific timeframe is recommended."
  },
  {
    "objectID": "posts/rental_price_analysis/rental_analysis_article.html#methods",
    "href": "posts/rental_price_analysis/rental_analysis_article.html#methods",
    "title": "Investigating the Determinants of Rent in Canadian Provinces",
    "section": "Methods",
    "text": "Methods\n\nData\nData for this analysis was retrieved from the CMHC and Statistics Canada APIs using the cmhc and statcan R packages. To facilitate modeling, the data was aggregated to a yearly format, condensing or summarizing any relevant variables.\nI employed factor-specific transformations where necessary. Rent prices were adjusted for inflation using the Consumer Price Index (CPI). Similarly, mortgage interest paid was adjusted by both CPI and population to account for inflation and population growth, resulting in a relative measure of the mortgage burden on landlords.\nTo investigate potential time-delayed effects, I created a separate dataset for a time-lagged rent model. In this dataset, all independent variables were lagged by one year.\n\n\nModel Estimation\nModels were estimated using the fixest package in R."
  },
  {
    "objectID": "posts/rental_price_analysis/rental_analysis_article.html#citations",
    "href": "posts/rental_price_analysis/rental_analysis_article.html#citations",
    "title": "Investigating the Determinants of Rent in Canadian Provinces",
    "section": "Citations",
    "text": "Citations\nBarrett M (2024). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.12, https://CRAN.R-project.org/package=ggdag.\nBerge L (2018). “Efficient estimation of maximum likelihood models with multiple fixed-effects: the R package FENmlm.” CREA Discussion Papers.\nvon Bergmann and Shkolnik (2021). cansim: Accessing Statistics Canada Data Table and Vectors. https://CRAN.R-project.org/package=cansim\nvon Bergmann J (2023). cmhc: Access, Retrieve, and Work with CMHC Data. R package version 0.2.7, https://CRAN.R-project.org/package=cmhc.\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686 https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html",
    "href": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html",
    "title": "Modeling Canadian Rent Data and Visualizing Using Power BI",
    "section": "",
    "text": "Rentals play an important role in providing housing for residents of Canada. Recent economic stresses have created volatility in the rental market highlighting the need for both renters and landlords to stay up to date on current factors driving rent prices. To address the need for information, I created a rental market dashboard with data from Statistics Canada and CMHC and forecasted rent prices in Power BI."
  },
  {
    "objectID": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html#introduction",
    "href": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html#introduction",
    "title": "Modeling Canadian Rent Data and Visualizing Using Power BI",
    "section": "",
    "text": "Rentals play an important role in providing housing for residents of Canada. Recent economic stresses have created volatility in the rental market highlighting the need for both renters and landlords to stay up to date on current factors driving rent prices. To address the need for information, I created a rental market dashboard with data from Statistics Canada and CMHC and forecasted rent prices in Power BI."
  },
  {
    "objectID": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html#methodology",
    "href": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html#methodology",
    "title": "Modeling Canadian Rent Data and Visualizing Using Power BI",
    "section": "Methodology",
    "text": "Methodology\nData was taken from APIs from Statistics Canada and CMHC portals. Data was transformed and rent was modeled. Best performing model was selected and used to forecast the next year. Data is then visualized in Power BI.\n\nObtaining data from Statistics Canada and CMHC\nStatistics Canada releases economic data and indicators for Canada. They make their data available through an API. cansim is an R wrapper around their API that makes it easier to retrieve standardized data in R. The Canadian Mortgage and Housing Corporation (CMHC) releases housing and rental data for Canada. This information is made available through their data portal. cmhc is an R package written to obtain data from that portal. A list of sgc codes is required for cmhc. A list of provinces (excluding territories) was provided.\nTime series of several economic indicators were taken from Statistics Canada in order to provide exogenous regressors for modeling. Data from CMHC, including rent prices, was taken and combined with data from CMHC on date, geography and rental room type.\n\n\nModeling Rent Prices\nRent price per year was adjusted by consumer price index to account for inflation. They were also transformed to percent rent price change per year to create a more stationary response variable for models that perform better on stationary data.\nRent price was modeled by ARIMA, ETS, linear regression, drift, mean and naive. Mean, naive and drift models were used as a baseline. Model performance was assessed by time series cross validation with a forecast of one year. Models were compared with RMSSE to account for scale differences between models trained on rent price vs those trained on percentage rent price per year (depending on model requirements).\n\nbase_models &lt;- tscv_model_data |&gt;\n  model(\n    mean = MEAN(avg_rent_cpi_adj_pct_change),\n    naive = NAIVE(avg_rent_cpi_adj_pct_change)\n  )\nrw_models &lt;- tscv_model_data |&gt;\n  model(\n    drift = RW(avg_rent_cpi_adj ~ drift())\n  )\narima_models &lt;- tscv_model_data |&gt;\n  model(\n    arima = ARIMA(avg_rent_cpi_adj_pct_change)\n  )  \nets_models &lt;- tscv_model_data |&gt;\n  model(\n    ets = ETS(avg_rent_cpi_adj)\n  )\nlinear_models &lt;- lagged_tscv_model_data |&gt;\n  model(\n    base_linear = TSLM(avg_rent_cpi_adj_pct_change ~ lagged_rental_supply_per_person_pct_change + lagged_five_year_mortgage_change),\n    trended_linear = TSLM(avg_rent_cpi_adj_pct_change ~ trend() + lagged_rental_supply_per_person_pct_change + lagged_five_year_mortgage_change)\n  )\n\nLjung box test was used to check residuals on the longest cross validation set. Models failing test were removed prior to model selection.\n\nget_ljung_box &lt;- function(model, lag) {\n  ljung_box_stats &lt;- model |&gt;\n    augment() |&gt;\n    features(.innov, ljung_box, lag = lag) |&gt;\n    filter(`.id` == max(`.id`, na.rm = TRUE)) |&gt; \n    select(-`.id`)\n  \n  return(ljung_box_stats)\n}\n\nbase_residuals &lt;- base_models |&gt;\n  get_ljung_box(lag = 4)\nrw_residuals &lt;- rw_models |&gt;\n  get_ljung_box(lag = 4)\narima_residuals &lt;- arima_models |&gt;\n  get_ljung_box(lag = 4)\nets_residuals &lt;- ets_models |&gt;\n  get_ljung_box(lag = 4)\nlinear_residuals &lt;- linear_models |&gt;\n  get_ljung_box(lag = 4)\n\n\n\nVisualizing Data in Power BI\nTo visualize rent data in Power BI, relevant tables were converted to excel format prior to ingestion.\n\nnamed_excel_table_list &lt;- list(\n  avg_rent = avg_rent_data,\n  provincial_room_indicators = yearly_provincial_room_indicators_data,\n  provincial_indicators = yearly_provincial_indicators_data,\n  indicators = yearly_indicators_data,\n  forecast = yearly_forecast_table\n)\n\ncreate_canadian_rent_excel_tables &lt;- function(named_excel_table_list) {\n  workbook &lt;- createWorkbook()\n  \n  table_names &lt;- names(named_excel_table_list)\n  \n  create_excel_pages_and_tables &lt;- function(table_name, table) {\n    addWorksheet(workbook, table_name)\n    writeDataTable(workbook, sheet = table_name, x = table, tableName = table_name)\n  }\n  \n  mapply(\n    create_excel_pages_and_tables,\n    table_name = table_names,\n    table = named_excel_table_list\n  )\n  \n  return(workbook)\n}\n\nexcel_workbook &lt;- named_excel_table_list |&gt;\n  create_canadian_rent_excel_tables()\n\nIn Power BI a new table was created to visualize forecasting data using DAX.\n\nforecast_viz = ADDCOLUMNS(\n  DISTINCT(\n    UNION(\n      SELECTCOLUMNS(\n        avg_rent, \"year\", avg_rent[year], \"geography\", avg_rent[Geography], \n        \"room_type\", avg_rent[room_type], \"Average Rent\", avg_rent[Rent], \n        \"Average Rent % Change\", [avg_rent_pct_change], \"80% Upper Confidence\",\n        avg_rent[Rent], \"Percent 80% Upper Confidence\", \n        avg_rent[avg_rent_pct_change], \"80% Lower Confidence\", avg_rent[Rent], \n        \"Percent 80% Lower Confidence\", avg_rent[avg_rent_pct_change]\n      ), \n      SELECTCOLUMNS(\n        forecast, \"year\", forecast[year], \"geography\", forecast[geography], \n        \"room_type\", forecast[room_type], \"Average Rent\", forecast[forecast], \n        \"Average Rent % Change\", forecast[pct_forecast], \"80% Upper Confidence\", \n        forecast[upper], \"Percent 80% Upper Confidence\", forecast[pct_upper], \n        \"80% Lower Confidence\", forecast[lower], \"Percent 80% Lower Confidence\", \n        forecast[pct_lower]\n      )\n    )\n  ), \n  \"geography_room\", CONCATENATE([geography], [room_type])\n)\n\nData model was organized as shown below:\n\n\n\n\n\n\n\n\n\nImportantly, several indicators were connected through a concatenation of geography and room type to ensure cross filtering would work across both variables."
  },
  {
    "objectID": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html#references",
    "href": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html#references",
    "title": "Modeling Canadian Rent Data and Visualizing Using Power BI",
    "section": "References",
    "text": "References\nO’Hara-Wild M, Hyndman R, Wang E (2024). fable: Forecasting Models for Tidy Time Series. R package version 0.3.4, https://CRAN.R-project.org/package=fable\nO’Hara-Wild M, Hyndman R, Wang E (2024). feasts: Feature Extraction and Statistics for Time Series. R package version 0.3.2, https://CRAN.R-project.org/package=feasts.\nvon Bergmann and Shkolnik (2021). cansim: Accessing Statistics Canada Data Table and Vectors. https://CRAN.R-project.org/package=cansim.\nvon Bergmann J (2023). cmhc: Access, Retrieve, and Work with CMHC Data. R package version 0.2.7, https://CRAN.R-project.org/package=cmhc.\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686."
  },
  {
    "objectID": "posts/fileR_report.html",
    "href": "posts/fileR_report.html",
    "title": "fileR: A Pipeable Interface to Directory Creation and Use",
    "section": "",
    "text": "The R programming language can be used for a variety of purposes. The use case I find most relevant to my day-to-day life (read day job), is the repetitive conversion of data from one form to another (typically for entry into a database). Without cooperate business infrastructure for R, most work needs to be done on my own workstation.\nMany great packages and ecosystems exist for data manipulation in R. One collection of packages which frequently enters my work is the tidyverse. A key feature of these collection of packages is the ability of functions to be piped together.\nThe fileR package is my attempt at a framework that helps to facilitate repetitive data wrangling by separating file input/outputs into two distinct directories, ‘data’ and ‘results’."
  },
  {
    "objectID": "posts/fileR_report.html#introduction",
    "href": "posts/fileR_report.html#introduction",
    "title": "fileR: A Pipeable Interface to Directory Creation and Use",
    "section": "",
    "text": "The R programming language can be used for a variety of purposes. The use case I find most relevant to my day-to-day life (read day job), is the repetitive conversion of data from one form to another (typically for entry into a database). Without cooperate business infrastructure for R, most work needs to be done on my own workstation.\nMany great packages and ecosystems exist for data manipulation in R. One collection of packages which frequently enters my work is the tidyverse. A key feature of these collection of packages is the ability of functions to be piped together.\nThe fileR package is my attempt at a framework that helps to facilitate repetitive data wrangling by separating file input/outputs into two distinct directories, ‘data’ and ‘results’."
  },
  {
    "objectID": "posts/fileR_report.html#rationaleframework",
    "href": "posts/fileR_report.html#rationaleframework",
    "title": "fileR: A Pipeable Interface to Directory Creation and Use",
    "section": "Rationale/Framework",
    "text": "Rationale/Framework\nR by default saves and reads files into and from the working directory. As the number of file inputs and outputs increase, the readability decreases. This can be a problem when returning to older projects.\nSub-folder and directories is the obvious answer to organization problems however, working with file paths is cumbersome and prone to error. Approaches to file organization should be standardized across projects so file inputs and outputs are more readily understood between projects.\nTo facilitate reproducible and easy file organization, I created a minimal framework for working directory structure. I then created wrappers around base R functions for reading and writing files to interact with that framework.\nThe working directory is divided into three sections. The working directory itself, the ‘data’ directory and the ‘results’ directory. The working directory should only contain R scripts and the ‘data’ and ‘results’ directories. The ‘data’ directory should only contain raw data. The ‘results’ directory should contain all products of manipulation in R. Folders (sub-directories) can be created within the ‘data’ and ‘results’ directory however, are recommended to be terminal."
  },
  {
    "objectID": "posts/fileR_report.html#methodology",
    "href": "posts/fileR_report.html#methodology",
    "title": "fileR: A Pipeable Interface to Directory Creation and Use",
    "section": "Methodology",
    "text": "Methodology\nWrappers were written around two base R functions: read.csv and write.csv. To ensure compatibility with the tidyverse set of packages, all functions that write files were made to be pipeable (i.e., the first argument is always an object that is returned back). Read functions are typically at the start of a pipe.\n\nsave_csv_to_results_folder &lt;- function(x, folder_name, file_name, ...) {\n  if(!dir.exists(\"results\")) {\n    stop(\"results directory does not exist\")\n  }\n  if(!is.character(folder_name) | !is.character(file_name)) {\n    stop(\"folder_name and file_name must be of type character\")\n  }\n  if(!dir.exists(paste0(\"./results/\", folder_name))) {\n    dir.create(paste0(\"./results/\", folder_name))\n  }\n  write.csv(x, paste0(\"./results/\", folder_name, \"/\", file_name, \".csv\"), ...)\n  invisible(x)\n}\n\nThe above ‘save_csv_to_results_folder’ function returns ‘x’ invisibly. One use case, would be to save intermediate results to a folder which allows you to produce multiple outputs in one pipe without having to exit the pipe.\nFunctions were also written to read data from entire folders and return them as a list. There is no function to recursively retrieve files from an entire directory as it should be clear where your data is coming from (i.e., you should state it in code).\n\nread_all_csv_from_data &lt;- function(folder_name = NULL, ...) {\n  if(!dir.exists(\"data\")) {\n    stop(\"data directory does not exist\")\n  }\n  if(!is.null(folder_name)) {\n    if(!is.character(folder_name)) {\n      stop(\"folder_name must be of type character\")\n    }\n    if(!dir.exists(paste0(\"./data/\", folder_name))) {\n      stop(paste0(folder_name, \" does not exist in the data directory\"))\n    }\n    file_list &lt;- list.files(paste0(\"./data/\", folder_name))\n    file_accumulator &lt;- vector(\"list\", length = length(file_list))\n    names(file_accumulator) &lt;- file_list\n    lapply(\n      file_list,\n      function(file_name){\n        file &lt;- read.csv(paste0(\"./data/\", folder_name, \"/\", file_name), ...)\n        file_accumulator[[file_name]] &lt;&lt;- file\n      }\n    )\n    return(file_accumulator)\n  } else {\n    file_list &lt;- list.files(\"./data\")\n    file_accumulator &lt;- vector(\"list\", length = length(file_list))\n    names(file_accumulator) &lt;- file_list\n    lapply(\n      file_list,\n      function(file_name){\n        file &lt;- read.csv(paste0(\"./data/\", file_name), ...)\n        file_accumulator[[file_name]] &lt;&lt;- file\n      }\n    )\n    return(file_accumulator)\n  }\n  return(file_accumulator)\n}\n\nThe above ‘read_all_csv_from_data’ initializes a list and with names based on the file names being read. Data is then read in and appended to the pre-existing list. This approach is to prevent growing a vector in R as it can cause drastic speed reductions for larger datasets.\nTo facilitate repetitive cleanups of folders and directories (e.g., cleaning out a data input folder to prevent processing data twice), functions were written to move batches of files between folders within a directory.\n\nmove_all_files_to_data_folder &lt;- function(to, exclude = NULL) {\n  if(!is.character(to)) {\n    stop(\"to parameter must be of type character\")\n  }\n  if(length(to) != 1) {\n    stop(\"to parameter must be of length 1\")\n  }\n  all_files &lt;- list.files(paste0(\"./data\"), recursive = TRUE, full.names = TRUE)\n  target_files &lt;- all_files[!grepl(paste0(\"^./data/\", to, \"/\"), all_files)]\n  if(!is.null(exclude)) {\n    if(is.character(exclude)) {\n      lapply(\n        exclude,\n        function(x) target_files &lt;&lt;- target_files[!grepl(paste0(\"^./data/\", x, \"/\"), target_files)]\n      )\n    } else {\n      stop(\"exclusion(s) must be of type character\")\n    }\n  }\n  file.copy(\n    from = target_files,\n    to = paste0(\"./data/\", to, \"/\")\n  )\n  from_test &lt;- list.files(\"./data\", recursive = TRUE)\n  if(!is.null(exclude)) {\n    if(is.character(exclude)) {\n      lapply(\n        exclude,\n        function(x) from_test &lt;&lt;- from_test[!grepl(paste0(\"^\", x, \"/\"), from_test)]\n      )\n    } else {\n      stop(\"exclusion(s) must be of type character\")\n    }\n  }\n  from_test &lt;- sub(\"^.*/\", \"\", from_test)\n  to_test &lt;- list.files(paste0(\"./data/\", to))\n  if(!all((from_test %in% to_test))) {\n    stop(\"files were not correctly copied\")\n  }\n  file.remove(target_files)\n  invisible(NULL)\n}\n\nIn the above function, files are copied to the new folder, checked, then deleted from the original folder. There is an implicit assumption in the way the check is done. The string cleanup assumes that the ‘data’ directory does not recurse more than one level. As a check with these assumptions is done, unintentional usage should generate an error. This is in line with the opinions of this package. The ‘to’ folder is excluded from the copy function to prevent unnecessary repetition however, further exclusions can be provided, optionally."
  },
  {
    "objectID": "posts/fileR_report.html#list-of-functions",
    "href": "posts/fileR_report.html#list-of-functions",
    "title": "fileR: A Pipeable Interface to Directory Creation and Use",
    "section": "List of Functions",
    "text": "List of Functions\nFunctions were named to be descriptive at the cost of being verbose.\n\ncreate_data_folder()\nPurpose: To create a folder in the ‘data’ directory. Data should be moved into pre-existing ‘data’ locations.\n\n\nlist_data_files()\nPurpose: To list files in ‘data’ directory and optionally a sub-directory of the ‘data’ directory.\n\n\nmove_all_files_to_data_folder()\nPurpose: To move all files recursively in the ‘data’ directory to a specified ‘data’ folder. Files in the target folder prior to move are not affected. Additional folders can be specified, optionally.\n\n\nmove_all_files_to_results_folder()\nPurpose: To move all files recursively in the ‘results’ directory to a specified ‘results’ folder. Files in the target folder prior to move are not affected. Additional folders can be specified, optionally.\n\n\nmove_files_between_data_folders()\nPurpose: To move all files in one ‘data’ folder to another ‘data’ folder.\n\n\nmove_files_between_results_folders()\nPurpose: To move all files in one ‘results’ folder to another ‘results’ folder.\n\n\nread_all_csv_from_data()\nPurpose: To return all csv files in the ‘data’ directory as a list. Sub-directories of the ‘data’ directory can also be targeted.\n\n\nread_all_csv_from_results()\nPurpose: To return all csv files in the ‘results’ directory as a list. Sub-directories of the ‘results’ directory should be targeted over the entire ‘results’ directory.\n\n\nread_csv_from_data()\nPurpose: To read a single csv file from the ‘data’ directory or one of its sub-directories.\n\n\nread_csv_from_results()\nPurpose: To read a single csv file from the ‘data’ directory or one of its sub-directories.\n\n\nsave_csv_to_results_folder()\nPurpose: To save an object to the ‘results’ directory or one of its sub-directories as a csv file. Will create a sub-directory or ‘folder’ if one does not exist. Will return the object passed in invisibly.\n\n\nsetup_fileR_directories()\nPurpose: Creates ‘data’ and ‘results’ directories. Projects using fileR should start with this function."
  },
  {
    "objectID": "listings.html",
    "href": "listings.html",
    "title": "Reports",
    "section": "",
    "text": "Investigating the Determinants of Rent in Canadian Provinces\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nVictor Lao\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Canadian Rent Data and Visualizing Using Power BI\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nVictor Lao\n\n\n\n\n\n\n\n\n\n\n\n\nShiny App of Metacritic Featured Games Throughout Time\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\nVictor Lao\n\n\n\n\n\n\n\n\n\n\n\n\nfileR: A Pipeable Interface to Directory Creation and Use\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2023\n\n\nVictor Lao\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Rvest to Create a Product Code Translator\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nVictor Lao\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Data Exploration Tool for Revenue Data Using Shiny\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nVictor Lao\n\n\n\n\n\n\n\n\n\n\n\n\nWorld Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nVictor Lao\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork Mapping the Most Influential Addresses Using DAI (2021)\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nVictor Lao\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork Mapping the Most Influential Addresses using DAI (2021): Extracting Blockchain Data from Public Google Datasets with BigQuery and SQL\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2022\n\n\nVictor Lao\n\n\n\n\n\n\nNo matching items"
  }
]
[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a cell and systems biologist, by training, that pivoted into data. Using the skills I learned as a researcher I am exploring new ways to pursue my passion.\n\nExperience\nGuillevin International | Data Specialist | 2022-2024\nUniversity of Toronto | Research Assistant | 2021\nUniversity of Toronto | Teaching Assistant | 2018-2020\n\n\nEducation\nUniversity of Toronto, St. George | Toronto, ON | Master of Cell and Systems Biology | 2018-2020\nUniversity of Toronto, St. George | Toronto, ON | Honors Bachelors of Science | 2014-2018"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Portfolio Site",
    "section": "",
    "text": "A researcher at heart, I love to work with data and I am passionate about learning new ways to pursue research based hypotheses.\nThis website is a space for me to share my journey with data. I include all personal projects with structured reports. Personal career information is also available."
  },
  {
    "objectID": "listings.html",
    "href": "listings.html",
    "title": "Reports",
    "section": "",
    "text": "Modeling Canadian Rent Data and Visualizing Using Power BI\n\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nVictor Lao\n\n\n\n\n\n\n\n\nShiny App of Metacritic Featured Games Throughout Time\n\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\nVictor Lao\n\n\n\n\n\n\n\n\nfileR: A Pipeable Interface to Directory Creation and Use\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2023\n\n\nVictor Lao\n\n\n\n\n\n\n\n\nUsing Rvest to Create a Product Code Translator\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nVictor Lao\n\n\n\n\n\n\n\n\nCreating a Data Exploration Tool for Revenue Data Using Shiny\n\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nVictor Lao\n\n\n\n\n\n\n\n\nWorld Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nVictor Lao\n\n\n\n\n\n\n\n\nNetwork Mapping the Most Influential Addresses Using DAI (2021)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nVictor Lao\n\n\n\n\n\n\n\n\nNetwork Mapping the Most Influential Addresses using DAI (2021): Extracting Blockchain Data from Public Google Datasets with BigQuery and SQL\n\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2022\n\n\nVictor Lao\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/MakerDAO_dataset_generation_report.html",
    "href": "posts/MakerDAO_dataset_generation_report.html",
    "title": "Network Mapping the Most Influential Addresses using DAI (2021): Extracting Blockchain Data from Public Google Datasets with BigQuery and SQL",
    "section": "",
    "text": "Blockchain technologies are inarguably reshaping how the general public thinks of finance. One particularly interesting innovation is the concept of decentralized autonomous organizations (or DAOs). Leveraging the transparency of the blockchain, DAOs endeavour to collaboratively run organizations in a trustless way. This novel method of conducting corporate operations makes a compelling subject for analysis.\nOne prominent DAO is MakerDAO. They are most well known for their over-collateralized USD stable coin, DAI. As all core MakerDAO operations and products operate through the blockchain, I retrieved all emitted event logs pertaining to MakerDAO smart contracts (executes blockchain functions) using BigQuery and the MakerDAO technical docs."
  },
  {
    "objectID": "posts/MakerDAO_dataset_generation_report.html#methodology",
    "href": "posts/MakerDAO_dataset_generation_report.html#methodology",
    "title": "Network Mapping the Most Influential Addresses using DAI (2021): Extracting Blockchain Data from Public Google Datasets with BigQuery and SQL",
    "section": "Methodology",
    "text": "Methodology\nTable of MakerDAO events was created using the publicly available Ethereum blockchain dataset on BigQuery. As addresses are stored on the blockchain as a 42 character hexadecimal, all smart contract addresses were converted to their contract name using MakerDAO technical docs. Topic 0 of event logs typically contains a unqiue event type specific index and was used to annotate function events. Each unique event type was individually parsed for sender identity, reciever identity and value. Makerdao stores values as integers which stand for differing decimals of percision depending on the event type. An unfortunate side effect of this storage method is that the values are too large for BigQuery to handle (can only go up to INT64). This necessitated using R to view the correct values. Values were stored on BigQuery as hexadecimals.\n\nCREATE TABLE \n     makerdao_operations.may2021to2022\nPARTITION BY DATE (\n     block_timestamp\n)\nCLUSTER BY\n     makerdao_smartcontract,\n     makerdao_function\nAS (\n\nSELECT\n     logs.block_timestamp AS block_timestamp,\n     transaction_hash,\n     from_address,\n     to_address,\n     value AS eth_value,\n     gas_price,\n     CASE\n          WHEN address = '0x6b175474e89094c44da98b954eedeac495271d0f' THEN 'DAI'\n          WHEN address = '0x9759A6Ac90977b93B58547b4A71c78317f391A28' THEN 'DAIJOIN'\n          WHEN address = '0x2F0b23f53734252Bda2277357e97e1517d6B042A' THEN 'GEMJOIN'\n          WHEN address = '0x197E90f9FAD81970bA7976f33CbD77088E5D7cf7' THEN 'POT'\n          WHEN address = '0x19c0976f590D67707E62397C87829d896Dc0f1F1' THEN 'JUG'\n          WHEN address = '0xA950524441892A31ebddF91d3cEEFa04Bf454466' THEN 'VOW'\n          WHEN address = '0xC4269cC7acDEdC3794b221aA4D9205F564e27f0d' THEN 'FLAPPER'\n          WHEN address = '0xA41B6EF151E06da0e34B009B86E828308986736D' THEN 'FLOPPER'\n          WHEN address = '0x35D1b3F3D7966A1DFe207aa4514C12a259A0492B' THEN 'VAT'\n          WHEN address = '0x65C79fcB50Ca1594B025960e539eD7A9a6D434A3' THEN 'SPOT'\n          WHEN address = '0x135954d155898D42C90D2a57824C690e0c7BEf1B' THEN 'DOG'\n          WHEN address = '0xc67963a226eddd77B91aD8c421630A1b0AdFF270' THEN 'CLIPPER'\n          WHEN address = '0x1EB4CF3A948E7D72A198fe073cCb8C7a948cD853' THEN 'FLASH'\n          WHEN address = '0x9f8F72aA9304c8B593d555F12eF6589cC3A579A2' THEN 'MKR'\n          WHEN address = '0x0a3f6849f78076aefaDf113F5BED87720274dDC0' THEN 'DSCHIEF'\n          WHEN address = '0xA618E54de493ec29432EbD2CA7f14eFbF6Ac17F7' THEN 'DSTOKEN'\n          WHEN address = '0x82ecD135Dce65Fbc6DbdD0e4237E0AF93FFD5038' THEN 'PROXY'\n          WHEN address = '0x5ef30b9986345249bc32d8928B7ee64DE9435E39' THEN 'CDPMANAGER'\n               END AS makerdao_smartcontract,\n     CASE\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(1)] NOT IN (\n                                        '0x0000000000000000000000000000000000000000',\n                                        '0x2F0b23f53734252Bda2277357e97e1517d6B042A',\n                                        '0x0a3f6849f78076aefadf113f5bed87720274ddc0'\n                                        )                                                               AND\n               topics[OFFSET(2)] NOT IN ('0x0000000000000000000000000000000000000000',\n                                        '0x2F0b23f53734252Bda2277357e97e1517d6B042A',\n                                        '0x0a3f6849f78076aefadf113f5bed87720274ddc0'\n                                        )                                                               THEN 'transfer' --DAI/MKR/IOU\n          WHEN topics[OFFSET(0)] = '0x8c5be1e5ebec7d5bd14f71427d1e84f3dd0314c0f7b2291e5b200ac8c7c3b925' THEN 'approve' --DAI/MKR/IOU\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(1)] = '0x0000000000000000000000000000000000000000'                         THEN 'mint/exit' --DAIJOIN\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(2)] = '0x0000000000000000000000000000000000000000'                         THEN 'burn/join' --DAIJOIN\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(1)] = '0x2F0b23f53734252Bda2277357e97e1517d6B042A'                         THEN 'mint/exit' --GEMJOIN\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(2)] = '0x2F0b23f53734252Bda2277357e97e1517d6B042A'                         THEN 'burn/join' --GEMJOIN\n          WHEN topics[OFFSET(0)] = '0x9f678cca00000000000000000000000000000000000000000000000000000000' THEN 'drip' --POT\n          WHEN topics[OFFSET(0)] = '0x7f8661a100000000000000000000000000000000000000000000000000000000' THEN 'exit' --POT\n          WHEN topics[OFFSET(0)] = '0x049878f300000000000000000000000000000000000000000000000000000000' THEN 'join' --POT\n          WHEN topics[OFFSET(0)] = '0x44e2a5a800000000000000000000000000000000000000000000000000000000' THEN 'drip' --JUG\n          WHEN topics[OFFSET(0)] = '0xf37ac61c00000000000000000000000000000000000000000000000000000000' THEN 'heal' --VOW/VAT\n          WHEN topics[OFFSET(0)] = '0xd7ee674b00000000000000000000000000000000000000000000000000000000' THEN 'flog' --VOW\n          WHEN topics[OFFSET(0)] = '0xc959c42b00000000000000000000000000000000000000000000000000000000' THEN 'deal' --FLAPPER\n          WHEN topics[OFFSET(0)] = '0xa3b22fc400000000000000000000000000000000000000000000000000000000' THEN 'hope' --VAT\n          WHEN topics[OFFSET(0)] = '0xdc4d20fa00000000000000000000000000000000000000000000000000000000' THEN 'nope' --VAT\n          WHEN topics[OFFSET(0)] = '0xbb35783b00000000000000000000000000000000000000000000000000000000' THEN 'move' --VAT\n          WHEN topics[OFFSET(0)] = '0x7cdd3fde00000000000000000000000000000000000000000000000000000000' THEN 'slip' --VAT\n          WHEN topics[OFFSET(0)] = '0x7bab3f4000000000000000000000000000000000000000000000000000000000' THEN 'grab' --VAT\n          WHEN topics[OFFSET(0)] = '0xdfd7467e425a8107cfd368d159957692c25085aacbcf5228ce08f10f2146486e' THEN 'poke' --SPOT\n          WHEN topics[OFFSET(0)] = '0x85258d09e1e4ef299ff3fc11e74af99563f022d21f3f940db982229dc2a3358c' THEN 'bark' --DOG\n          WHEN topics[OFFSET(0)] = '0x54f095dc7308776bf01e8580e4dd40fd959ea4bf50b069975768320ef8d77d8a' THEN 'digs' --DOG\n          WHEN topics[OFFSET(0)] = '0x7c5bfdc0a5e8192f6cd4972f382cec69116862fb62e6abff8003874c58e064b8' THEN 'kick' --CLIPPER\n          WHEN topics[OFFSET(0)] = '0x05e309fd6ce72f2ab888a20056bb4210df08daed86f21f95053deb19964d86b1' THEN 'take' --CLIPPER\n          WHEN topics[OFFSET(0)] = '0x0d7d75e01ab95780d3cd1c8ec0dd6c2ce19e3a20427eec8bf53283b6fb8e95f0' THEN 'flashloan' --FLASH\n          WHEN topics[OFFSET(0)] = '0xcc16f5dbb4873280815c1ee09dbd06736cffcc184412cf7a71a0fdb75d397ca5' THEN 'burn' --MKR/DSTOKEN\n          WHEN topics[OFFSET(0)] = '0x0f6798a560793a54c3bcfe86a93cde1e73087d944c0ea20544137d4121396885' THEN 'mint' --MKR/DSTOKEN\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(1)] = '0x0a3f6849f78076aefadf113f5bed87720274ddc0'                         THEN 'free' --DSCHIEF\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' AND\n               topics[OFFSET(2)] = '0x0a3f6849f78076aefadf113f5bed87720274ddc0'                         THEN 'lock' --DSCHIEF\n          WHEN topics[OFFSET(0)] = '0x4f0892983790f53eea39a7a496f6cb40e8811b313871337b6a761efc6c67bb1f' THEN 'etch' --DSCHIEF(slate) \n          WHEN topics[OFFSET(0)] IN (\n                                   '0xa69beaba00000000000000000000000000000000000000000000000000000000',\n                                   '0xed08132900000000000000000000000000000000000000000000000000000000'\n                                   )                                                                    THEN 'vote' --DSCHIEF\n          WHEN topics[OFFSET(0)] = '0xd6be0bc178658a382ff4f91c8c68b542aa6b71685b8fe427966b87745c3ea7a2' THEN 'open' --CDPMANAGER\n          WHEN topics[OFFSET(0)] = '0xfcafcc6800000000000000000000000000000000000000000000000000000000' THEN 'give' --CDPMANAGER\n          WHEN topics[OFFSET(0)] = '0x45e6bdcd00000000000000000000000000000000000000000000000000000000' THEN 'frob' --CDPMANAGER\n          WHEN topics[OFFSET(0)] = '0x9bb8f83800000000000000000000000000000000000000000000000000000000' THEN 'flux' --CDPMANAGER\n          WHEN topics[OFFSET(0)] = '0xf9f30db600000000000000000000000000000000000000000000000000000000' THEN 'move' --CDPMANAGER\n          WHEN topics[OFFSET(0)] = '0x1b0dbf7200000000000000000000000000000000000000000000000000000000' THEN 'quit' --CDPMANAGER\n               END AS makerdao_function,\n     CASE --note following are in hex string format (as the numbers are too big for... bigquery)\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' --transfer/burn/mint value\n               THEN data --wad (10e18)\n          WHEN topics[OFFSET(0)] = '0x7f8661a100000000000000000000000000000000000000000000000000000000' --pot/DSR exit value\n               THEN topics[OFFSET(2)] --wad\n          WHEN topics[OFFSET(0)] = '0x049878f300000000000000000000000000000000000000000000000000000000' --pot/DSR join value\n               THEN topics[OFFSET(3)] --wad\n          WHEN topics[OFFSET(0)] = '0xbb35783b00000000000000000000000000000000000000000000000000000000' --vat move value\n               THEN topics[OFFSET(2)] --rad (10e45)\n          WHEN topics[OFFSET(0)] = '0xf37ac61c00000000000000000000000000000000000000000000000000000000' --vow/vat heal amount\n               THEN topics[OFFSET(1)] --rad\n          WHEN topics[OFFSET(0)] = '0x7cdd3fde00000000000000000000000000000000000000000000000000000000' --vat slip amount\n               THEN topics[OFFSET(3)] --wad\n          WHEN topics[OFFSET(0)] = '0x7bab3f4000000000000000000000000000000000000000000000000000000000' --vat grab debt (dart) amount\n               THEN CONCAT('0x', SUBSTR(data, 459, 64)) --wad\n          WHEN topics[OFFSET(0)] = '0x85258d09e1e4ef299ff3fc11e74af99563f022d21f3f940db982229dc2a3358c' --dog bark liquidation (art)\n               THEN CONCAT('0x', SUBSTR(data, 67, 64)) --wad\n          WHEN topics[OFFSET(0)] = '0x7c5bfdc0a5e8192f6cd4972f382cec69116862fb62e6abff8003874c58e064b8' --clipper kick DAI debt (tab) \n               THEN CONCAT('0x', SUBSTR(data, 67, 64)) --rad\n          WHEN topics[OFFSET(0)] = '0x05e309fd6ce72f2ab888a20056bb4210df08daed86f21f95053deb19964d86b1' --clipper take bid (price) \n               THEN CONCAT('0x', SUBSTR(data, 67, 64)) --ray (10e27)\n          WHEN topics[OFFSET(0)] = '0x0d7d75e01ab95780d3cd1c8ec0dd6c2ce19e3a20427eec8bf53283b6fb8e95f0' --flashloan loan value (amount)\n               THEN CONCAT('0x', SUBSTR(data, 67, 64)) --ray\n          WHEN topics[OFFSET(0)] = '0xcc16f5dbb4873280815c1ee09dbd06736cffcc184412cf7a71a0fdb75d397ca5' --MKR/DSTOKEN burn value\n               THEN data --wad\n          WHEN topics[OFFSET(0)] = '0x0f6798a560793a54c3bcfe86a93cde1e73087d944c0ea20544137d4121396885' --MKR/DSTOKEN mint value\n               THEN data --wad\n          WHEN topics[OFFSET(0)] = '0x45e6bdcd00000000000000000000000000000000000000000000000000000000' --CDPMANAGER frob debt change\n               THEN CONCAT('0x', SUBSTR(data, 267, 64)) --wad (assuming dart value of similar percision across contracts)\n          WHEN topics[OFFSET(0)] = '0x9bb8f83800000000000000000000000000000000000000000000000000000000' --CDPMANAGER flux collateral\n               THEN CONCAT('0x', SUBSTR(data, 203, 64)) --wad\n          WHEN topics[OFFSET(0)] = '0xf9f30db600000000000000000000000000000000000000000000000000000000' --CDPMANAGER move value\n               THEN CONCAT('0x', SUBSTR(data, 267, 64)) --rad\n                    END AS value,\n     CASE\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' --transfer/burn/mint\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x8c5be1e5ebec7d5bd14f71427d1e84f3dd0314c0f7b2291e5b200ac8c7c3b925' --approve\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x9f678cca00000000000000000000000000000000000000000000000000000000' --drip\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x7f8661a100000000000000000000000000000000000000000000000000000000' --exit\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x049878f300000000000000000000000000000000000000000000000000000000' --join\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x44e2a5a800000000000000000000000000000000000000000000000000000000' --drip\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xf37ac61c00000000000000000000000000000000000000000000000000000000' --heal\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xd7ee674b00000000000000000000000000000000000000000000000000000000' --flog\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xc959c42b00000000000000000000000000000000000000000000000000000000' --deal\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xa3b22fc400000000000000000000000000000000000000000000000000000000' --hope\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xdc4d20fa00000000000000000000000000000000000000000000000000000000' --nope\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xbb35783b00000000000000000000000000000000000000000000000000000000' --move(vat)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x7bab3f4000000000000000000000000000000000000000000000000000000000' --grab (u; user liquidated)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x85258d09e1e4ef299ff3fc11e74af99563f022d21f3f940db982229dc2a3358c' --bark (urn liquidated)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x7c5bfdc0a5e8192f6cd4972f382cec69116862fb62e6abff8003874c58e064b8' --kick (auction id)\n               THEN SAFE_CAST(SAFE_CAST(topics[OFFSET(1)] AS INT64) AS STRING)\n          WHEN topics[OFFSET(0)] = '0x05e309fd6ce72f2ab888a20056bb4210df08daed86f21f95053deb19964d86b1' --take (who; auction bidder)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x0d7d75e01ab95780d3cd1c8ec0dd6c2ce19e3a20427eec8bf53283b6fb8e95f0' --flashloan (reciever)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xcc16f5dbb4873280815c1ee09dbd06736cffcc184412cf7a71a0fdb75d397ca5' --burn (DSTOKEN)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xa69beaba00000000000000000000000000000000000000000000000000000000' --vote\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xd6be0bc178658a382ff4f91c8c68b542aa6b71685b8fe427966b87745c3ea7a2' --open\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xfcafcc6800000000000000000000000000000000000000000000000000000000' --give (cdp id)\n               THEN topics[OFFSET(2)]\n          WHEN topics[OFFSET(0)] = '0x45e6bdcd00000000000000000000000000000000000000000000000000000000' --frob\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x9bb8f83800000000000000000000000000000000000000000000000000000000' --flux (cpd id)\n               THEN topics[OFFSET(2)]\n          WHEN topics[OFFSET(0)] = '0xf9f30db600000000000000000000000000000000000000000000000000000000' --move (cdp id)\n               THEN topics[OFFSET(2)]\n          WHEN topics[OFFSET(0)] = '0x1b0dbf7200000000000000000000000000000000000000000000000000000000' --quit (cdp id)\n               THEN topics[OFFSET(2)]\n                    END AS sender,\n     CASE\n          WHEN topics[OFFSET(0)] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' --transfer/burn/mint\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x8c5be1e5ebec7d5bd14f71427d1e84f3dd0314c0f7b2291e5b200ac8c7c3b925' --approve\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xbb35783b00000000000000000000000000000000000000000000000000000000' --move(vat)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x7cdd3fde00000000000000000000000000000000000000000000000000000000' --slip(vat)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(2)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x7bab3f4000000000000000000000000000000000000000000000000000000000' --grab (w; user recieving debt)\n               THEN CONCAT('0x', SUBSTR(data, 355, 40)) --retrieve address from data (not logged)\n          WHEN topics[OFFSET(0)] = '0x85258d09e1e4ef299ff3fc11e74af99563f022d21f3f940db982229dc2a3358c' --bark (auction id)\n               THEN SAFE_CAST(SAFE_CAST(topics[OFFSET(3)] AS INT64) AS STRING)\n          WHEN topics[OFFSET(0)] = '0x7c5bfdc0a5e8192f6cd4972f382cec69116862fb62e6abff8003874c58e064b8' --kick (kpr of dai incentive)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(3)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x05e309fd6ce72f2ab888a20056bb4210df08daed86f21f95053deb19964d86b1' --take (auction id)\n               THEN SAFE_CAST(SAFE_CAST(topics[OFFSET(1)] AS INT64) AS STRING)\n          WHEN topics[OFFSET(0)] = '0x0d7d75e01ab95780d3cd1c8ec0dd6c2ce19e3a20427eec8bf53283b6fb8e95f0' --flashloan (token)\n               THEN CONCAT('0x', SUBSTR(data, 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x0f6798a560793a54c3bcfe86a93cde1e73087d944c0ea20544137d4121396885' --mint (DSTOKEN)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x4f0892983790f53eea39a7a496f6cb40e8811b313871337b6a761efc6c67bb1f' --etch\n               THEN topics[OFFSET(1)]\n          WHEN topics[OFFSET(0)] = '0xa69beaba00000000000000000000000000000000000000000000000000000000' --vote (slate)\n               THEN topics[OFFSET(2)]\n          WHEN topics[OFFSET(0)] = '0xd6be0bc178658a382ff4f91c8c68b542aa6b71685b8fe427966b87745c3ea7a2' --open (cdp id)\n               THEN topics[OFFSET(3)]\n          WHEN topics[OFFSET(0)] = '0xfcafcc6800000000000000000000000000000000000000000000000000000000' --give (dst address)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(1)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x45e6bdcd00000000000000000000000000000000000000000000000000000000' --frob (affected cdp)\n               THEN topics[OFFSET(2)]\n          WHEN topics[OFFSET(0)] = '0x9bb8f83800000000000000000000000000000000000000000000000000000000' --flux (dst)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(3)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0xf9f30db600000000000000000000000000000000000000000000000000000000' --move (dst)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(3)], 27, 40)) --remove zero padding\n          WHEN topics[OFFSET(0)] = '0x1b0dbf7200000000000000000000000000000000000000000000000000000000' --quit (dst)\n               THEN CONCAT('0x', SUBSTR(topics[OFFSET(3)], 27, 40)) --remove zero padding\n                    END AS reciever,\n     CASE WHEN topics[OFFSET(0)] IN ('0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef',\n                                     '0x7f8661a100000000000000000000000000000000000000000000000000000000',\n                                     '0x049878f300000000000000000000000000000000000000000000000000000000',\n                                     '0x7cdd3fde00000000000000000000000000000000000000000000000000000000',\n                                     '0x7bab3f4000000000000000000000000000000000000000000000000000000000',\n                                     '0x85258d09e1e4ef299ff3fc11e74af99563f022d21f3f940db982229dc2a3358c',\n                                     '0xcc16f5dbb4873280815c1ee09dbd06736cffcc184412cf7a71a0fdb75d397ca5',\n                                     '0x0f6798a560793a54c3bcfe86a93cde1e73087d944c0ea20544137d4121396885',\n                                     '0x45e6bdcd00000000000000000000000000000000000000000000000000000000',\n                                     '0x9bb8f83800000000000000000000000000000000000000000000000000000000'\n                                   )\n               THEN 'wad'\n          WHEN topics[OFFSET(0)] IN ('0x05e309fd6ce72f2ab888a20056bb4210df08daed86f21f95053deb19964d86b1',\n                                     '0x0d7d75e01ab95780d3cd1c8ec0dd6c2ce19e3a20427eec8bf53283b6fb8e95f0'\n                                   )\n               THEN 'ray'\n          WHEN topics[OFFSET(0)] IN ('0xbb35783b00000000000000000000000000000000000000000000000000000000',\n                                     '0xf37ac61c00000000000000000000000000000000000000000000000000000000',\n                                     '0x7c5bfdc0a5e8192f6cd4972f382cec69116862fb62e6abff8003874c58e064b8',\n                                     '0xf9f30db600000000000000000000000000000000000000000000000000000000'\n                                   )\n               THEN 'rad'\n                    END AS percision, --see value section\n     topics,\n     data\nFROM bigquery-public-data.crypto_ethereum.logs AS logs\nRIGHT JOIN bigquery-public-data.crypto_ethereum.transactions AS transactions\n     ON logs.transaction_hash = transactions.hash\nWHERE logs.block_timestamp BETWEEN '2021-05-30' and '2022-05-30'\n     AND address IN (\n     '0x6b175474e89094c44da98b954eedeac495271d0f', -- DAI address (ERC20 form)\n     '0x9759A6Ac90977b93B58547b4A71c78317f391A28', -- DAIJOIN address (mint/burn)\n     '0x2F0b23f53734252Bda2277357e97e1517d6B042A', -- GEMJOIN address (collateral)\n     '0x197E90f9FAD81970bA7976f33CbD77088E5D7cf7', -- POT address (savings contract)\n     '0x19c0976f590D67707E62397C87829d896Dc0f1F1', -- JUG address (stability fees)\n     '0xA950524441892A31ebddF91d3cEEFa04Bf454466', -- VOW address (balance sheet)\n     '0xC4269cC7acDEdC3794b221aA4D9205F564e27f0d', -- FLAPPER address (surplus)\n     '0xA41B6EF151E06da0e34B009B86E828308986736D', -- FLOPPER address (deficit - never been called thus far)\n     '0x35D1b3F3D7966A1DFe207aa4514C12a259A0492B', -- VAT address (vaults)\n     '0x135954d155898D42C90D2a57824C690e0c7BEf1B', -- DOG address (liquidations)\n     '0xc67963a226eddd77B91aD8c421630A1b0AdFF270', -- CLIPPER address (liquidations)\n     '0x1EB4CF3A948E7D72A198fe073cCb8C7a948cD853', -- FLASH address (flash loans)\n     '0x9f8F72aA9304c8B593d555F12eF6589cC3A579A2', -- MKR address (governance ERC20)\n     '0x0a3f6849f78076aefaDf113F5BED87720274dDC0', -- DSCHIEF address (governance voting system)\n     '0xA618E54de493ec29432EbD2CA7f14eFbF6Ac17F7', -- DSTOKEN address (governance IOU)\n     '0x82ecD135Dce65Fbc6DbdD0e4237E0AF93FFD5038', -- PROXY address (for proxy actions)\n     '0x5ef30b9986345249bc32d8928B7ee64DE9435E39'  -- CDPMANAGER address (for managing vaults)\n     )\n\n)"
  },
  {
    "objectID": "posts/MakerDAO_dataset_generation_report.html#references",
    "href": "posts/MakerDAO_dataset_generation_report.html#references",
    "title": "Network Mapping the Most Influential Addresses using DAI (2021): Extracting Blockchain Data from Public Google Datasets with BigQuery and SQL",
    "section": "References",
    "text": "References\nMakerDAO (2021). MakerDAO Technical Docs. URL https://docs.makerdao.com/\nGoogle (2021). BigQuery. URL https://cloud.google.com/bigquery"
  },
  {
    "objectID": "posts/revenue_data_exploration_app_report.html",
    "href": "posts/revenue_data_exploration_app_report.html",
    "title": "Creating a Data Exploration Tool for Revenue Data Using Shiny",
    "section": "",
    "text": "Shiny provides a framework for rapid development of data-oriented applications. This can be particularly useful when combined with data manipulation frameworks. Leveraging both technologies it is possible to disseminate substantial amounts of aggregated data, in a readable, interactive way, for domain experts to make use of.\nIn order to demonstrate the functionality of R, and particularly the Shiny framework, to the workplace, I took some of my own time to create a demo app summarizing and visualizing sales data. The variables and comparisons implemented for exploration were based on current workplace data availability. The app was then ported to a desktop format using the DesktopDeployR framework to avoid IT security bureaucracy."
  },
  {
    "objectID": "posts/revenue_data_exploration_app_report.html#methodology",
    "href": "posts/revenue_data_exploration_app_report.html#methodology",
    "title": "Creating a Data Exploration Tool for Revenue Data Using Shiny",
    "section": "Methodology",
    "text": "Methodology\nThe data used by the app is synthetic, but the organization is strongly influenced by the format of the data currently obtainable. The benefit of this is two-fold. First, the structure of data is familiar which aids in communication of app functions (i.e., employees can extrapolate how the data is crunched as they have worked with it before). Second, by forming a template of minimally required information, new data can more easily be coerced into an app compatible form (given all the necessary variables exist).\nA key feature of the app is the ability to optionally filter the data for time period and all categorical variables (Department, Taker, Salesman ID, Vendor and Customer) allowing the user control over the granularity of detail visualized. The ability to filter dates also allows for comparisons across time periods. Filtering of data was placed directly upstream of all visual/analytical endpoints (i.e., all visualizations are reactive and change to the same filtered dataset). Filtering placed on a collapsible sidebar to allow convenient manipulation.\n\nSales_Values <- reactive({\n\ntempData <- importedData %\\>%\n\ndplyr::filter(Invoice.Date >= input$MInput[1] & Invoice.Date <= input$MInput[2])\n\nif (!is.null(input$DInput)) {\n\ntempData <- tempData %\\>%\n\nfilter(Department %in% input$DInput)\n\n}\n\nif (!is.null(input$TInput)) {\n\ntempData <- tempData %\\>%\n\nfilter(Taker %in% input$TInput)\n\n}\n\nif (!is.null(input$RInput)) {\n\ntempData <- tempData %\\>%\n\nfilter(Salesrep.Id %in% input$RInput)\n\n}\n\nif (!is.null(input$CInput)) {\n\ntempData <- tempData %\\>%\n\nfilter(Customer.Name %in% input$CInput)\n\n}\n\nif (!is.null(input$VInput)) {\n\ntempData <- tempData %\\>%\n\nfilter(Vendor %in% input$VInput)\n\n}\n\ntempData\n\n})\n\nDirectly right of the sidebar are three key summary stats on the filtered data (Revenue, Number of Invoices, Unique Customers). These are frozen on top of the screen as they are numbers that are very likely to be relevant to analysis.\nA collection of analytical visualizations was placed in a set of tabs. The Summary section expands the summary stats frozen at the top of the tab set. One key measure available here is a calculation of profit margin. By making the Summary section the default tab, computation is kept to a minimum allowing the app to respond quickly to user filters. Other tabs contain graphical interfaces to the data and take longer to respond – particularly if the data visualized is large.\nA combination of ggplot and plotly (mainly to convert the ggplot into an interactive plot) were used to create graphs that splay out data in a way that is permissive to exploration. Plotly interactivity features were key in unlocking the ability to drill down on data. The hope is that these plots will not only allow users to summarize data visually but also form further questions which can feed back into further analysis. A raw table was also included using the DT package.\nR is an open-source language geared towards data analysis but is unfortunately not ubiquitous in business. IT bureaucracy is notorious in approving new technologies and with shiny being a webapp framework primarily – which requires investment of web infrastructure, quick deployment of solutions is all but impossible. To circumvent this annoyance, I used DesktopDeployR to convert the shiny app into a desktop app (essentially). Briefly, DesktopDeployR uses a combination of a portable R framework and scripts to run the shiny app from folder. Importantly, this framework does not require administrative privileges or effort on the IT teams’ part to safely deploy."
  },
  {
    "objectID": "posts/revenue_data_exploration_app_report.html#references",
    "href": "posts/revenue_data_exploration_app_report.html#references",
    "title": "Creating a Data Exploration Tool for Revenue Data Using Shiny",
    "section": "References",
    "text": "References\nC. Sievert. Interactive Web-Based Data Visualization with R, plotly, and shiny. Chapman and Hall/CRC Florida, 2020.\nChang W, Cheng J, Allaire J, Sievert C, Schloerke B, Xie Y, Allen J, McPherson J, Dipert A, Borges B (2022). shiny: Web Application Framework for R. R package version 1.7.4, https://CRAN.R-project.org/package=shiny.\nwleepang (2020) DesktopDeployR [Source code] https://github.com/wleepang/DesktopDeployR.\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686.\nXie Y, Cheng J, Tan X (2023). DT: A Wrapper of the JavaScript Library ‘DataTables’. R package version 0.27, https://CRAN.R-project.org/package=DT."
  },
  {
    "objectID": "posts/WTO_export_map/WTO_export_map_report.html",
    "href": "posts/WTO_export_map/WTO_export_map_report.html",
    "title": "World Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet",
    "section": "",
    "text": "The recent pandemic has caused disruptions to global trade activities. Coupled with economic insecurity and rearranging world politics, market relations between countries have changed greatly from what we remember. In order to clarify the current state of global markets, data was retrieved from the World Trade Organization stats portal for exports by partner economies in 2021 and visualized on a map using leaflet. Reported trade connections were plotted alongside value to visualize potential associations between a country’s trade connectivity and export value. In this report, I will go over the methodology used to create the map as well as some insight the map provides."
  },
  {
    "objectID": "posts/WTO_export_map/WTO_export_map_report.html#methodology",
    "href": "posts/WTO_export_map/WTO_export_map_report.html#methodology",
    "title": "World Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet",
    "section": "Methodology",
    "text": "Methodology\nThe data used for constructing the map was sourced from the World Trade Organization, an international organization aimed towards promoting fair and free trade in global economies. In pursuit of this goal they conduct economic research with datasets freely available on their stat portal. Data of export value for agricultural and non-agricultural products by partner economy in 2021 was exported as a csv file and imported into R.\nBefore the data could be visualized, it had to be manipulated into a format that leaflet would accept. Unfortunately, the exported dataset contained extra indicators I did not select for. In addition, some characters were not correctly translated. Data was filtered on the relevant indicators and incorrectly formatted country names were individually transformed into a UTF-8 compliant form.\n\nfiltered_data <- data %>%\n  filter(Indicator == \"AG - Value of exports to partner (imports by partner)\" |\n         Indicator == \"Non-AG - Value of exports to partner (imports by partner)\") %>%\n  select(Reporting.Economy, Partner.Economy, Value) %>%\n  group_by(Reporting.Economy, Partner.Economy) %>%\n  summarise(Value = sum(Value)) %>%\n  mutate(Reporting.Economy = ifelse(Reporting.Economy == \"C\\xf4te d'Ivoire\",\n                                    \"Cote d'Ivoire\",\n                                    ifelse(Reporting.Economy == \"T\\xfcrkiye\",\n                                           \"Turkiye\",\n                                           Reporting.Economy)),\n         Partner.Economy = ifelse(Partner.Economy == \"C\\xf4te d'Ivoire\",\n                                  \"Cote d'Ivoire\",\n                                  ifelse(Partner.Economy == \"T\\xfcrkiye\",\n                                         \"Turkiye\",\n                                         ifelse(Partner.Economy == \"Sao Tom\\xe9 and Principe\",\n                                                \"Sao Tome and Principe\",\n                                                Partner.Economy))))\n\n\n\n# A tibble: 734 × 3\n# Groups:   Reporting.Economy [113]\n   Reporting.Economy   Partner.Economy           Value\n   <chr>               <chr>                     <dbl>\n 1 Albania             Cabo Verde              0.00339\n 2 Albania             Montenegro             20.9    \n 3 Angola              Bhutan                  0.0971 \n 4 Angola              Djibouti                2.78   \n 5 Angola              Gabon                   0.250  \n 6 Angola              Gibraltar               0.259  \n 7 Angola              Liberia                 7.30   \n 8 Angola              Sao Tome and Principe   0.103  \n 9 Angola              Togo                  952.     \n10 Antigua and Barbuda Dominica                1.06   \n# ℹ 724 more rows\n\n\nCoordinates for each country were retrieved from the Google Maps API using ggmap.\n\ngeolocated_data <- filtered_data %>%\n  mutate_geocode(Reporting.Economy) %>%\n  rename(Reporting.lon = lon, Reporting.lat = lat) %>%\n  mutate_geocode(Partner.Economy) %>%\n  rename(Partner.lon = lon, Partner.lat = lat)\n\ngeolocated_data\n\n\n\n\nFinally, data was subset into several tables for future use in leaflet.\n\n##subset coordinate data to reporting country\nReporting_lonlat <- geolocated_data %>%\n  select(Reporting.Economy, Reporting.lon, Reporting.lat) %>%\n  unique() %>%\n  mutate(Grouping = \"World\")\n\n##add degree data for sizing\ndegree_data <- geolocated_data %>%\n  ungroup() %>%\n  select(Reporting.Economy, Partner.Economy) %>%\n  group_by(Reporting.Economy) %>%\n  summarise(Degree = n())\n\n##Create world data frame for sizing\nworld_data <- geolocated_data %>%\n  summarize(Value = sum(Value)) %>%\n  left_join(Reporting_lonlat) %>%\n  left_join(degree_data)\n\n##combine data\ncombined_data <- geolocated_data %>%\n  left_join(degree_data, by = c(\"Partner.Economy\" = \"Reporting.Economy\"))\n\nLeaflet is a popular mapping JavaScript library which has been implemented in R and was used to visualize trade data on a world map. Functions for color scales were provided by leaflet and was used to segment data by five color quantiles based on export value. Trade connectivity (or degree), was visualized by an incrementally increasing circle marker radius centered on each country.\n\n##functions and preparation for leaflet\nquantile_pal <- colorQuantile(\"Reds\", geolocated_data$Value, 5)\nquantile_pal_world <- colorQuantile(\"Blues\", world_data$Value, 5)\ndegree_size <- function(x) {\n  ifelse(x > 40, 60,\n         ifelse(40 > x & x > 30, 50,\n                ifelse(30 > x & x > 20, 40,\n                       ifelse(20 > x & x > 10, 30, 20))))\n}\n\nA minimal background map was used from the provider tiles included in the leaflet package. Each country was then represented as circle markers with sizes that reflected their trade connectivity. The color of each circle marker is assigned based export value quantile (described previously). Each country was given their own overlay group showcasing reported export value by partner economy in red. Total export trade value for the selected country was shown in blue. A “world” group was added with only total export trade values for all reporting economies (this is the default for the map).\n\n##visualize with leaflet\nWTO_Export_Map <- leaflet(combined_data) %>%\n  addProviderTiles(\"CartoDB.Positron\",\n                   options = providerTileOptions(minZoom = 0,\n                                                 maxZoom = 4)) %>%\n  addCircleMarkers(lng = ~Partner.lon,\n                   lat = ~Partner.lat,\n                   group = ~Reporting.Economy,\n                   label = ~paste0(Partner.Economy,\n                                  \": \",\n                                  round(Value, 2),\n                                  \" Million\"),\n                   stroke = TRUE,\n                   color = ~quantile_pal(Value),\n                   radius = ~degree_size(Degree)) %>%\n  addCircleMarkers(data = world_data,\n                   lng = ~Reporting.lon,\n                   lat = ~Reporting.lat,\n                   group = ~Reporting.Economy,\n                   label = ~paste0(Reporting.Economy,\n                                   \" Total Export Value: \",\n                                   round(Value, 2),\n                                   \" Million\"),\n                   color = ~quantile_pal_world(Value),\n                   radius = ~degree_size(Degree)) %>%\n  addCircleMarkers(data = world_data,\n                   lng = ~Reporting.lon,\n                   lat = ~Reporting.lat,\n                   group = ~Grouping,\n                   label = ~paste0(Reporting.Economy,\n                                   \" Total Export Value: \",\n                                   round(Value, 2),\n                                   \" Million\"),\n                   color = ~quantile_pal_world(Value),\n                   radius = ~degree_size(Degree)) %>%\n  addLegend(data = world_data,\n            pal = quantile_pal_world, \n            values = world_data$Value,\n            group = \"World\") %>%\n  addLayersControl(overlayGroups = ~c(Reporting.Economy, \"World\")) %>%\n  hideGroup(group = ~Reporting.Economy) %>%\n  setMaxBounds(-190, -100, 190, 100) %>%\n  setView(10, 10, 2)"
  },
  {
    "objectID": "posts/WTO_export_map/WTO_export_map_report.html#analysis",
    "href": "posts/WTO_export_map/WTO_export_map_report.html#analysis",
    "title": "World Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet",
    "section": "Analysis",
    "text": "Analysis\nVisualization of geographical data in an interactive format makes it easier to do exploratory data analysis of the dataset. It is obvious upon first glance at the “World” overlay group that trade connectivity is generally positively correlated with export value. This is expected as a greater number of trade relations should be correlated with greater export demand which naturally leads to greater export values. One exception may be Australia which seems to handle relatively small export values for its trade connectivity. A closer look at Australia’s trading partners suggest that this may be due to the lack of major export destinations such as the United States or European Union. In contrast, Vietnam, despite a lower number of export destinations, exported 25 times the value of Australia in 2021 (according to this dataset). This is plausibly, in part, due to its export relations with South Korea. It is important to remember while analyzing this dataset, that the pandemic was a time of great global change and trade relations are not yet likely to have been solidified. It would be interesting to redo this map for 2022 and see how trade relations may or may not have changed."
  },
  {
    "objectID": "posts/WTO_export_map/WTO_export_map_report.html#references",
    "href": "posts/WTO_export_map/WTO_export_map_report.html#references",
    "title": "World Trade Organization Export Map: Agricultural and Non-Agricultural Export Data for 2021 Visualized Using Leaflet",
    "section": "References",
    "text": "References\nD. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2. The R Journal, 5(1), 144-161. URL http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf\nJ. Cheng, B. Karambelkar and Y. Xie (2022). leaflet: Create Interactive Web Maps with the JavaScript ‘Leaflet’ Library. R package version 2.1.1 URL https://CRAN.R-project.org/package=leaflet\nR Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/\nR. Vaidyanathan, Y. Xie, J. Allaire, J. Cheng, C. Sievert, K. Russell (2021). htmlwidgets: HTML Widgets for R. R package version 1.5.4, https://CRAN.R-project.org/package=htmlwidgets\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\nWorld Trade Organization (2021). WTO Stats. http://stats.wto.org\nSpecial thanks to Tamim M. for discussion and review."
  },
  {
    "objectID": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html",
    "href": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html",
    "title": "Network Mapping the Most Influential Addresses Using DAI (2021)",
    "section": "",
    "text": "Decentralized Autonomous Organizations (DAO) use the blockchain to conduct transparent corporate operations. One of the most successful DAOs to date is MakerDAO. Their staying power in the tumultuous environment of crypto is largely attributed to the wide adoption of their overcollateralized backed USD stable coin, DAI.\nAll blockchain transactions are public and pseudonymous. This gives unprecedented public transparency to the inner workings of blockchain based businesses and their products. In this report, I extract all DAI specific transfers of value and map influential addresses in an interactive network using ggraph and visNetwork to investigate key actors in the DAI ecosystem. I will outline my methodology and analyze the resultant networkmap."
  },
  {
    "objectID": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#methodology",
    "href": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#methodology",
    "title": "Network Mapping the Most Influential Addresses Using DAI (2021)",
    "section": "Methodology",
    "text": "Methodology\nUsing the MakerDAO events table I created, I subset all DAI events. Values were stored in BigQuery as hexadecimals because values were too large to store in BigQuery.\n\nCREATE TABLE \n  makerdao_operations.dai_events_may2021to2022\nPARTITION BY DATE (\n  block_timestamp\n)\nCLUSTER BY\n  makerdao_smartcontract,\n  makerdao_function\nAS (\n  SELECT\n    block_timestamp,\n    makerdao_smartcontract,\n    makerdao_function,\n    to_address AS transaction_sender,\n    from_address AS transaction_reciever,\n    sender AS event_sender,\n    reciever AS event_reciever,\n    eth_value,\n    value,\n    percision\n  FROM `ethereum-explorer-354118.makerdao_operations.may2021to2022`\n  WHERE makerdao_smartcontract IN (\n    'DAI',\n    'DAIJOIN',\n    'GEMJOIN',\n    'JUG',\n    'POT'\n  )\n)\n\nThe subset table was imported into R and converted into decimal form according to the event specific degree of precision.\n\n#clean data from extracted from google bigquery\ndata <- as_tibble(\n  read.csv(\n    file = 'dai_events_may2021to2022.csv',\n    header = TRUE,\n    colClasses = c(\"Date\", \"character\", \"character\", \"character\",\n                  \"character\", \"numeric\", \"character\"))) %>% \n  mutate(value =\n    ifelse (percision == 'wad', value/10e18,\n      ifelse (percision == 'ray', value/10e27,\n        ifelse (percision == 'rad', value/10e45, NaN)))) %>% \n  select(-percision)\n\nFinally, all transfer (of value) events pertaining to the DAI smartcontract were extracted.\n\n#filter events to pull out only DAI transaction data\ndai_transactions <- data %>% \n  filter(makerdao_smartcontract == 'DAI' & \n    makerdao_function == 'transfer')\n\nBefore mapping a network, data had to be transformed into a form readable by network mapping packages. The node list was formed from unique senders and receivers. The edge list was formed from all transactions between unique addresses. Multiple transactions between the same addresses were counted as ‘weight’. Total transaction value was summed for each unique address pair.\n\n#create node list (list of nodes with corresponding IDs)\nnode_list <- dai_transactions %>% \n  distinct(sender) %>% \n  full_join(distinct(dai_transactions, reciever), \n    by = c(\"sender\" = \"reciever\")) %>% \n  rowid_to_column(\"id\")\n\n#check node list\nnode_list\n\n#create edge list with count as weight and add value column\nedge_list <- dai_transactions %>% \n  group_by(sender, reciever) %>% \n  summarize(weight = n(), amount = sum(value)) %>% \n  ungroup()\n\n#convert edge list addresses into id specified in node_list\nedge_list_byid <- edge_list %>%\n  left_join(node_list, by = \"sender\") %>%\n  rename(from = id) %>%\n  left_join(node_list, by = c(\"reciever\" = \"sender\")) %>%\n  rename(to = id) %>%\n  select(from, to, weight, amount)\n\n#check converted edge_list then remove original\nedge_list_byid\n\nIt is not uncommon for a user to have multiple accounts which can skew network analysis if not accounted for. To alleviate the effects of this, I used the infomap algorithm to cluster addresses into communities. The rationale was that most alternate accounts would closely transact with each other forming distinct clusters. The infomap algorithm was used as it takes into account edge (trade) direction. In order to identified key nodes in the network, addresses’ eigenvector centrality was calculated using trade frequency as a weight.\n\n#using the node_list and edge_list_byid we can create a\n#graph object\ngraph <- tbl_graph(nodes = node_list, edges = edge_list_byid, \n          directed = TRUE)\n\n#cluster nodes to find communities and centralities and rewrite graph object\ngraph <- graph %>%\n  activate(nodes) %>%\n  mutate(community = group_infomap(weights = weight)) %>%\n  mutate(centrality = centrality_eigen(weights = weight, directed = TRUE))\n\nAs the DAI transaction dataset consists of over 1,500,000 transactions, addresses (nodes) were filtered for unique transaction pairs (edges) that were in the top 50% quantile for total transaction amount. Low value transacting nodes are unlikely to be key to the network. As I am looking for important actors in the network, I selected for the top 0.1% of addresses by eigenvector centrality. The selected addresses were collapsed on their communities.\n\n#filter graph by centrality to reduce size by trimming data to visualize, value amount filtered to remove low value actors\n#note this also reduces community sizing as low value actors are excluded\nfiltered_graph <- graph %>%\n  activate(edges) %>%\n  filter(amount > quantile(amount, 0.50), weight > 100) %>%\n  activate(nodes) %>%\n  filter(centrality > quantile(centrality, 0.999)) %>%\n  filter(!node_is_isolated())\n\n#network is too large to be visualized so it will be contracted on its communities, weight and value will be summed\ncontracted_graph <- filtered_graph %>%\n  activate(nodes) %>%\n  convert(to_contracted, community) %>%\n  activate(nodes) %>%\n  mutate(community_size = map_dbl(.orig_data, ~ nrow(.x))) %>%\n  activate(edges) %>%\n  mutate(contracted_weight = map_dbl(.orig_data, ~ sum(.x$weight)), \n         contracted_amount = map_dbl(.orig_data, ~ sum(.x$amount)))\n\n\ncontracted_graph <- readRDS(\"contracted_graph.rds\")\ncontracted_graph\n\nThis graph was created by an old(er) igraph version.\n  Call upgrade_graph() on it to use with the current igraph version\n  For now we convert it on the fly...\n\n\n# A tbl_graph: 184 nodes and 968 edges\n#\n# A directed simple graph with 1 component\n#\n# Edge Data: 968 × 6 (active)\n    from    to .tidygraph_edge_index .orig_data        contracted_weight\n   <int> <int> <list>                <list>                        <dbl>\n 1     1     2 <int [45]>            <tibble [45 × 4]>             61537\n 2     1     3 <int [4]>             <tibble [4 × 4]>               1777\n 3     1     4 <int [20]>            <tibble [20 × 4]>              5699\n 4     1     5 <int [2]>             <tibble [2 × 4]>                332\n 5     1     6 <int [13]>            <tibble [13 × 4]>              4668\n 6     1     7 <int [15]>            <tibble [15 × 4]>             13995\n 7     1     8 <int [2]>             <tibble [2 × 4]>                493\n 8     1     9 <int [8]>             <tibble [8 × 4]>               3170\n 9     1    10 <int [12]>            <tibble [12 × 4]>             14052\n10     1    11 <int [5]>             <tibble [5 × 4]>               1281\n# ℹ 958 more rows\n# ℹ 1 more variable: contracted_amount <dbl>\n#\n# Node Data: 184 × 4\n  community .orig_data        .tidygraph_node_index community_size\n      <int> <list>            <list>                         <dbl>\n1        56 <tibble [95 × 3]> <int [95]>                        95\n2         8 <tibble [4 × 3]>  <int [4]>                          4\n3        72 <tibble [1 × 3]>  <int [1]>                          1\n# ℹ 181 more rows\n\n\nResulting network was graphed using ggraph. Node radius reflected community size and color reflected degree of connections (darker blue representing a higher degree).\n\n#graph network\nnetwork_graph <- ggraph(contracted_graph, layout = 'igraph', algorithm = 'kk') +\n  geom_edge_link(aes(edge_alpha = contracted_weight, edge_colour = contracted_amount)) +\n  scale_edge_colour_continuous(high = '#000000', low = '#56B1F7') +\n  geom_node_point(aes(size = sqrt(community_size), colour = centrality_degree())) +\n  scale_colour_continuous(high = '#000000', low = '#56B1F7')\n\n\n\n\n\n\nThough some tentative conclusions may be made about the general network, there are too many nodes to pick out key addresses. Only the highest valued transaction pairs were selected to keep the map readable.\n\n#subset graph on value transacted to visualize high value connections (and importantly, visualize readable labels)\n\nsubset_graph <- contracted_graph %>%\n  activate(edges) %>%\n  top_n(50, contracted_amount) %>%\n  activate(nodes) %>%\n  filter(!node_is_isolated())\n\nsubset_network_graph <- ggraph(subset_graph, layout = 'igraph', algorithm = 'kk') +\n  geom_edge_link(aes(edge_alpha = contracted_weight, edge_colour = contracted_amount)) +\n  scale_edge_colour_continuous(high = '#000000', low = '#56B1F7') +\n  geom_node_point(aes(size = sqrt(community_size), colour = centrality_degree())) +\n  scale_colour_continuous(high = '#000000', low = '#56B1F7') +\n  geom_node_text(aes(label = community), position = position_nudge(x = 0.2, y = 0.2))\n\n\n\n\n\n\nCommunity 56 looked to be the most promising community and was thus graphed. As addresses are long and cumbersome to read, I thought an interactive visualization may be more user friendly. Before mapping in visNetwork, community 56 data had to be transformed into a format visNetwork would accept.\n\n\n\n\n#prepare data for visualization of community 56 in order to display addresses (sender) in a readable way\n\n#create color palette for groups\npalette <- as_tibble(brewer.pal(6, 'Set3'))\n\nvis_edge_list <- community_56_graph %>%\n  activate(edges) %>%\n  arrange(desc(to)) %>%\n  as_tibble() %>%\n  mutate(arrows = 'to') %>%\n  rename(title = amount, value = weight)\n\nvis_edge_list\n\n# A tibble: 86 × 5\n    from    to value    title arrows\n   <int> <int> <int>    <dbl> <chr> \n 1    49    52     1 1200000  to    \n 2    17    50    34 1614047. to    \n 3    18    49    48 1403938. to    \n 4    18    48     8  898990. to    \n 5    18    47    20 1287273. to    \n 6    18    46     3  836676. to    \n 7    10    45    32  797796. to    \n 8    17    44    20 2348153. to    \n 9    17    43    53 1594078. to    \n10    28    40    14 1570956. to    \n# ℹ 76 more rows\n\nvis_node_list <- community_56_graph %>%\n  activate(nodes) %>%\n  mutate(grouping = group_infomap()) %>%\n  as_tibble() %>%\n  select(-id, -community) %>%\n  rowid_to_column('id') %>%\n  rename(title = sender, value = centrality) %>%\n  mutate(label = \"\", color = case_when(grouping == 1 ~ '#FB8072',\n                                       grouping == 2 ~ '#FFFFB3',\n                                       grouping == 3 ~ '#BEBADA',\n                                       grouping == 4 ~ '#8DD3C7',\n                                       grouping == 5 ~ '#80B1D3',\n                                       grouping == 6 ~ '#FDB462'))\n\nvis_node_list\n\n# A tibble: 54 × 6\n      id title                                        value grouping label color\n   <int> <chr>                                        <dbl>    <int> <chr> <chr>\n 1     1 0x0000000000007f150bd6f54c40a34d7c3d5e9f56 5.98e-2        2 \"\"    #FFF…\n 2     2 0xf7d9c9c06812c4a611a352ac82f638bdca6e09a8 2.00e-2        4 \"\"    #8DD…\n 3     3 0xd0effc6828972483db1c64106f71d6ad12606a53 1.35e-2        4 \"\"    #8DD…\n 4     4 0x2af33e93f68b8497bb535c492f5867c7e83f2bc4 4.82e-4        1 \"\"    #FB8…\n 5     5 0xe592427a0aece92de3edee1f18e0157c05861564 4.47e-1        4 \"\"    #8DD…\n 6     6 0x6c2d992b7739dfb363a473cc4f28998b7f1f6de2 2.19e-3        2 \"\"    #FFF…\n 7     7 0x2a84e2bd2e961b1557d6e516ca647268b432cba4 9.11e-3        3 \"\"    #BEB…\n 8     8 0x4a137fd5e7a256ef08a7de531a17d0be0cc7b6b6 1.65e-2        3 \"\"    #BEB…\n 9     9 0x4d246be90c2f36730bb853ad41d0a189061192d3 2.61e-2        5 \"\"    #80B…\n10    10 0xbf3f6477dbd514ef85b7d3ec6ac2205fd0962039 1.05e-2        1 \"\"    #FB8…\n# ℹ 44 more rows\n\n\nNetwork was mapped so that addresses would display on node mouse over and total transaction value would display on edge mouse over. Click lights up one degree of edges.\n\n#graph dynamic network with prepared data\ndynamic_network_graph <- visNetwork(vis_node_list, vis_edge_list, main = \n                                    \"Transaction Network of Top 1% Most Influential Addresses Using DAI\") %>%\n  visOptions(highlightNearest = list(enabled = TRUE, degree = list(from = 0, to = 1))) %>%\n  visInteraction(dragView = FALSE) %>%\n  visLayout(randomSeed = 11)\n\ndynamic_network_graph"
  },
  {
    "objectID": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#analysis",
    "href": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#analysis",
    "title": "Network Mapping the Most Influential Addresses Using DAI (2021)",
    "section": "Analysis",
    "text": "Analysis\nTo determine if my analysis picked out key players in the the DAI ecosystem, I checked to see if there were any exchange addresses. I expect exchanges would be central to the network because they deal in high transaction volumes and values. Indeed, I found 0x220bda5c8994804ac96ebe4df184d25e5c2196d4 and 0x1111111254fb6c44bAC0beD2854e76F90643097d. Both are smart contracts of 1inch, a popular DeFi aggregator. Sushiswap’s MANA-DAI pool, 0x495F8Ef80E13e9E1e77d60d2f384bb49694823ef, was among the addresses visualized. And finally, Uniswap’s V3 router 0xe592427a0aece92de3edee1f18e0157c05861564 was also present.\nThe more unknown addresses provided interesting insight into the users that hold up DAI. 0x1e3D6eAb4BCF24bcD04721caA11C478a2e59852D is one such mysterious address. Activity on this address stopped around May 2021 but beforehand seemed involved in high frequency trading of renBTC and wBTC. One explanation may be that this address was an arbitrage trader transacting in BTC derived cryptos. Their high DAI transaction values may indicate they used DAI as an arbitrage currency. DAI allows for fee-less flash-loans (loans that are instantaneously paid) making it very permissive for arbitrage traders."
  },
  {
    "objectID": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#references",
    "href": "posts/2022-09-01-DAI-networkmap/DaiTransfers_NetworkAnalysis_May2021to2022_Report.html#references",
    "title": "Network Mapping the Most Influential Addresses Using DAI (2021)",
    "section": "References",
    "text": "References\nAlmende B.V. and Contributors, Thieurmel B, Robert T (2021). visNetwork: Network Visualization using ‘vis.js’ Library. R package version 2.1.0, URL https://CRAN.R-project.org/package=visNetwork\nGoogle (2021). BigQuery. URL https://cloud.google.com/bigquery\nNeuwirth E (2022). RColorBrewer: ColorBrewer Palettes. R package version 1.1-3, URL https://CRAN.R-project.org/package=RColorBrewer\nPedersen T (2021). ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. R package version 2.0.5, URL https://CRAN.R-project.org/package=ggraph\nPedersen T (2022). tidygraph: A Tidy API for Graph Manipulation. R package version 1.2.1, URL https://CRAN.R-project.org/package=tidygraph\nR Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, URL https://doi.org/10.21105/joss.01686"
  },
  {
    "objectID": "posts/fileR_report.html",
    "href": "posts/fileR_report.html",
    "title": "fileR: A Pipeable Interface to Directory Creation and Use",
    "section": "",
    "text": "The R programming language can be used for a variety of purposes. The use case I find most relevant to my day-to-day life (read day job), is the repetitive conversion of data from one form to another (typically for entry into a database). Without cooperate business infrastructure for R, most work needs to be done on my own workstation.\nMany great packages and ecosystems exist for data manipulation in R. One collection of packages which frequently enters my work is the tidyverse. A key feature of these collection of packages is the ability of functions to be piped together.\nThe fileR package is my attempt at a framework that helps to facilitate repetitive data wrangling by separating file input/outputs into two distinct directories, ‘data’ and ‘results’."
  },
  {
    "objectID": "posts/fileR_report.html#rationaleframework",
    "href": "posts/fileR_report.html#rationaleframework",
    "title": "fileR: A Pipeable Interface to Directory Creation and Use",
    "section": "Rationale/Framework",
    "text": "Rationale/Framework\nR by default saves and reads files into and from the working directory. As the number of file inputs and outputs increase, the readability decreases. This can be a problem when returning to older projects.\nSub-folder and directories is the obvious answer to organization problems however, working with file paths is cumbersome and prone to error. Approaches to file organization should be standardized across projects so file inputs and outputs are more readily understood between projects.\nTo facilitate reproducible and easy file organization, I created a minimal framework for working directory structure. I then created wrappers around base R functions for reading and writing files to interact with that framework.\nThe working directory is divided into three sections. The working directory itself, the ‘data’ directory and the ‘results’ directory. The working directory should only contain R scripts and the ‘data’ and ‘results’ directories. The ‘data’ directory should only contain raw data. The ‘results’ directory should contain all products of manipulation in R. Folders (sub-directories) can be created within the ‘data’ and ‘results’ directory however, are recommended to be terminal."
  },
  {
    "objectID": "posts/fileR_report.html#methodology",
    "href": "posts/fileR_report.html#methodology",
    "title": "fileR: A Pipeable Interface to Directory Creation and Use",
    "section": "Methodology",
    "text": "Methodology\nWrappers were written around two base R functions: read.csv and write.csv. To ensure compatibility with the tidyverse set of packages, all functions that write files were made to be pipeable (i.e., the first argument is always an object that is returned back). Read functions are typically at the start of a pipe.\n\nsave_csv_to_results_folder <- function(x, folder_name, file_name, ...) {\n  if(!dir.exists(\"results\")) {\n    stop(\"results directory does not exist\")\n  }\n  if(!is.character(folder_name) | !is.character(file_name)) {\n    stop(\"folder_name and file_name must be of type character\")\n  }\n  if(!dir.exists(paste0(\"./results/\", folder_name))) {\n    dir.create(paste0(\"./results/\", folder_name))\n  }\n  write.csv(x, paste0(\"./results/\", folder_name, \"/\", file_name, \".csv\"), ...)\n  invisible(x)\n}\n\nThe above ‘save_csv_to_results_folder’ function returns ‘x’ invisibly. One use case, would be to save intermediate results to a folder which allows you to produce multiple outputs in one pipe without having to exit the pipe.\nFunctions were also written to read data from entire folders and return them as a list. There is no function to recursively retrieve files from an entire directory as it should be clear where your data is coming from (i.e., you should state it in code).\n\nread_all_csv_from_data <- function(folder_name = NULL, ...) {\n  if(!dir.exists(\"data\")) {\n    stop(\"data directory does not exist\")\n  }\n  if(!is.null(folder_name)) {\n    if(!is.character(folder_name)) {\n      stop(\"folder_name must be of type character\")\n    }\n    if(!dir.exists(paste0(\"./data/\", folder_name))) {\n      stop(paste0(folder_name, \" does not exist in the data directory\"))\n    }\n    file_list <- list.files(paste0(\"./data/\", folder_name))\n    file_accumulator <- vector(\"list\", length = length(file_list))\n    names(file_accumulator) <- file_list\n    lapply(\n      file_list,\n      function(file_name){\n        file <- read.csv(paste0(\"./data/\", folder_name, \"/\", file_name), ...)\n        file_accumulator[[file_name]] <<- file\n      }\n    )\n    return(file_accumulator)\n  } else {\n    file_list <- list.files(\"./data\")\n    file_accumulator <- vector(\"list\", length = length(file_list))\n    names(file_accumulator) <- file_list\n    lapply(\n      file_list,\n      function(file_name){\n        file <- read.csv(paste0(\"./data/\", file_name), ...)\n        file_accumulator[[file_name]] <<- file\n      }\n    )\n    return(file_accumulator)\n  }\n  return(file_accumulator)\n}\n\nThe above ‘read_all_csv_from_data’ initializes a list and with names based on the file names being read. Data is then read in and appended to the pre-existing list. This approach is to prevent growing a vector in R as it can cause drastic speed reductions for larger datasets.\nTo facilitate repetitive cleanups of folders and directories (e.g., cleaning out a data input folder to prevent processing data twice), functions were written to move batches of files between folders within a directory.\n\nmove_all_files_to_data_folder <- function(to, exclude = NULL) {\n  if(!is.character(to)) {\n    stop(\"to parameter must be of type character\")\n  }\n  if(length(to) != 1) {\n    stop(\"to parameter must be of length 1\")\n  }\n  all_files <- list.files(paste0(\"./data\"), recursive = TRUE, full.names = TRUE)\n  target_files <- all_files[!grepl(paste0(\"^./data/\", to, \"/\"), all_files)]\n  if(!is.null(exclude)) {\n    if(is.character(exclude)) {\n      lapply(\n        exclude,\n        function(x) target_files <<- target_files[!grepl(paste0(\"^./data/\", x, \"/\"), target_files)]\n      )\n    } else {\n      stop(\"exclusion(s) must be of type character\")\n    }\n  }\n  file.copy(\n    from = target_files,\n    to = paste0(\"./data/\", to, \"/\")\n  )\n  from_test <- list.files(\"./data\", recursive = TRUE)\n  if(!is.null(exclude)) {\n    if(is.character(exclude)) {\n      lapply(\n        exclude,\n        function(x) from_test <<- from_test[!grepl(paste0(\"^\", x, \"/\"), from_test)]\n      )\n    } else {\n      stop(\"exclusion(s) must be of type character\")\n    }\n  }\n  from_test <- sub(\"^.*/\", \"\", from_test)\n  to_test <- list.files(paste0(\"./data/\", to))\n  if(!all((from_test %in% to_test))) {\n    stop(\"files were not correctly copied\")\n  }\n  file.remove(target_files)\n  invisible(NULL)\n}\n\nIn the above function, files are copied to the new folder, checked, then deleted from the original folder. There is an implicit assumption in the way the check is done. The string cleanup assumes that the ‘data’ directory does not recurse more than one level. As a check with these assumptions is done, unintentional usage should generate an error. This is in line with the opinions of this package. The ‘to’ folder is excluded from the copy function to prevent unnecessary repetition however, further exclusions can be provided, optionally."
  },
  {
    "objectID": "posts/fileR_report.html#list-of-functions",
    "href": "posts/fileR_report.html#list-of-functions",
    "title": "fileR: A Pipeable Interface to Directory Creation and Use",
    "section": "List of Functions",
    "text": "List of Functions\nFunctions were named to be descriptive at the cost of being verbose.\n\ncreate_data_folder()\nPurpose: To create a folder in the ‘data’ directory. Data should be moved into pre-existing ‘data’ locations.\n\n\nlist_data_files()\nPurpose: To list files in ‘data’ directory and optionally a sub-directory of the ‘data’ directory.\n\n\nmove_all_files_to_data_folder()\nPurpose: To move all files recursively in the ‘data’ directory to a specified ‘data’ folder. Files in the target folder prior to move are not affected. Additional folders can be specified, optionally.\n\n\nmove_all_files_to_results_folder()\nPurpose: To move all files recursively in the ‘results’ directory to a specified ‘results’ folder. Files in the target folder prior to move are not affected. Additional folders can be specified, optionally.\n\n\nmove_files_between_data_folders()\nPurpose: To move all files in one ‘data’ folder to another ‘data’ folder.\n\n\nmove_files_between_results_folders()\nPurpose: To move all files in one ‘results’ folder to another ‘results’ folder.\n\n\nread_all_csv_from_data()\nPurpose: To return all csv files in the ‘data’ directory as a list. Sub-directories of the ‘data’ directory can also be targeted.\n\n\nread_all_csv_from_results()\nPurpose: To return all csv files in the ‘results’ directory as a list. Sub-directories of the ‘results’ directory should be targeted over the entire ‘results’ directory.\n\n\nread_csv_from_data()\nPurpose: To read a single csv file from the ‘data’ directory or one of its sub-directories.\n\n\nread_csv_from_results()\nPurpose: To read a single csv file from the ‘data’ directory or one of its sub-directories.\n\n\nsave_csv_to_results_folder()\nPurpose: To save an object to the ‘results’ directory or one of its sub-directories as a csv file. Will create a sub-directory or ‘folder’ if one does not exist. Will return the object passed in invisibly.\n\n\nsetup_fileR_directories()\nPurpose: Creates ‘data’ and ‘results’ directories. Projects using fileR should start with this function."
  },
  {
    "objectID": "posts/leviton_crossreference_scrape.html",
    "href": "posts/leviton_crossreference_scrape.html",
    "title": "Using Rvest to Create a Product Code Translator",
    "section": "",
    "text": "Webpages remain a key source of information in the digital age (think Wikipedia). For repetitive tasks is inefficient to manually interact with these sources of information. Thankfully, web scraping tools exist that help to make retrieving information from the internet easier.\nA recent problem I came across was the conversion of product codes from one manufacturer to another. Different vendors have different naming conventions for the same part. These vendors typically have online resources for converting competitor product names into their own. To accomplish the task of converting our own product list, I developed a small scrapper using the “rvest” package to automate translation."
  },
  {
    "objectID": "posts/leviton_crossreference_scrape.html#repository-link",
    "href": "posts/leviton_crossreference_scrape.html#repository-link",
    "title": "Using Rvest to Create a Product Code Translator",
    "section": "Repository Link",
    "text": "Repository Link\nhttps://github.com/Spingelhoff/leviton_crossreference_scraper"
  },
  {
    "objectID": "posts/leviton_crossreference_scrape.html#methodology",
    "href": "posts/leviton_crossreference_scrape.html#methodology",
    "title": "Using Rvest to Create a Product Code Translator",
    "section": "Methodology",
    "text": "Methodology\nThe goal of the project was to create a re-usable system that would be able to read an arbitrary amount of source items and convert them into the closest equivalent of another vendor. The fileR package (personal package) was used to standardize input and output into directories. The rvest package was used to extract relevant information (in this case the equivalent product code).\nThe reference manual was served on static pages. The URL changed predictably with each search query. This allowed for the programmatic generation of URL links from a list of source items.\n\naccess_lev_reference <- function(product) {\n\n  html_search_start <- \"https://www.leviton.com/en/support/resources-tools/manufacturer-cross-reference?id-upc=\"\n  html_search_end <- \"&itemsPerPage=1&page=1\"\n\n  html_link <- str_c(\n    html_search_start,\n    product,\n    html_search_end\n  )\n\n  read_html(html_link)\n}\n\nThis collection of links can then be scraped for translations by targeting the relevant element (using CS selectors).\n\ncrawl_nodes_for_lev <- function(x) {\n  result <- x |>\n    html_elements(\".ss-md.product__title\") |>\n    html_element(\"a\") |>\n    html_attr(\"href\") |>\n    str_remove(\"/en/products/\") |>\n    str_to_upper() |>\n    as.character() # coerce from list to character to allow for row binding\n\n  prefixed_result <- str_c(\n    \"LEV-\",\n    result\n  )\n\n  standardized_result <- if(!length(prefixed_result) == 1) {\n    \"NO MATCH FOUND\"\n  } else {\n    prefixed_result\n  }\n}"
  },
  {
    "objectID": "posts/leviton_crossreference_scrape.html#citations",
    "href": "posts/leviton_crossreference_scrape.html#citations",
    "title": "Using Rvest to Create a Product Code Translator",
    "section": "Citations",
    "text": "Citations\nLao V (2023) fileR: A Pipeable Interface To Directory Creation and Use. R package, https://github.com/Spingelhoff/fileR.\nWickham H (2022). rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.3, https://CRAN.R-project.org/package=rvest."
  },
  {
    "objectID": "posts/video_game_analysis_2024_report.html",
    "href": "posts/video_game_analysis_2024_report.html",
    "title": "Shiny App of Metacritic Featured Games Throughout Time",
    "section": "",
    "text": "Gaming is a rapidly growing industry with a consumer history that stretches back to the 1970s. Throughout the years many games have been released that have had profound impact on the gaming community. To sift through the quantity of releases, video game critics have established themselves as core members of the gaming ecosystem. Metacritic is a website that aggregates video game (among other mediums) reviews and has proven to be highly influential.\nTaking the history of games featured on Metacritic and diving into the data can reveal many insights into the historic interplay between games and consumer opinion. It can also help in suggesting high quality games to play (which may be my main motivator for this analysis…). Using Shiny can aid in this exploration by facilitating dynamic viewpoints of the data."
  },
  {
    "objectID": "posts/video_game_analysis_2024_report.html#methodology",
    "href": "posts/video_game_analysis_2024_report.html#methodology",
    "title": "Shiny App of Metacritic Featured Games Throughout Time",
    "section": "Methodology",
    "text": "Methodology\nMetacritic data was obtained through Kaggle as a publicly available dataset (link: https://www.kaggle.com/datasets/beridzeg45/video-games). Data was obtained by the publisher through scraping the Metacritic website using Python libraries.\nData was then enriched through personal knowledge to create a more descriptive dataset. In brief, genres were condensed into 12 categories.\n\n  mutate(\n    ...,\n    genre = case_when(\n      str_detect(genre, \"RPG\") ~ \"RPG\",\n      str_detect(genre, \"Adventure|Platform|Metroid|Point\") ~ \"Adventure\",\n      str_detect(genre, \"Strategy|RTS|Tactic|MOBA|Defense\") ~ \"Strategy\",\n      str_detect(genre, \"Racing\") ~ \"Racing\",\n      str_detect(genre, \"Fight\") ~ \"Fighter\",\n      str_detect(genre, \"Action|Beat|Arcade|Sandbox|Rougelike\") ~ \"Action\",\n      str_detect(genre, \"Shoot|FPS|Gun|Artillery\") ~ \"Shooter\",\n      str_detect(genre, \"Sim|Virtual|Manage|Tycoon|Visual Novel\") ~ \"Sim\",\n      str_detect(genre, \"Puzzle|Hidden|Trivia|Edutainment\") ~ \"Puzzle\",\n      str_detect(genre, \"Sport|Ball|ball|Golf|Ski|Wrest|Rugb|Hock|Billi|Tenn|Socc|Bik|Skat|Hunt|Surf|Crick|Bowl|Fish|Athlet|Gambl\") ~ \"Sport\",\n      str_detect(genre, \"Danc|Party|Rhythm|Exercise\") ~ \"Party\",\n      str_detect(genre, \"Survival\") ~ \"Survival\",\n      TRUE ~ \"Other\"\n    ),\n    ...\n  )\n\nPlatform versions were grouped into console families.\n\n  mutate(\n    platform_condensed = case_when(\n      str_detect(platform, \"iOS\") ~ \"Phone\",\n      str_detect(platform, \"Xbox\") ~ \"Xbox\",\n      str_detect(platform, \"Playstation|PlayStation|PSP\") ~ \"Playstation\",\n      str_detect(platform, \"Game|Wii|Nintendo|DS\") ~ \"Nintendo\",\n      str_detect(platform, \"Dreamcast\") ~ \"Dreamcast\",\n      str_detect(platform, \"PC\") ~ \"PC\",\n      str_detect(platform, \"Meta Quest\") ~ \"Meta VR\",\n      TRUE ~ \"Other\"\n    ),\n    ...\n  )\n\nLocality of each platform was annotated.\n\n  mutate(\n    ...,\n    locality = case_when(\n      str_detect(platform, \"iOS|PSP|DS|Game Boy\") ~ \"Mobile\",\n      TRUE ~ \"Home Console\"\n    ),\n    ...\n  )\n\nExclusivity was determined by marking duplicated titles.\n\n  mutate(\n    ...,\n    exclusivity = case_when(\n    duplicated(title) | duplicated(title, fromLast = TRUE) ~ \"Multi-Platform\",\n    TRUE ~ \"Exclusive\"\n    ),\n    ...\n  )\n\nScores were annotated with colloquial interpretations to give a more accurate representation of sentiment.\n\n  mutate(\n    ...,\n    metacritic_score = as.numeric(metacritic_score),\n    metacritic_score_factorized = case_when(\n      metacritic_score <= 50 ~ \"Poor\",\n      metacritic_score > 50 & metacritic_score <= 70 ~ \"Average\",\n      metacritic_score > 70 & metacritic_score <= 90 ~ \"Good\",\n      metacritic_score > 90 ~ \"Excellent\",\n      TRUE ~ \"Unscored\"\n    ),\n    ...\n  )\n\nIndependence was defined as publishers with only one developer under them. It should be noted that the validity of this categorization increases with time as successful indie companies eventually become commercial.\n\n  group_by(\n    publisher\n  ) |>\n  mutate(\n    total_developer_under_publisher = n_distinct(developer)\n  ) |>\n  ungroup() |>\n  mutate(\n    independance = case_when(\n      total_developer_under_publisher == 1 ~ \"Indie\",\n      TRUE ~ \"Commercial\"\n    )\n  )\n\nThe full data cleaning manipulation procedure can be found on my github: https://github.com/Spingelhoff/video_game_analysis_2024/blob/main/video_game_analysis_2024.R\nCleaned data was then showcased through Shiny. Number of games featured were divided by genre and color coded by colloquial score categories in a stacked bar graph. Distribution of titles by user score and Metacritic score were plotted as a scatterplot. Both graphs were connected interactively via ggiraph. By hovering over each genre bar, the distribution of points on the score scatterplot corresponding to the genre selected is highlighted.\n\noutput$combined_plot <- renderGirafe({\n    genre_plot <- ggplot(\n      yearly_genre_ratings(),\n      aes(x = fct_rev(genre), y = n_scored, fill = metacritic_score_factorized, data_id = genre)) +\n      geom_col_interactive(color = \"black\") +\n      coord_flip() +\n      theme_classic() +\n      scale_x_discrete(expand = c(0,0)) +\n      scale_y_continuous(expand = c(0,0)) +\n      scale_fill_brewer(palette = \"RdBu\", direction = -1) +\n      theme(\n        legend.position = \"bottom\",\n        text = element_text(size = 10)\n      ) +\n      xlab(\"# of Games\") +\n      ylab(\"Genre\") +\n      labs(\n        fill = \"Scoring\"\n      )\n    \n    ambivalence_plot <- ggplot(\n      yearly_ambivalence(),\n      aes(x = avg_user_score, y = avg_metacritic_score, size = critic_coverage, alpha = critic_coverage, color = classification, data_id = genre)) +\n      geom_point_interactive(\n        aes(tooltip = paste0(\n          \"Title: \", title, \"\\n\",\n          \"Genre: \", genre, \"\\n\",\n          \"Critic Score: \", avg_metacritic_score, \"\\n\",\n          \"User Score:\", avg_user_score, \"\\n\")\n        )\n      ) +\n      theme_classic() +\n      scale_color_manual(\n        name = \"Preference\",\n        values = c(\"#4F7EBB\", \"grey\", \"#BC3D41\")) +\n      scale_size_continuous(name = \"Coverage\") +\n      scale_alpha_continuous(name = \"Coverage\") +\n      theme(\n        legend.position = \"bottom\",\n        text = element_text(size = 10)\n      ) +\n      xlab(\"User Score\") +\n      ylab(\"Metacritic Score\") +\n      guides(\n        color = \"none\"\n      )\n    \n    girafe(ggobj = genre_plot + ambivalence_plot, width_svg = 12, height_svg = 5) |>\n      girafe_options(\n        opts_hover(css = \"stroke:black;stroke-width:2px;fill-opacity:1;\"),\n        opts_hover_inv(css = \"fill-opacity:0.2\"),\n        opts_selection(css = \"stroke:black;stroke-width:2px;fill-opacity:1;\",\n                       type = \"multiple\"),\n        opts_selection_inv(css = \"fill-opacity:0.2\")\n      )\n  })\n\nTop 5 Metacritic and user scored games as well as most divergently scored Metacritic and user scored games were shown through tables. Finally, locality, exclusivity and independance of titles were plotted as pie charts."
  },
  {
    "objectID": "posts/video_game_analysis_2024_report.html#references",
    "href": "posts/video_game_analysis_2024_report.html#references",
    "title": "Shiny App of Metacritic Featured Games Throughout Time",
    "section": "References",
    "text": "References\nChang W, Cheng J, Allaire J, Sievert C, Schloerke B, Xie Y, Allen J, McPherson J, Dipert A, Borges B (2022). shiny: Web Application Framework for R. R package version 1.7.4, https://CRAN.R-project.org/package=shiny.\nDavid Gohel and Panagiotis Skintzos (2024). ggiraph: Make ‘ggplot2’ Graphics Interactive. R package version 0.8.9. https://CRAN.R-project.org/package=ggiraph\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686."
  },
  {
    "objectID": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html",
    "href": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html",
    "title": "Modeling Canadian Rent Data and Visualizing Using Power BI",
    "section": "",
    "text": "Rentals play an important role in providing housing for residents of Canada. Recent economic stresses have created volatility in the rental market highlighting the need for both renters and landlords to stay up to date on current factors driving rent prices. To address the need for information, I created a rental market dashboard with data from Statistics Canada and CMHC and forecasted rent prices in Power BI."
  },
  {
    "objectID": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html#methodology",
    "href": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html#methodology",
    "title": "Modeling Canadian Rent Data and Visualizing Using Power BI",
    "section": "Methodology",
    "text": "Methodology\nData was taken from APIs from Statistics Canada and CMHC portals. Data was transformed and rent was modeled. Best performing model was selected and used to forecast the next year. Data is then visualized in Power BI.\n\nObtaining data from Statistics Canada and CMHC\nStatistics Canada releases economic data and indicators for Canada. They make their data available through an API. cansim is an R wrapper around their API that makes it easier to retrieve standardized data in R. The Canadian Mortgage and Housing Corporation (CMHC) releases housing and rental data for Canada. This information is made available through their data portal. cmhc is an R package written to obtain data from that portal. A list of sgc codes is required for cmhc. A list of provinces (excluding territories) was provided.\nTime series of several economic indicators were taken from Statistics Canada in order to provide exogenous regressors for modeling. Data from CMHC, including rent prices, was taken and combined with data from CMHC on date, geography and rental room type.\n\n\nModeling Rent Prices\nRent price per year was adjusted by consumer price index to account for inflation. They were also transformed to percent rent price change per year to create a more stationary response variable for models that perform better on stationary data.\nRent price was modeled by ARIMA, ETS, linear regression, drift, mean and naive. Mean, naive and drift models were used as a baseline. Model performance was assessed by time series cross validation with a forecast of one year. Models were compared with RMSSE to account for scale differences between models trained on rent price vs those trained on percentage rent price per year (depending on model requirements).\n\nbase_models <- tscv_model_data |>\n  model(\n    mean = MEAN(avg_rent_cpi_adj_pct_change),\n    naive = NAIVE(avg_rent_cpi_adj_pct_change)\n  )\nrw_models <- tscv_model_data |>\n  model(\n    drift = RW(avg_rent_cpi_adj ~ drift())\n  )\narima_models <- tscv_model_data |>\n  model(\n    arima = ARIMA(avg_rent_cpi_adj_pct_change)\n  )  \nets_models <- tscv_model_data |>\n  model(\n    ets = ETS(avg_rent_cpi_adj)\n  )\nlinear_models <- lagged_tscv_model_data |>\n  model(\n    base_linear = TSLM(avg_rent_cpi_adj_pct_change ~ lagged_rental_supply_per_person_pct_change + lagged_five_year_mortgage_change),\n    trended_linear = TSLM(avg_rent_cpi_adj_pct_change ~ trend() + lagged_rental_supply_per_person_pct_change + lagged_five_year_mortgage_change)\n  )\n\nLjung box test was used to check residuals on the longest cross validation set. Models failing test were removed prior to model selection.\n\nget_ljung_box <- function(model, lag) {\n  ljung_box_stats <- model |>\n    augment() |>\n    features(.innov, ljung_box, lag = lag) |>\n    filter(`.id` == max(`.id`, na.rm = TRUE)) |> \n    select(-`.id`)\n  \n  return(ljung_box_stats)\n}\n\nbase_residuals <- base_models |>\n  get_ljung_box(lag = 4)\nrw_residuals <- rw_models |>\n  get_ljung_box(lag = 4)\narima_residuals <- arima_models |>\n  get_ljung_box(lag = 4)\nets_residuals <- ets_models |>\n  get_ljung_box(lag = 4)\nlinear_residuals <- linear_models |>\n  get_ljung_box(lag = 4)\n\n\n\nVisualizing Data in Power BI\nTo visualize rent data in Power BI, relevant tables were converted to excel format prior to ingestion.\n\nnamed_excel_table_list <- list(\n  avg_rent = avg_rent_data,\n  provincial_room_indicators = yearly_provincial_room_indicators_data,\n  provincial_indicators = yearly_provincial_indicators_data,\n  indicators = yearly_indicators_data,\n  forecast = yearly_forecast_table\n)\n\ncreate_canadian_rent_excel_tables <- function(named_excel_table_list) {\n  workbook <- createWorkbook()\n  \n  table_names <- names(named_excel_table_list)\n  \n  create_excel_pages_and_tables <- function(table_name, table) {\n    addWorksheet(workbook, table_name)\n    writeDataTable(workbook, sheet = table_name, x = table, tableName = table_name)\n  }\n  \n  mapply(\n    create_excel_pages_and_tables,\n    table_name = table_names,\n    table = named_excel_table_list\n  )\n  \n  return(workbook)\n}\n\nexcel_workbook <- named_excel_table_list |>\n  create_canadian_rent_excel_tables()\n\nIn Power BI a new table was created to visualize forecasting data using DAX.\n\nforecast_viz = ADDCOLUMNS(\n  DISTINCT(\n    UNION(\n      SELECTCOLUMNS(\n        avg_rent, \"year\", avg_rent[year], \"geography\", avg_rent[Geography], \n        \"room_type\", avg_rent[room_type], \"Average Rent\", avg_rent[Rent], \n        \"Average Rent % Change\", [avg_rent_pct_change], \"80% Upper Confidence\",\n        avg_rent[Rent], \"Percent 80% Upper Confidence\", \n        avg_rent[avg_rent_pct_change], \"80% Lower Confidence\", avg_rent[Rent], \n        \"Percent 80% Lower Confidence\", avg_rent[avg_rent_pct_change]\n      ), \n      SELECTCOLUMNS(\n        forecast, \"year\", forecast[year], \"geography\", forecast[geography], \n        \"room_type\", forecast[room_type], \"Average Rent\", forecast[forecast], \n        \"Average Rent % Change\", forecast[pct_forecast], \"80% Upper Confidence\", \n        forecast[upper], \"Percent 80% Upper Confidence\", forecast[pct_upper], \n        \"80% Lower Confidence\", forecast[lower], \"Percent 80% Lower Confidence\", \n        forecast[pct_lower]\n      )\n    )\n  ), \n  \"geography_room\", CONCATENATE([geography], [room_type])\n)\n\nData model was organized as shown below:\n\n\n\n\n\n\n\n\n\nImportantly, several indicators were connected through a concatenation of geography and room type to ensure cross filtering would work across both variables."
  },
  {
    "objectID": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html#references",
    "href": "posts/canadian_provincial_rent_powerbi_model/canadian_provincial_rent_modeling_report.html#references",
    "title": "Modeling Canadian Rent Data and Visualizing Using Power BI",
    "section": "References",
    "text": "References\nO’Hara-Wild M, Hyndman R, Wang E (2024). fable: Forecasting Models for Tidy Time Series. R package version 0.3.4, https://CRAN.R-project.org/package=fable\nO’Hara-Wild M, Hyndman R, Wang E (2024). feasts: Feature Extraction and Statistics for Time Series. R package version 0.3.2, https://CRAN.R-project.org/package=feasts.\nvon Bergmann and Shkolnik (2021). cansim: Accessing Statistics Canada Data Table and Vectors. https://CRAN.R-project.org/package=cansim.\nvon Bergmann J (2023). cmhc: Access, Retrieve, and Work with CMHC Data. R package version 0.2.7, https://CRAN.R-project.org/package=cmhc.\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686."
  }
]
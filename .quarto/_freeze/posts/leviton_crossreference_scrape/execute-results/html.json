{
  "hash": "28d09ab76227005da2b43b676496484b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Using Rvest to Create a Product Code Translator\"\nimage: \"DataTransfers.jpeg\"\nauthor: \"Victor Lao\"\ndate: 2023-03-28\noutput: \n  blogdown::html_page:\n    toc: TRUE\n---\n\n\n## Introduction\n\nWebpages remain a key source of information in the digital age (think Wikipedia). For repetitive tasks is inefficient to manually interact with these sources of information. Thankfully, web scraping tools exist that help to make retrieving information from the internet easier.\n\nA recent problem I came across was the conversion of product codes from one manufacturer to another. Different vendors have different naming conventions for the same part. These vendors typically have online resources for converting competitor product names into their own. To accomplish the task of converting our own product list, I developed a small scrapper using the \"rvest\" package to automate translation.\n\n## Repository Link\n\n<https://github.com/Spingelhoff/leviton_crossreference_scraper>\n\n## Methodology\n\nThe goal of the project was to create a re-usable system that would be able to read an arbitrary amount of source items and convert them into the closest equivalent of another vendor. The fileR package (personal package) was used to standardize input and output into directories. The rvest package was used to extract relevant information (in this case the equivalent product code).\n\nThe reference manual was served on static pages. The URL changed predictably with each search query. This allowed for the programmatic generation of URL links from a list of source items.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccess_lev_reference <- function(product) {\n\n  html_search_start <- \"https://www.leviton.com/en/support/resources-tools/manufacturer-cross-reference?id-upc=\"\n  html_search_end <- \"&itemsPerPage=1&page=1\"\n\n  html_link <- str_c(\n    html_search_start,\n    product,\n    html_search_end\n  )\n\n  read_html(html_link)\n}\n```\n:::\n\n\nThis collection of links can then be scraped for translations by targeting the relevant element (using CS selectors).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrawl_nodes_for_lev <- function(x) {\n  result <- x |>\n    html_elements(\".ss-md.product__title\") |>\n    html_element(\"a\") |>\n    html_attr(\"href\") |>\n    str_remove(\"/en/products/\") |>\n    str_to_upper() |>\n    as.character() # coerce from list to character to allow for row binding\n\n  prefixed_result <- str_c(\n    \"LEV-\",\n    result\n  )\n\n  standardized_result <- if(!length(prefixed_result) == 1) {\n    \"NO MATCH FOUND\"\n  } else {\n    prefixed_result\n  }\n}\n```\n:::\n\n\n## Citations\n\nLao V (2023) *fileR: A Pipeable Interface To Directory Creation and Use*. R package, <https://github.com/Spingelhoff/fileR>.\n\nWickham H (2022). *rvest: Easily Harvest (Scrape) Web Pages*. R package version 1.0.3, <https://CRAN.R-project.org/package=rvest>.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}